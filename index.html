<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="LXY&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="LXY&#39;s Blog">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LXY&#39;s Blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>LXY's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LXY's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/04/Cross-Validation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/04/Cross-Validation/" itemprop="url">Cross Validation</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-04T15:57:25-04:00">
                2019-10-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/Performance-Measures/" itemprop="url" rel="index">
                    <span itemprop="name">Performance Measures</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/04/SDGclassifier/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/04/SDGclassifier/" itemprop="url">SDGclassifier</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-04T15:55:03-04:00">
                2019-10-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/Classification/" itemprop="url" rel="index">
                    <span itemprop="name">Classification</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html" target="_blank" rel="noopener">SGDClassifier官方文档</a></p>
</blockquote>
<p>梯度下降法（SGD）是一个简单有效的方法，用于判断使用凸loss函数（convex loss function）的分类器（SVM或logistic回归）。SGD被成功地应用在大规模稀疏机器学习问题上（large-scale and sparse machine learning），经常用在文本分类及自然语言处理上。假如数据是稀疏的，该模块的分类器可以轻松地解决这样的问题：超过10^5的训练样本、超过10^5的features。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sklearn.linear_model.SGDClassifier(loss=’hinge’, penalty=’l2’, alpha=<span class="number">0.0001</span>, l1_ratio=<span class="number">0.15</span>, fit_intercept=<span class="literal">True</span>, </span><br><span class="line">max_iter=<span class="literal">None</span>, tol=<span class="literal">None</span>, shuffle=<span class="literal">True</span>, verbose=<span class="number">0</span>, epsilon=<span class="number">0.1</span>, n_jobs=<span class="literal">None</span>, random_state=<span class="literal">None</span>, </span><br><span class="line">learning_rate=’optimal’, eta0=<span class="number">0.0</span>, power_t=<span class="number">0.5</span>, early_stopping=<span class="literal">False</span>, validation_fraction=<span class="number">0.1</span>, n_iter_no_change=<span class="number">5</span>, class_weight=<span class="literal">None</span>, warm_start=<span class="literal">False</span>, average=<span class="literal">False</span>, n_iter=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>SGD的优点是：</p>
<ul>
<li>高效</li>
<li>容易实现（有许多机会进行代码调优）</li>
</ul>
<p>SGD的缺点是：</p>
<ul>
<li>SGD需要许多超参数：比如正则项参数、迭代数。</li>
<li>SGD对于特征归一化（feature scaling）是敏感的。</li>
</ul>
<p>注意：SGD是一种优化方法，不是分类算法，SGDClassifier函数实际使用的是SVM或logistic回归算法进行分类！！！</p>
<p>介绍SGDClassifier内的几个重要参数：</p>
<ol>
<li>损失函数loss，有三种选择，对应不同分类算法</li>
</ol>
<p>loss=”hinge”: (soft-margin)线性SVM.<br>loss=”modified_huber”: 带平滑的hinge loss.<br>loss=”log”: logistic回归</p>
<ol start="2">
<li>通过penalty参数，可以设置对应的惩罚项。SGD支持下面的罚项：</li>
</ol>
<p>penalty=”l2”: 对coef_的L2范数罚项（岭回归）<br>penalty=”l1”: 对coef_的L1范数罚项（套索回归）<br>penalty=”elasticnet”: L2和L1的convex组合; (1 - l1_ratio) * L2 + l1_ratio * L1 （弹性网络）<br>正则化器是对损失函数添加的惩罚，其使用平方欧几里德范数L2或绝对范数L1或两者的组合（弹性网络）将模型参数缩减到零向量。更多参考：<a href="http://d0evi1.com/sklearn/sgd/" target="_blank" rel="noopener">http://d0evi1.com/sklearn/sgd/</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/04/MNIST/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/04/MNIST/" itemprop="url">MNIST</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-04T01:15:18-04:00">
                2019-10-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/dataset/" itemprop="url" rel="index">
                    <span itemprop="name">dataset</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>MNIST 数据集来自美国国家标准与技术研究所, National Institute of Standards and Technology (NIST). 训练集 (training set) 由来自 250 个不同人手写的数字构成, 其中 50% 是高中学生, 50% 来自人口普查局 (the Census Bureau) 的工作人员. 测试集(test set) 也是同样比例的手写数字数据.</p>
<p>MNIST 数据集下载地址： <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/</a> </p>
<p>包含如下四个部分：</p>
<ul>
<li>Training set images: train-images-idx3-ubyte.gz (9.9 MB, 解压后 47 MB, 包含 60,000 个样本)</li>
<li>Training set labels: train-labels-idx1-ubyte.gz (29 KB, 解压后 60 KB, 包含 60,000 个标签)</li>
<li>Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 解压后 7.8 MB, 包含 10,000 个样本)</li>
<li>Test set labels: t10k-labels-idx1-ubyte.gz (5KB, 解压后 10 KB, 包含 10,000 个标签)</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/03/绝对路径与相对路径/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/03/绝对路径与相对路径/" itemprop="url">绝对路径与相对路径</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-03T23:55:54-04:00">
                2019-10-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index">
                    <span itemprop="name">python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>windows平台</p>
<p>python中路径的表达方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">open(<span class="string">'xxx.txt'</span>)</span><br><span class="line">open(<span class="string">'/data/xxx.txt'</span>)</span><br><span class="line">open(<span class="string">'D:\\data\\xxx.txt'</span>)</span><br></pre></td></tr></table></figure>

<p>前两者为相对路径，第三个为绝对路径。绝对路径指的是完整的路径，相对路径指相对于当前文件夹的路径。</p>
<p>第一个命令为打开当前文件夹内文件；</p>
<p>第二个命令为打开当前文件夹内，data文件夹里面的txt；</p>
<p>第三个命令为打开该路径文件。</p>
<blockquote>
<p>特别注意：‘/’用来表示绝对路径，’\‘用来表示相对路径，而’\\‘则是转义。网址和linux系统常用’/‘表示。</p>
</blockquote>
<p>获取当前文件夹的绝对路径：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">path = os.path.abspath(<span class="string">'.'</span>) <span class="comment">#当前文件夹绝对路径</span></span><br><span class="line">path = os.path.abspath(<span class="string">'..'</span>)<span class="comment">#当前文件夹的上级文件夹的绝对路径</span></span><br></pre></td></tr></table></figure>

<p>特别注意：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'..'</span> <span class="comment">#表示上级文件夹</span></span><br><span class="line"><span class="string">'.'</span>  <span class="comment">#表示当前文件夹</span></span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/03/Hexo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/03/Hexo/" itemprop="url">Hexo建立博客</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-03T19:15:35-04:00">
                2019-10-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Blog/" itemprop="url" rel="index">
                    <span itemprop="name">Blog</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>如何通过Hexo + Github 快速搭建个人Blog：</p>
<p>Hexo 可以生成静态站点，可以把Markdown文件展示成静态页面。</p>
<p>步骤：</p>
<ul>
<li>搭建本地环境</li>
<li>建立Github Pages</li>
<li>绑定域名</li>
<li>经营、优化博客</li>
</ul>
<h1 id="本地环境"><a href="#本地环境" class="headerlink" title="本地环境"></a>本地环境</h1><ol>
<li>安装 Node.js</li>
</ol>
<p>Hexo是使用 Node.js开发的，所以必须安装。</p>
<ol start="2">
<li>安装 Git</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/03/Next-theme/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/03/Next-theme/" itemprop="url">Next theme</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-03T18:28:58-04:00">
                2019-10-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Blog/" itemprop="url" rel="index">
                    <span itemprop="name">Blog</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何美化NEXT主题"><a href="#如何美化NEXT主题" class="headerlink" title="如何美化NEXT主题"></a>如何美化NEXT主题</h1><h2 id="1-添加动态背景"><a href="#1-添加动态背景" class="headerlink" title="1. 添加动态背景"></a>1. 添加动态背景</h2><p>NEXT主题5.1.1版本以上内置该功能(太受欢迎)。</p>
<p>配置文件 <code>themes/next/_config.yml</code>中找到：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Canvas-nest</span><br><span class="line">canvas_nest: false</span><br></pre></td></tr></table></figure>

<p>改为：</p>
<p><code>canvas_nest: true</code></p>
<p>想要更改该背景的参数：</p>
<p>文件所在位置<code>themes\next\source\lib\canvas-nest\canvas-nest.min.js</code></p>
<p>参数说明：</p>
<ul>
<li><code>color</code>: 线条色彩，默认<code>&quot;0,0,0&quot;</code> 三个值分别对应（R,G,B)</li>
<li><code>opacity</code>: 线条透明度，默认 <code>0.5</code>，取值范围 <code>0~1</code></li>
<li><code>count</code>: 线条总数，默认<code>99</code></li>
<li><code>zIndex</code> 该背景的z-index属性，CSS属性用于控制所在层的位置，默认<code>-1</code></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/02/读书笔记-规模-Scale/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/02/读书笔记-规模-Scale/" itemprop="url">读书笔记-规模(Scale)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-02T22:30:07-04:00">
                2019-10-02
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/读书笔记/" itemprop="url" rel="index">
                    <span itemprop="name">读书笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="读《规模-复杂世界的简单法则》"><a href="#读《规模-复杂世界的简单法则》" class="headerlink" title="读《规模:复杂世界的简单法则》"></a>读《规模:复杂世界的简单法则》</h1><blockquote>
<p>杰弗里·韦斯特. 规模:复杂世界的简单法则 中信出版集团. Kindle 版本.</p>
</blockquote>
<h2 id="第一章-大背景"><a href="#第一章-大背景" class="headerlink" title="第一章 大背景"></a>第一章 大背景</h2><p>世界上有超过800万种物种，形态不一。所有复杂性和多样性的背后是否有一种潜在的规律？</p>
<p>所有迥异的高度复杂现象中，都存在着共同的概念框架————动物、植物、人类社会行为、城市与公司的活力、增长和组织事实上都遵循类似的一般规律。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/01/Lesson 5 Training/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/01/Lesson 5 Training/" itemprop="url">Hands on Machine Learning-Chapter 4</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-01T19:52:14-04:00">
                2019-10-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/Hands-on-Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Hands-on Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Training-Linear-Models"><a href="#Training-Linear-Models" class="headerlink" title="Training Linear Models"></a>Training Linear Models</h1><p>Before we used ML models without understanding how they work. Now we will try to look under the hood. We will look at two versions of Linear regression: regular closed-form equation $\beta = \frac{X’Y}{X’X}$ and iterative optimization Gradient Descent (GD) approach which tweaks model parameters to minimize cost function. Even though eventually GD will converge to the parameters $\beta$ estimated by the first method, GD is the important concepts that will help us to study neural networks. </p>
<p>Next we will look at the polynomial regression, which is more flexible than linear regression. It has more parameters and more prone to overfitting. Finally, we will look at regression models used in classification: Logistic Regression and Softmax Regression. </p>
<p>In Chapter 1, we looked at simple regression: <br> <br><br>$life_satisfaction = \theta_0 + \theta_1 \times GDP_percapita$ <br> <br><br>This is a linear function of the input feature of one input feature. The model parameters are $\theta_0$ and $\theta_1$. In general regression has the following form: <br> <br><br>$$\hat{y} = \theta_0 + \theta_1 x_1 +  \theta_2 x_2 + … +  \theta_n x_n$$</p>
<ul>
<li>$\hat{y}$ is the predicted value</li>
<li>$n$ is the number of features</li>
<li>$x_i$ is the $i^{th}$ feature</li>
<li>$\theta_j$ is the $j^{th}$ model parameters. Number of parameters is $n+1$ because of the constant</li>
</ul>
<p>We can write linear regression in the vector form:<br>$$\hat{y} = h_{\theta} (x) = \theta^T \cdot x$$</p>
<ul>
<li>$\theta^T$ is the transpose of vector of parameters or feature weights $\theta$.</li>
<li>$\theta^T \cdot x$ is the dot product of vector of parameters $\theta^T$ and vector of features $x$.</li>
<li>$h_{\theta} (x)$ is the hypothesis function or prediction using parameters $\theta$</li>
</ul>
<p>First we train linear regression on training data minimizing Mean Square Error (MSE). Usually we use Root Mean Square Error (RMSE) as a metric to compare OLS models, the same set of parameters minimize both MSE and RMSE.<br>$$MSE(X, h_\theta) = \frac{1}{m}\sum^m_{i=1} \big(\theta^T \cdot x^{(i)} - y^{(i)}\big)^2$$</p>
<p>Later we will refer to $h_\theta$ as just $h$, and $MSE(\theta)$ instead of $MSE(X,h_\theta)$</p>
<p>Normal regression find $\theta$ that minimizes MSE:<br>$$ \hat{\theta} = (\pmb{X}^T \cdot \pmb{X})^{-1} \cdot \pmb{X}^T \cdot \pmb{y}$$</p>
<ul>
<li>$\hat{\theta}$ is the $\theta$ that minimizes MSE</li>
<li>$\pmb{\theta}$ is the vector of target values $y^{(1)}$ to $y^{(m)}$.</li>
</ul>
<p><strong>Note</strong>: the first releases of the book implied that the <code>LinearRegression</code> class was based on the Normal Equation. This was an error, my apologies: as explained above, it is based on the pseudoinverse, which ultimately relies on the SVD matrix decomposition of $\mathbf{X}$ (see chapter 8 for details about the SVD decomposition). Its time complexity is $O(n^2)$ and it works even when $m &lt; n$ or when some features are linear combinations of other features (in these cases, $\mathbf{X}^T \mathbf{X}$ is not invertible so the Normal Equation fails), see <a href="https://github.com/ageron/handson-ml/issues/184" target="_blank" rel="noopener">issue #184</a> for more details. However, this does not change the rest of the description of the <code>LinearRegression</code> class, in particular, it is based on an analytical solution, it does not scale well with the number of features, it scales linearly with the number of instances, all the data must fit in memory, it does not require feature scaling and the order of the instances in the training set does not matter.</p>
<p>There is a mistake in the book: the LinearRegression class does not actually use the Normal Equation, it computes the pseudoinverse of X (specifically the Moore-Penrose pseudoinverse), and it multiplies it by y. It gives the same result as the normal equation, but it has two important advantages:</p>
<p>It is $O(n^2)$ instead of $O(n^2.4)$ to $O(n^3)$. So it’s much faster than the Normal Equation when there are many features.<br>It behaves better when some of the eigenvalues of X are small (or equal to zero). In plain English, this is when some features are highly correlated (or perfectly correlated, that is when one feature is a linear combination of the others). It also behaves well when m &lt; n. More details below.</p>
<p>When some features are linear combinations of the others, or when m &lt; n, the matrix $X^T$, $X$ is not invertible (it is said to be “singular” or “degenerate”). So the Normal Equation cannot be used. However, the pseudoinverse is always defined: it is based on the SVD decomposition of the matrix $X$, which finds the eigenvalues and eigenvectors of the matrix (as explained in chapter 8 on dimensionality reduction, when talking about PCA), so it can simply ignore the eigenvalues that are smaller than a tiny threshold. This means that the LinearRegression class will find the optimal solution even when some features are redundant or when m &lt; n.<br>This also explains two parameters that the LinearRegression predictor learns during training:</p>
<p>rank_ is the rank of the matrix X, i.e., the number of eigenvalues that are not tiny or zero.<br>singular_ is the list of eigenvalues (just like PCA().fit(X).singular_values_). If a feature is a linear combination of other features, then one of these features will have a tiny or zero eigenvalue.<br>Fortunately, this error does not change much of what I wrote in the book about the LinearRegression class: it is based on an analytical solution, it performs poorly when there is a large number of features, it is linear with regards to the number of instances, it does not support out-of-core (i.e., all data must fit in memory to use it), it does not require feature scaling, the order of the instances in the training set does not matter, and so on. You could say it’s an implementation detail, but it’s quite an important one, and I apologize for the error.</p>
<h1 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h1><p>First, let’s make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># To support both python 2 and python 3</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division, print_function, unicode_literals</span><br><span class="line"></span><br><span class="line"><span class="comment"># Common imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># to make this notebook's output stable across runs</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># To plot pretty figures</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">'axes.labelsize'</span>] = <span class="number">14</span></span><br><span class="line">plt.rcParams[<span class="string">'xtick.labelsize'</span>] = <span class="number">12</span></span><br><span class="line">plt.rcParams[<span class="string">'ytick.labelsize'</span>] = <span class="number">12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Where to save the figures</span></span><br><span class="line">PROJECT_ROOT_DIR = <span class="string">".."</span></span><br><span class="line">CHAPTER_ID = <span class="string">"training_linear_models"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_fig</span><span class="params">(fig_id, tight_layout=True)</span>:</span></span><br><span class="line">    path = os.path.join(PROJECT_ROOT_DIR, <span class="string">"images"</span>, CHAPTER_ID, fig_id + <span class="string">".png"</span>)</span><br><span class="line">    print(<span class="string">"Saving figure"</span>, fig_id)</span><br><span class="line">    <span class="keyword">if</span> tight_layout:</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">    plt.savefig(path, format=<span class="string">'png'</span>, dpi=<span class="number">300</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ignore useless warnings (see SciPy issue #5998)</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(action=<span class="string">"ignore"</span>, module=<span class="string">"scipy"</span>, message=<span class="string">"^internal gelsd"</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Linear-regression-using-the-Normal-Equation"><a href="#Linear-regression-using-the-Normal-Equation" class="headerlink" title="Linear regression using the Normal Equation"></a>Linear regression using the Normal Equation</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#Generate random numbers between 0 and 1. </span></span><br><span class="line">X = <span class="number">2</span> * np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * X + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>We generate data with a noisy linear trend, let’s plot it.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(X, y, <span class="string">"b."</span>)</span><br><span class="line">plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">15</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_14_0.png" alt="png"></p>
<p>Let’s compute $\hat{\theta}$ using $(X’X)^{-1}\cdot(X’y)$. Command inv() inverts a matrix. First we add constant term to the X, which will have two features ($X_1$ and constant). $X = X[1,X_1]$. In Python matrix transposition is <font color="blue">X_b.T<font color="black"> $=X’$, matrix multuplication $X’X$ = <font color="blue">X_b.T.dot.X_b<font color="black"></font></font></font></font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_b = np.c_[np.ones((<span class="number">100</span>, <span class="number">1</span>)), X]  <span class="comment"># add x0 = 1 to each instance</span></span><br><span class="line">theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)</span><br><span class="line">theta_best</span><br></pre></td></tr></table></figure>

<pre><code>array([[4.21509616],
       [2.77011339]])</code></pre><p>Actual function to generate noise was $y = 4 + 3x + u$, where $u$ is the Gaussian noise. If we were to have a lot of data we would have estimated $\theta_0 = 4$ and $\theta_1 = 3$, instead we estimate $\theta_0 = 4.21$ and $\theta_1 = 2.77$. We can see how adding more observations moves us closer to the desired parameters. We are getting closer to $[4,3]$ as $n \rightarrow \infty$ <br></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># Define function to estimate OLS as with different number of observations</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">OLS</span><span class="params">(n)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">42</span>)</span><br><span class="line">    X = <span class="number">2</span> * np.random.rand(n, <span class="number">1</span>)</span><br><span class="line">    y = <span class="number">4</span> + <span class="number">3</span> * X + np.random.randn(n, <span class="number">1</span>)</span><br><span class="line">    X_b = np.c_[np.ones((n, <span class="number">1</span>)), X]  <span class="comment"># add x0 = 1 to each instance</span></span><br><span class="line">    theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)</span><br><span class="line">    <span class="keyword">return</span> theta_best </span><br><span class="line">theta_best = OLS(<span class="number">100</span>)</span><br><span class="line">print(<span class="string">"n= 100"</span>, <span class="string">"theta0 ="</span> ,theta_best[<span class="number">0</span>] , <span class="string">"theta1 ="</span>, theta_best[<span class="number">1</span>] )</span><br><span class="line">theta_best = OLS(<span class="number">1000</span>)</span><br><span class="line">print(<span class="string">"n= 1000"</span>, <span class="string">"theta0 ="</span> ,theta_best[<span class="number">0</span>] , <span class="string">"theta1 ="</span>, theta_best[<span class="number">1</span>] )</span><br><span class="line">theta_best = OLS(<span class="number">10000</span>)</span><br><span class="line">print(<span class="string">"n= 10000"</span>, <span class="string">"theta0 ="</span> ,theta_best[<span class="number">0</span>] , <span class="string">"theta1 ="</span>, theta_best[<span class="number">1</span>] )</span><br></pre></td></tr></table></figure>

<pre><code>n= 100 theta0 = [4.21509616] theta1 = [2.77011339]
n= 1000 theta0 = [4.17478026] theta1 = [2.92260742]
n= 10000 theta0 = [4.03177675] theta1 = [2.98034911]</code></pre><p>We can predict y for the values of x = (0,2), extreme value of x to plot the regression function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)</span><br><span class="line"><span class="comment"># Extereme value of X</span></span><br><span class="line">X_new = np.array([[<span class="number">0</span>], [<span class="number">2</span>]])</span><br><span class="line">X_new_b = np.c_[np.ones((<span class="number">2</span>, <span class="number">1</span>)), X_new]  <span class="comment"># add x0 = 1 to each instance</span></span><br><span class="line"><span class="comment"># Extreme value for y, knowing both bounds of X and Y will allow us to draw a line between them.</span></span><br><span class="line">y_predict = X_new_b.dot(theta_best)</span><br><span class="line">y_predict</span><br></pre></td></tr></table></figure>

<pre><code>array([[4.21509616],
       [9.75532293]])</code></pre><p>Plot the regression line using extreme values.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(X_new, y_predict, <span class="string">"r-"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Predictions"</span>)</span><br><span class="line">plt.plot(X, y, <span class="string">"b."</span>)</span><br><span class="line">plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">15</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_22_0.png" alt="png"></p>
<p>We can simply use prepackaged Linear Regression model from sklearn.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X, y)</span><br><span class="line">lin_reg.intercept_, lin_reg.coef_</span><br></pre></td></tr></table></figure>

<pre><code>(array([4.21509616]), array([[2.77011339]]))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lin_reg.predict(X_new)</span><br></pre></td></tr></table></figure>

<pre><code>array([[4.21509616],
       [9.75532293]])</code></pre><p>The LinearRegression class is based on the scipy.linalg.lstsq() function (the name stands for “least squares”), which you could call directly:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=<span class="number">1e-6</span>)</span><br><span class="line">theta_best_svd</span><br></pre></td></tr></table></figure>

<pre><code>array([[4.21509616],
       [2.77011339]])</code></pre><p>This function computes $X^{+}y$, where $X^+$ is the pseudoinverse of $X$ (specifically the Moore-Penrose inverse). You can use np.linalg.pinv() to compute the pseudoinverse directly:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.linalg.pinv(X_b).dot(y)</span><br></pre></td></tr></table></figure>

<pre><code>array([[4.21509616],
       [2.77011339]])</code></pre><p>If we have complete linear dependence, the classication regression will fail, but SVD will not.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">Q = <span class="number">2</span> * np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">W = <span class="number">4</span> + <span class="number">3</span> * Q + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">Q_b = np.c_[np.ones((<span class="number">100</span>, <span class="number">1</span>)), Q, Q]  </span><br><span class="line">theta_best = np.linalg.inv(Q_b.T.dot(Q_b)).dot(Q_b.T).dot(W)</span><br><span class="line">print(theta_best)</span><br></pre></td></tr></table></figure>

<pre><code>[[-1008.14178262]
 [ 2039.99368946]
 [-1544.3488403 ]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.linalg.pinv(Q_b).dot(W)</span><br></pre></td></tr></table></figure>

<pre><code>array([[4.21509616],
       [1.38505669],
       [1.38505669]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lin_reg.fit(Q_b, W)</span><br><span class="line">lin_reg.intercept_, lin_reg.coef_</span><br></pre></td></tr></table></figure>

<pre><code>(array([4.21509616]), array([[0.        , 1.38505669, 1.38505669]]))</code></pre><h1 id="Linear-regression-using-batch-gradient-descent"><a href="#Linear-regression-using-batch-gradient-descent" class="headerlink" title="Linear regression using batch gradient descent"></a>Linear regression using batch gradient descent</h1><p>The Normal Equation computes the inverse of $X^T · X$, which is an $n × n$ matrix (where n is the number of features). The computational complexity of inverting such a matrix is typically about O$(n^{2.4})$ to $O(n^3)$ (depending on the implementation). In other words, if you double the number of features, you multiply the computation time by roughly $2^{2.4} = 5.3$ to $2^3 = 8$.</p>
<ul>
<li>However, the question is linear to the number of instances (observations) $O(m)$, so we can handle large datasets if they fit in the memory. </li>
<li>Once the coefficients are estimated, predictions are fast to make, the complexity is linear with respect to both features and instances, twice as many features will take twice the time. <br></li>
</ul>
<p>Next we will look how to estimate linear with very large datasets</p>
<p>Gradient descent minimizes a cost function.  Starting with several random numbers, you try steps in different directions testing if a step minimizes a cost function. We stop the algorithm converges – reduction in cost function stopped. (picture).</p>
<ul>
<li>An important parameter in Gradient Descent is the size of the steps, determined by the learning rate hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time.</li>
<li>If the learning rate is too high, you might jump across the valley and end up on the other side, possibly even higher up than you were before. (picture)</li>
<li>Real-life cost function is complex and multi-dimensional. Starting point, direction of change, and the step-size can all lead to success or failure of optimization (picture). </li>
</ul>
<p>MSE function of linear regression is convininent for optimization. It is:</p>
<ul>
<li>Convex – if you pick any two points on the curve, the line segment joining them never crosses the curve). Hence, there are not local minima, only global minimum.</li>
<li>Lipschitz Continuous – the slope does not change very rapidly, i.e. the derivative is bounded a by a real number: $\mid  f(x_{1}) - f(x_{2}) \mid  \leq K\mid x_{1} - x_{2}\mid$, where $K \in \mathbf{R}$</li>
</ul>
<p>These two facts have a great consequence: Gradient Descent is guaranteed to approach arbitrarily close the global minimum (if you wait long enough and if the learning rate is not too high).</p>
<p>Regression with two parameters looks like a bowl, where need to find a minimum. If the features are scaled the bowl is round, and we will reach the bottom relatively quickly. If the features are not scaled different scales confuse the algorithm into thinking that features are important than others. <br></p>
<p>Training a model means searching for a combination of model parameters that minimizes a cost function. It is a search in the model’s parameter space: the more parameters a model has, the more dimensions this space has, and the harder the search is. Fortunately, since the cost function is convex in the case of Linear Regression, the needle is simply at the bottom of the bowl, so there is only one minimum point.</p>
<h1 id="Batch-Gradient-Descent"><a href="#Batch-Gradient-Descent" class="headerlink" title="Batch Gradient Descent"></a>Batch Gradient Descent</h1><p>To implement Gradient Descent, we compute the gradient of the cost function with regards to each model parameter $θ<em>j$. In other words, you need to calculate how much the cost function will change if you change θj just a little bit.This is called a partial derivative. (Ex. mountain slopes facing different directions). Next we compute the partial derivative of the cost function with regards to parameter $\theta_j$, noted $\frac{\partial MSE(\theta)}{\partial \theta_j}$.<br>$$<br>\frac{\partial MSE(\theta)}{\partial \theta_j} = \frac{2}{m}\sum\limits</em>{i=1}^m \big(\theta^{T} \cdot x^{(i)} - y^{(i)}\big)x^{(i)}<br>$$ <br><br>We estimate a vector of gradients in one go. The gradient vector, noted $\nabla\theta MSE(\theta)$, contains all the partial derivatives of the cost function (one for each model parameter):<br>$$<br>    \nabla\theta MSE(\theta)               \begin{cases}<br>                  \frac{\partial MSE(\theta)}{\partial \theta_0}\<br>                  \frac{\partial MSE(\theta)}{\partial \theta_1}\<br>                 \vdots \<br>                  \frac{\partial MSE(\theta)}{\partial \theta_n}\<br>                \end{cases}<br>$$<br>Notice that this formula involves calculations over the full training set X, at each Gradient Descent step! This is why the algorithm is called Batch Gradient Descent: it uses the whole batch of training data at every step. As a result it is terribly slow on very large training sets. However, Gradient Descent scales well with the number of features; training a Linear Regression model when there are hundreds of thousands of features is much faster using Gradient Descent than using the Normal Equation.</p>
<p>Once you have the gradient vector, which points uphill, just go in the opposite direction. This means subtracting $\nabla\theta MSE(\theta)$ from $\theta$. This is where the learning rate $\eta$ comes into play. multiply the gradient vector by $\eta$ to determine the size of the downhill step:<br>$$<br>\theta^{next :step} = \theta - \eta \nabla\theta MSE(\theta)<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set step at 0.1</span></span><br><span class="line">eta = <span class="number">0.1</span></span><br><span class="line"><span class="comment"># number of steps</span></span><br><span class="line">n_iterations = <span class="number">1000</span></span><br><span class="line"><span class="comment"># number of observations</span></span><br><span class="line">m = <span class="number">100</span></span><br><span class="line"><span class="comment"># randomly set the starting point</span></span><br><span class="line">theta = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Walk 1000 steps. </span></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">    gradients = <span class="number">2</span>/m * X_b.T.dot(X_b.dot(theta) - y)</span><br><span class="line">    theta = theta - eta * gradients</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theta</span><br></pre></td></tr></table></figure>

<pre><code>array([[4.21509616],
       [2.77011339]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is what we got from Matrix OLS.</span></span><br><span class="line">theta_best = OLS(<span class="number">100</span>)</span><br><span class="line">print(<span class="string">"n= 100"</span>, <span class="string">"theta0 ="</span> ,theta_best[<span class="number">0</span>] , <span class="string">"theta1 ="</span>, theta_best[<span class="number">1</span>] )</span><br></pre></td></tr></table></figure>

<pre><code>n= 100 theta0 = [4.21509616] theta1 = [2.77011339]</code></pre><p>Let’s look at the first 10 steps of gradient descent algorithms with different step sizes.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set a path vector</span></span><br><span class="line">theta_path_bgd = []</span><br><span class="line"><span class="comment"># program that plots gradient descent graph</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_gradient_descent</span><span class="params">(theta, eta, theta_path=None)</span>:</span></span><br><span class="line">    <span class="comment"># Number of observations m</span></span><br><span class="line">    m = len(X_b)</span><br><span class="line">    <span class="comment"># Plot target and features in scatter plot</span></span><br><span class="line">    plt.plot(X, y, <span class="string">"b."</span>)</span><br><span class="line">    <span class="comment"># set number of iterations</span></span><br><span class="line">    n_iterations = <span class="number">1000</span></span><br><span class="line">    <span class="comment"># loop over the iterations</span></span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">        theta_path_bgd.append(theta)</span><br><span class="line">    <span class="comment"># plot first 10 iterations</span></span><br><span class="line">        <span class="keyword">if</span> iteration &lt; <span class="number">10</span>:</span><br><span class="line">            <span class="comment"># preduct y using existing theta (starting values)</span></span><br><span class="line">            y_predict = X_new_b.dot(theta)</span><br><span class="line">            <span class="comment"># Plot first iteration with red, others in blue color</span></span><br><span class="line">            style = <span class="string">"b-"</span> <span class="keyword">if</span> iteration &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">"r--"</span></span><br><span class="line">            <span class="comment"># Plot regression line predicting y</span></span><br><span class="line">            plt.plot(X_new, y_predict, style)</span><br><span class="line">        <span class="comment"># Calculate gradient using formula above.</span></span><br><span class="line">        gradients = <span class="number">2</span>/m * X_b.T.dot(X_b.dot(theta) - y)</span><br><span class="line">        <span class="comment"># Update theta using the gradient</span></span><br><span class="line">        theta = theta - eta * gradients </span><br><span class="line">        <span class="comment">#Save estimate thetas in the vector theta_path</span></span><br><span class="line">    plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">15</span>])</span><br><span class="line">    plt.title(<span class="string">r"$\eta = &#123;&#125;$"</span>.format(eta), fontsize=<span class="number">16</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">theta = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)  <span class="comment"># random initialization</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">4</span>))</span><br><span class="line">plt.subplot(<span class="number">131</span>); plot_gradient_descent(theta, eta=<span class="number">0.02</span>)</span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.subplot(<span class="number">132</span>); plot_gradient_descent(theta, eta=<span class="number">0.1</span>, theta_path=theta_path_bgd)</span><br><span class="line">plt.subplot(<span class="number">133</span>); plot_gradient_descent(theta, eta=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_46_0.png" alt="png"></p>
<ul>
<li>On the left, the learning rate is too low: the algorithm will eventually reach the solution, but it will take a long time. </li>
<li>In the middle, the learning rate looks pretty good: in just a few iterations, it has already converged to the solution</li>
<li>On the right, the learning rate is too high: the algorithm diverges, jumping all over the place and actually getting further and further away from the solution at every step</li>
</ul>
<p>If you have enough time it is always better to use small steps and a lot of iterations. If the data is too big for that, then you need to experiment using grid search. <br></p>
<p>How to set the number of iterations? <br><br>If you set it too small, your may not find the minimum, if it is too large you may waste time without improving the fit. A good ways it set minimum gradient that would continue the iterations. If the rate of improvement is less than the minimum tolerance $\epsilon$, it is a good idea to stop the iterations.  </p>
<p>When the cost function is convex and its slope does not change abruptly (as is the case for the MSE cost function), the Batch Gradient Descent with a fixed learning rate has a convergence rate of $O = \frac{1}{Iterations}$. In other words, if you divide the tolerance $\epsilon$ by 10 (to have a more precise solution), then the algorithm will have to run about 10 times more iterations.</p>
<h1 id="Stochastic-Gradient-Descent-随机梯度下降"><a href="#Stochastic-Gradient-Descent-随机梯度下降" class="headerlink" title="Stochastic Gradient Descent 随机梯度下降"></a>Stochastic Gradient Descent 随机梯度下降</h1><ul>
<li>Batch Gradient Descent uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large.</li>
<li>Stochastic Gradient Descent (SGD) just picks a random instance in the training set at every step and computes the gradients based only on that single instance. This makes the algorithm much faster since it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration.</li>
<li>SGD, due to its stochastic (i.e., random) nature, is much less regular than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down . So once the algorithm stops, the final parameter values are good, but not optimal. (picture)</li>
</ul>
<p>Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then shuffle it again, and so on. However, this generally converges more slowly.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set path array</span></span><br><span class="line">theta_path_sgd = []</span><br><span class="line">m = len(X_b)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># number of iterations</span></span><br><span class="line">n_epochs = <span class="number">50</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">50</span>  <span class="comment"># learning schedule hyperparameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learning_schedule</span><span class="params">(t)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> t0 / (t + t1)</span><br><span class="line"><span class="comment"># start with randomly generated starting parameters theta</span></span><br><span class="line">theta = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)  <span class="comment"># random initialization</span></span><br><span class="line"><span class="comment"># loop over iterations</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">    <span class="comment"># add theta to the vector</span></span><br><span class="line">    theta_path_sgd.append(theta)                 <span class="comment"># not shown</span></span><br><span class="line">    <span class="comment"># loop over observations. Here we loop over all observations, we can just had a few draw, like range(20)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment"># show regression lines for the first 20 iterations</span></span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">0</span> <span class="keyword">and</span> i &lt; <span class="number">20</span>:                    <span class="comment"># not shown in the book</span></span><br><span class="line">            <span class="comment"># Predict y using existing theta</span></span><br><span class="line">            y_predict = X_new_b.dot(theta)           <span class="comment"># not shown</span></span><br><span class="line">            <span class="comment"># first line is red</span></span><br><span class="line">            style = <span class="string">"b-"</span> <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">"r--"</span>         <span class="comment"># not shown</span></span><br><span class="line">            <span class="comment">#other lines are blue. Plot predictions</span></span><br><span class="line">            plt.plot(X_new, y_predict, style)        <span class="comment"># not shown</span></span><br><span class="line">        <span class="comment"># Randomly select number between 1 and m.</span></span><br><span class="line">        random_index = np.random.randint(m)</span><br><span class="line">        <span class="comment"># extract randomly selected observation for x</span></span><br><span class="line">        xi = X_b[random_index:random_index+<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># extract randomly selected observation for y</span></span><br><span class="line">        yi = y[random_index:random_index+<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># Calculate gradient for randomly selected observations</span></span><br><span class="line">        gradients = <span class="number">2</span> * xi.T.dot(xi.dot(theta) - yi)</span><br><span class="line">        <span class="comment"># Tolerance level, is 50 / (epoch * m + i + 5)</span></span><br><span class="line">        eta = learning_schedule(epoch * m + i)</span><br><span class="line">        <span class="comment"># update theta</span></span><br><span class="line">        theta = theta - eta * gradients</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.plot(X, y, <span class="string">"b."</span>)                                 <span class="comment"># not shown</span></span><br><span class="line">plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)                     <span class="comment"># not shown</span></span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)           <span class="comment"># not shown</span></span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">15</span>])                              <span class="comment"># not shown</span></span><br><span class="line">plt.show()                                           <span class="comment"># not shown</span></span><br></pre></td></tr></table></figure>

<p><img src="output_53_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">thetapath = np.array(theta_path_sgd)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">thetapath[<span class="number">49</span>,:]</span><br></pre></td></tr></table></figure>

<pre><code>array([[4.21200431],
       [2.74968529]])</code></pre><p>Remember that correct values are $[4.21509616, 2.77011339]$, we are very close</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># First 20 steps of iteration 1</span></span><br><span class="line">plt.plot(thetapath[<span class="number">0</span>:<span class="number">20</span>,<span class="number">0</span>],thetapath[<span class="number">0</span>:<span class="number">20</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[&lt;matplotlib.lines.Line2D at 0x248543aa358&gt;]</code></pre><p><img src="output_58_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#40-50 steps</span></span><br><span class="line">plt.plot(thetapath[<span class="number">40</span>:<span class="number">49</span>,<span class="number">0</span>],thetapath[<span class="number">40</span>:<span class="number">49</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[&lt;matplotlib.lines.Line2D at 0x24854424d30&gt;]</code></pre><p><img src="output_59_1.png" alt="png"></p>
<p>We are close, but we are not really converging to the correct value. Next we estimate a SGD regression included in sklearn. Number of iterations is 50. The defaults to optimizing the squared error cost function. We run 50 epochs, starting with a learning rate of 0.1 (eta0=0.1), using the default learning schedule (different from the preceding one), and it does not use any regularization (penalty=None; more details on this shortly):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line">sgd_reg = SGDRegressor(max_iter=<span class="number">50</span>, penalty=<span class="literal">None</span>,  eta0 = <span class="number">0.1</span>,  random_state=<span class="number">42</span>)</span><br><span class="line">sgd_reg.fit(X, y.ravel())</span><br><span class="line">sgd_reg.intercept_, sgd_reg.coef_</span><br></pre></td></tr></table></figure>

<pre><code>(array([4.24365286]), array([2.8250878]))</code></pre><p>Not great, let’s increase the number of iterations</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line">sgd_reg = SGDRegressor(max_iter=<span class="number">500</span>, penalty=<span class="literal">None</span>, eta0=<span class="number">0.1</span>,random_state=<span class="number">42</span>)</span><br><span class="line">sgd_reg.fit(X, y.ravel())</span><br><span class="line">sgd_reg.intercept_, sgd_reg.coef_</span><br></pre></td></tr></table></figure>

<pre><code>(array([4.24365286]), array([2.8250878]))</code></pre><p>Much better. Alternatively we can reduce tolerance 10 times.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line">sgd_reg = SGDRegressor(max_iter=<span class="number">5000</span>, penalty=<span class="literal">None</span>, eta0=<span class="number">0.1</span>,random_state=<span class="number">42</span>)</span><br><span class="line">sgd_reg.fit(X, y.ravel())</span><br><span class="line">sgd_reg.intercept_, sgd_reg.coef_</span><br></pre></td></tr></table></figure>

<pre><code>(array([4.24365286]), array([2.8250878]))</code></pre><p>The results got worse we are dancing aroung correct value.</p>
<h1 id="Mini-batch-gradient-descent"><a href="#Mini-batch-gradient-descent" class="headerlink" title="Mini-batch gradient descent"></a>Mini-batch gradient descent</h1><p>Instead of computing the gradients based on the full training set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD computes the gradients on small random sets of instances called mini-batches. The main advantage of Mini-batch GD over Stochastic GD is that you can get a performance boost from hardware optimization of matrix operations, especially when using GPUs.</p>
<p>The algorithm’s progress in parameter space is less erratic than with SGD, especially with fairly large mini-batches. As a result, Mini-batch GD will end up walking around a bit closer to the minimum than SGD. But, on the other hand, it may be harder for it to escape from local minima (in the case of problems that suffer from local minima, unlike Linear Regression as we saw earlier). Next we will show the paths taken by the three Gradient Descent algorithms in parameter space during training. They all end up near the minimum, but Batch GD’s path actually stops at the minimum, while both Stochastic GD and Mini-batch GD continue to walk around. However, the Batch GD takes a lot of time to take each step, and Stochastic GD and Mini-batch GD would also reach the minimum if you used a good learning schedule.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">theta_path_mgd = []</span><br><span class="line"></span><br><span class="line">n_iterations = <span class="number">50</span></span><br><span class="line"><span class="comment"># We will just use 20 observations out of 100</span></span><br><span class="line">minibatch_size = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">theta = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)  <span class="comment"># random initialization</span></span><br><span class="line"></span><br><span class="line">t0, t1 = <span class="number">200</span>, <span class="number">1000</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learning_schedule</span><span class="params">(t)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> t0 / (t + t1)</span><br><span class="line"></span><br><span class="line">t = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">    <span class="comment"># Shuffle the data x and y. Because we will draw a series of 20 observation order now can matter a great deal. Before we</span></span><br><span class="line">    <span class="comment"># were just </span></span><br><span class="line">    shuffled_indices = np.random.permutation(m)</span><br><span class="line">    X_b_shuffled = X_b[shuffled_indices]</span><br><span class="line">    y_shuffled = y[shuffled_indices]</span><br><span class="line">    <span class="comment"># Loop each batch of observations from 0 to 20</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, m, minibatch_size):</span><br><span class="line">        <span class="comment"># set counter t</span></span><br><span class="line">        t += <span class="number">1</span></span><br><span class="line">        <span class="comment"># Get first of observation of the batch</span></span><br><span class="line">        xi = X_b_shuffled[i:i+minibatch_size]</span><br><span class="line">        yi = y_shuffled[i:i+minibatch_size]</span><br><span class="line">        <span class="comment"># calculate the gradient</span></span><br><span class="line">        gradients = <span class="number">2</span>/minibatch_size * xi.T.dot(xi.dot(theta) - yi)</span><br><span class="line">        <span class="comment"># calculate new step</span></span><br><span class="line">        eta = learning_schedule(t)</span><br><span class="line">        <span class="comment"># update theta</span></span><br><span class="line">        theta = theta - eta * gradients</span><br><span class="line">        <span class="comment"># record the path</span></span><br><span class="line">        theta_path_mgd.append(theta)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"final theta ="</span>,theta)</span><br></pre></td></tr></table></figure>

<pre><code>final theta = [[4.25214635]
 [2.7896408 ]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Lets create vetors for our paths</span></span><br><span class="line">theta_path_bgd = np.array(theta_path_bgd)</span><br><span class="line">theta_path_sgd = np.array(theta_path_sgd)</span><br><span class="line">theta_path_mgd = np.array(theta_path_mgd)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">7</span>,<span class="number">4</span>))</span><br><span class="line">plt.plot(theta_path_sgd[:, <span class="number">0</span>], theta_path_sgd[:, <span class="number">1</span>], <span class="string">"r-s"</span>, linewidth=<span class="number">1</span>, label=<span class="string">"Stochastic"</span>)</span><br><span class="line">plt.plot(theta_path_mgd[:, <span class="number">0</span>], theta_path_mgd[:, <span class="number">1</span>], <span class="string">"g-+"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Mini-batch"</span>)</span><br><span class="line"><span class="comment"># We called this function 3 times, so the vector has 3000 obs instead of 1000. We just use the first 1000</span></span><br><span class="line">plt.plot(theta_path_bgd[:<span class="number">1000</span>, <span class="number">0</span>], theta_path_bgd[:<span class="number">1000</span>, <span class="number">1</span>], <span class="string">"b-o"</span>, linewidth=<span class="number">3</span>, label=<span class="string">"Batch"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xlabel(<span class="string">r"$\theta_0$"</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.ylabel(<span class="string">r"$\theta_1$   "</span>, fontsize=<span class="number">20</span>, rotation=<span class="number">0</span>)</span><br><span class="line">plt.axis([<span class="number">2.5</span>, <span class="number">4.5</span>, <span class="number">2.3</span>, <span class="number">3.9</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_73_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot last 50 steps</span></span><br><span class="line">plt.figure(figsize=(<span class="number">7</span>,<span class="number">4</span>))</span><br><span class="line">plt.plot(theta_path_sgd[<span class="number">-50</span>:, <span class="number">0</span>], theta_path_sgd[<span class="number">-50</span>:, <span class="number">1</span>], <span class="string">"r-s"</span>, linewidth=<span class="number">1</span>, label=<span class="string">"Stochastic"</span>)</span><br><span class="line">plt.plot(theta_path_mgd[<span class="number">-50</span>:, <span class="number">0</span>], theta_path_mgd[<span class="number">-50</span>:, <span class="number">1</span>], <span class="string">"g-+"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Mini-batch"</span>)</span><br><span class="line"><span class="comment"># We called this function 3 times, so the vector has 3000 obs instead of 1000. We just use the first 1000</span></span><br><span class="line">plt.plot(theta_path_bgd[<span class="number">950</span>:<span class="number">1000</span>, <span class="number">0</span>], theta_path_bgd[<span class="number">950</span>:<span class="number">1000</span>, <span class="number">1</span>], <span class="string">"b-o"</span>, linewidth=<span class="number">3</span>, label=<span class="string">"Batch"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xlabel(<span class="string">r"$\theta_0$"</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.ylabel(<span class="string">r"$\theta_1$   "</span>, fontsize=<span class="number">20</span>, rotation=<span class="number">0</span>)</span><br><span class="line">plt.axis([<span class="number">2.5</span>, <span class="number">4.5</span>, <span class="number">2.3</span>, <span class="number">3.9</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_74_0.png" alt="png"></p>
<h1 id="Polynomial-regression"><a href="#Polynomial-regression" class="headerlink" title="Polynomial regression"></a>Polynomial regression</h1><p>What if your data is actually more complex than a simple straight line? A simple way to use Linear model for non-linear data is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called Polynomial Regression.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> numpy.random <span class="keyword">as</span> rnd</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<p>X is 100 observations draws from $X = 6*R - 3$, where $R \in (0,1)$. Then we estimate $Y = 0.5X^2 + X + 2 + R$.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = <span class="number">100</span></span><br><span class="line">X = <span class="number">6</span> * np.random.rand(m, <span class="number">1</span>) - <span class="number">3</span></span><br><span class="line">y = <span class="number">0.5</span> * X**<span class="number">2</span> + X + <span class="number">2</span> + np.random.randn(m, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>Plot X and Y</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(X, y, <span class="string">"b."</span>)</span><br><span class="line">plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">10</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_81_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import package to craeate polinomial features</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line">poly_features = PolynomialFeatures(degree=<span class="number">2</span>, include_bias=<span class="literal">False</span>)</span><br><span class="line">X_poly = poly_features.fit_transform(X)</span><br><span class="line">X[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<pre><code>array([-0.75275929])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(X_poly[<span class="number">0</span>])</span><br><span class="line">print((<span class="number">-0.75275929</span>)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>[-0.75275929  0.56664654]
0.566646548681304</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fit polinomial regression</span></span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X_poly, y)</span><br><span class="line">lin_reg.intercept_, lin_reg.coef_</span><br></pre></td></tr></table></figure>

<pre><code>(array([1.78134581]), array([[0.93366893, 0.56456263]]))</code></pre><p>The model estimates $\hat{Y} = 0.56X^2 + 0.93X + 1.78 + R$, whereas a real model is $Y = 0.5X^2 + X + 2 + R$. Polinomial regresion is capable of finding relationships<br>between features by addint all combinations of features up to the given degree. For<br>example, if there were two features $a$ and $b$, PolynomialFeatures with degree=3 would not only add<br>the features $a^2$, $a^3$, $b^2$, and $b^3$, but also the combinations $ab$, $a^{2}b$, and $ab^2$. Polynomial features models with a degree $d$ and $n$ features, would produce $\frac{(n+d)!}{d!n!}$ combinations of features.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generate space of drawing a solid prediction line.</span></span><br><span class="line">X_new=np.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">100</span>).reshape(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># square the X</span></span><br><span class="line">X_new_poly = poly_features.transform(X_new)</span><br><span class="line">y_new = lin_reg.predict(X_new_poly)</span><br><span class="line"><span class="comment"># Draw scattered data</span></span><br><span class="line">plt.plot(X, y, <span class="string">"b."</span>)</span><br><span class="line"><span class="comment"># Draw prediction line</span></span><br><span class="line">plt.plot(X_new, y_new, <span class="string">"r-"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Predictions"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">10</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_86_0.png" alt="png"></p>
<p>High-degree Polynomial Regression fits the training data much better than the plain Linear Regression. Next we estimate a 300-degree polynomial model to the preceding training data, and compare the result with a pure linear model and a quadratic model (2nd degree polynomial).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> style, width, degree <span class="keyword">in</span> ((<span class="string">"g-"</span>, <span class="number">1</span>, <span class="number">300</span>), (<span class="string">"b--"</span>, <span class="number">2</span>, <span class="number">2</span>), (<span class="string">"r-+"</span>, <span class="number">2</span>, <span class="number">1</span>)):</span><br><span class="line">    polybig_features = PolynomialFeatures(degree=degree, include_bias=<span class="literal">False</span>)</span><br><span class="line">    std_scaler = StandardScaler()</span><br><span class="line">    lin_reg = LinearRegression()</span><br><span class="line">    polynomial_regression = Pipeline([</span><br><span class="line">            (<span class="string">"poly_features"</span>, polybig_features),</span><br><span class="line">            (<span class="string">"std_scaler"</span>, std_scaler),</span><br><span class="line">            (<span class="string">"lin_reg"</span>, lin_reg),</span><br><span class="line">        ])</span><br><span class="line">    polynomial_regression.fit(X, y)</span><br><span class="line">    y_newbig = polynomial_regression.predict(X_new)</span><br><span class="line">    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)</span><br><span class="line"></span><br><span class="line">plt.plot(X, y, <span class="string">"b."</span>, linewidth=<span class="number">3</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">10</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_88_0.png" alt="png"></p>
<ul>
<li>This high-degree Polynomial Regression model is severely overfitting the training data, while<br>the linear model is underfitting it. The model that will generalize best in this case is the quadratic model.</li>
<li>The data was generated using a quadratic model, but in general you won’t know what<br>function generated the data. In Chapter 2 you used cross-validation to get an estimate of a model’s generalization performance.</li>
</ul>
<p>Another way is to look at the learning curves: these are plots of the model’s performance on the training<br>set and the validation set as a function of the training set size. To generate the plots, simply train the model<br>several times on different sized subsets of the training set. The following code defines a function that plots<br>the learning curves of a model given some training data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MS and train-test split</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curves</span><span class="params">(model, X, y)</span>:</span></span><br><span class="line">    <span class="comment"># Split data into test and train</span></span><br><span class="line">    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">10</span>)</span><br><span class="line">    <span class="comment"># create vectors to save errors</span></span><br><span class="line">    train_errors, val_errors = [], []</span><br><span class="line">    <span class="comment"># loop through all observations adding one at a time</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(<span class="number">1</span>, len(X_train)):</span><br><span class="line">        <span class="comment"># fit the model</span></span><br><span class="line">        model.fit(X_train[:m], y_train[:m])</span><br><span class="line">        <span class="comment"># predict y on training</span></span><br><span class="line">        y_train_predict = model.predict(X_train[:m])</span><br><span class="line">        <span class="comment"># predict on validation</span></span><br><span class="line">        y_val_predict = model.predict(X_val)</span><br><span class="line">        <span class="comment"># calculate training and validation errors</span></span><br><span class="line">        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))</span><br><span class="line">        val_errors.append(mean_squared_error(y_val, y_val_predict))</span><br><span class="line"><span class="comment"># plot errors and sample size</span></span><br><span class="line">    plt.plot(np.sqrt(train_errors), <span class="string">"r-+"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"train"</span>)</span><br><span class="line">    plt.plot(np.sqrt(val_errors), <span class="string">"b-"</span>, linewidth=<span class="number">3</span>, label=<span class="string">"val"</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">"upper right"</span>, fontsize=<span class="number">14</span>)   <span class="comment"># not shown in the book</span></span><br><span class="line">    plt.xlabel(<span class="string">"Training set size"</span>, fontsize=<span class="number">14</span>) <span class="comment"># not shown</span></span><br><span class="line">    plt.ylabel(<span class="string">"RMSE"</span>, fontsize=<span class="number">14</span>)              <span class="comment"># not shown</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set large data</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">m = <span class="number">100</span></span><br><span class="line">X = <span class="number">6</span> * np.random.rand(m, <span class="number">1</span>) - <span class="number">3</span></span><br><span class="line">y = <span class="number">0.5</span> * X**<span class="number">2</span> + X + <span class="number">2</span> + np.random.randn(m, <span class="number">1</span>)</span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">plot_learning_curves(lin_reg, X, y)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">80</span>, <span class="number">0</span>, <span class="number">3</span>])                         <span class="comment"># not shown in the book</span></span><br><span class="line">plt.show()                                      <span class="comment"># not shown</span></span><br></pre></td></tr></table></figure>

<p><img src="output_91_0.png" alt="png"></p>
<p>When there are just one or two instances in the training set, the model can fit them perfectly, which is why the curve starts<br>at zero. But as new instances are added to the training set, it becomes impossible for the model to fit the<br>training data perfectly, both because the data is noisy and because it is not linear at all. So the error on the<br>training data goes up until it reaches a plateau, at which point adding new instances to the training set<br>doesn’t make the average error much better or worse. <br></p>
<p>When the model is trained on very few training instances, it is incapable of generalizing properly, which is why the validation error is initially quite big. Then as the model is shown more training examples, it learns and thus the validation error slowly goes down. However, once again a straight line cannot do a good job modeling the data, so the error ends up at a plateau, very close to the other curve. These learning curves are typical of an underfitting model. Both curves have reached a plateau; they are close and fairly high. <br></p>
<p>Now let’s look at the learning curves of a 10th-degree polynomial model on the same data.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line">polynomial_regression = Pipeline([</span><br><span class="line">        (<span class="string">"poly_features"</span>, PolynomialFeatures(degree=<span class="number">10</span>, include_bias=<span class="literal">False</span>)),</span><br><span class="line">        (<span class="string">"lin_reg"</span>, LinearRegression()),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">plot_learning_curves(polynomial_regression, X, y)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">80</span>, <span class="number">0</span>, <span class="number">3</span>])           <span class="comment"># not shown</span></span><br><span class="line">plt.show()                        <span class="comment"># not shown</span></span><br></pre></td></tr></table></figure>

<p><img src="output_93_0.png" alt="png"></p>
<p>These learning curves look a bit like the previous ones, but there are two very important differences: <br></p>
<ol>
<li>The error on the training data is much lower than with the Linear Regression model.</li>
<li>There is a persistent gap between the curves. This means that the model performs significantly better on the<br>training data than on the validation data, which is the hallmark of an overfitting model. However, if you used a much larger training set, the two curves would continue to get closer.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">polynomial_regression = Pipeline([</span><br><span class="line">        (<span class="string">"poly_features"</span>, PolynomialFeatures(degree=<span class="number">2</span>, include_bias=<span class="literal">False</span>)),</span><br><span class="line">        (<span class="string">"lin_reg"</span>, LinearRegression()),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">plot_learning_curves(polynomial_regression, X, y)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">80</span>, <span class="number">0</span>, <span class="number">3</span>])           <span class="comment"># not shown</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_95_0.png" alt="png"></p>
<p>Maybe we have too few obervations?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set large data</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">m = <span class="number">1000</span></span><br><span class="line">X = <span class="number">6</span> * np.random.rand(m, <span class="number">1</span>) - <span class="number">3</span></span><br><span class="line">y = <span class="number">0.5</span> * X**<span class="number">2</span> + X + <span class="number">2</span> + np.random.randn(m, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">plot_learning_curves(lin_reg, X, y)</span><br><span class="line">plt.axis([<span class="number">700</span>, <span class="number">800</span>, <span class="number">0</span>, <span class="number">3</span>])                         <span class="comment"># not shown in the book</span></span><br><span class="line">plt.show()                                      <span class="comment"># not shown</span></span><br></pre></td></tr></table></figure>

<p><img src="output_97_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">polynomial_regression = Pipeline([</span><br><span class="line">        (<span class="string">"poly_features"</span>, PolynomialFeatures(degree=<span class="number">10</span>, include_bias=<span class="literal">False</span>)),</span><br><span class="line">        (<span class="string">"lin_reg"</span>, LinearRegression()),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">plot_learning_curves(polynomial_regression, X, y)</span><br><span class="line">plt.axis([<span class="number">700</span>, <span class="number">800</span>, <span class="number">0</span>, <span class="number">3</span>])           <span class="comment"># not shown</span></span><br><span class="line">plt.show()                        <span class="comment"># not shown</span></span><br></pre></td></tr></table></figure>

<p><img src="output_98_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">polynomial_regression = Pipeline([</span><br><span class="line">        (<span class="string">"poly_features"</span>, PolynomialFeatures(degree=<span class="number">2</span>, include_bias=<span class="literal">False</span>)),</span><br><span class="line">        (<span class="string">"lin_reg"</span>, LinearRegression()),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">plot_learning_curves(polynomial_regression, X, y)</span><br><span class="line">plt.axis([<span class="number">700</span>, <span class="number">800</span>, <span class="number">0</span>, <span class="number">3</span>])           <span class="comment"># not shown</span></span><br><span class="line">plt.show()                        <span class="comment"># not shown</span></span><br></pre></td></tr></table></figure>

<p><img src="output_99_0.png" alt="png"></p>
<p>We fixed overfitting issues by using more data.</p>
<h1 id="Regularized-models"><a href="#Regularized-models" class="headerlink" title="Regularized models"></a>Regularized models</h1><p>A good way to reduce overfitting is to regularize the model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. For example, a simple way to regularize a polynomial model is to reduce the number of polynomial degrees. <br><br>For a linear model, regularization is typically achieved by constraining the weights of the model. We will<br>now look at <strong>Ridge Regression</strong>, <strong>Lasso Regression</strong>, and <strong>Elastic Net</strong>, which implement three different ways<br>to constrain the weights. <br></p>
<h1 id="Ridge-Regression"><a href="#Ridge-Regression" class="headerlink" title="Ridge Regression"></a>Ridge Regression</h1><p>Ridge Regression (also called Tikhonov regularization) is a regularized version of Linear Regression: a regularization term equal to is added to the cost function. This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to evaluate the model’s performance using the unregularized performance measure. <br></p>
<p>The hyperparameter $\alpha$ controls how much you want to regularize the model. If $\alpha = 0$ then Ridge Regression is just Linear Regression. If $\alpha$ is very large, then all weights end up very close to zero and the result is a flat line going through the data’s mean. Ridge regression cost function, where we add a sum of squared parameters.<br>$$ J(\theta) = MSE(\theta) + \alpha \frac{1}{2} \sum^m_{i=1} \theta^{2}_i$$</p>
<p>Note that the bias term $\theta_0$ is not regularized (the sum starts at $i = 1$, not $0$). If we define $w$ as the vector of<br>feature weights ($\theta_1$ to $\theta_n$), then the regularization term is simply equal to $\frac{1}{2}(\lVert w \rVert _2)^2$, where $\lVert . \rVert _2$ is teh $l_2$ norm of the weight vector. It is important to scale the data (e.g., using a StandardScaler) before performing Ridge Regression, as it is sensitive to the scale<br>of the input features. This is true of most regularized models. For Gradient Descent, just add $\alpha w$ to the MSE gradient<br>vector. <br></p>
<p>Next we will show several Ridge models trained on some linear data using different $\alpha$ value. On the left, plain Ridge models are used, leading to linear predictions. On the right, the data is first expanded using PolynomialFeatures(degree=10), then it is scaled using a StandardScaler, and finally the Ridge models are applied to the resulting features: this is Polynomial Regression with Ridge regularization. <br></p>
<p>Increasing $\alpha$ leads to flatter (i.e., less extreme, more reasonable) predictions; this reduces the<br>model’s variance but increases its bias.</p>
<p>As with Linear Regression, we can perform Ridge Regression either by computing a closed-form<br>equation or by performing Gradient Descent. The closed-form solution (where $A$ is the $n \times n$ identity matrix except with a 0 in the top-left cell, corresponding to the bias term).<br>$$ \hat{\theta} = ( X^T \cdot X  + \alpha A)^{-1} \cdot X^T \cdot y$$<br>Remember the regression without ridge it:<br>$$ \hat{\theta} = ( X^T \cdot X )^{-1} \cdot X^T \cdot y$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line">train_errors = []</span><br><span class="line">val_errors = []</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># Take 30 obs</span></span><br><span class="line">m = <span class="number">50</span></span><br><span class="line">X = <span class="number">3</span> * np.random.rand(m, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">1</span> + <span class="number">0.5</span> * X + np.random.randn(m, <span class="number">1</span>) / <span class="number">1.5</span></span><br><span class="line"></span><br><span class="line">X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X_new = np.linspace(<span class="number">0</span>, <span class="number">3</span>, <span class="number">100</span>).reshape(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_model</span><span class="params">(model_class, polynomial, alphas, **model_kargs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> alpha, style <span class="keyword">in</span> zip(alphas, (<span class="string">"b-"</span>, <span class="string">"g--"</span>, <span class="string">"r:"</span>)):</span><br><span class="line">        model = model_class(alpha, **model_kargs) <span class="keyword">if</span> alpha &gt; <span class="number">0</span> <span class="keyword">else</span> LinearRegression()</span><br><span class="line">        <span class="keyword">if</span> polynomial:</span><br><span class="line">            model = Pipeline([</span><br><span class="line">                    (<span class="string">"poly_features"</span>, PolynomialFeatures(degree=<span class="number">10</span>, include_bias=<span class="literal">False</span>)),</span><br><span class="line">                    (<span class="string">"std_scaler"</span>, StandardScaler()),</span><br><span class="line">                    (<span class="string">"regul_reg"</span>, model),</span><br><span class="line">                ])</span><br><span class="line">        model.fit(X_train, y_train)</span><br><span class="line">        print(<span class="string">"Coefficients from"</span>,model_class,<span class="string">"with alpha ="</span>,alpha )</span><br><span class="line">        <span class="keyword">if</span> polynomial: </span><br><span class="line">            classifier = model.named_steps[<span class="string">'regul_reg'</span>]</span><br><span class="line">            print(classifier.coef_)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(model.coef_)</span><br><span class="line">        y_new_regul = model.predict(X_new)</span><br><span class="line">        <span class="comment"># look at cross validation error</span></span><br><span class="line">        y_train_predict = model.predict(X_train[:m])</span><br><span class="line">        <span class="comment"># predict on validation</span></span><br><span class="line">        y_val_predict = model.predict(X_val)</span><br><span class="line">        <span class="comment"># calculate training and validation errors</span></span><br><span class="line">        train_errors.append(mean_squared_error(y_train, y_train_predict))</span><br><span class="line">        val_errors.append(mean_squared_error(y_val, y_val_predict))</span><br><span class="line">        lw = <span class="number">2</span> <span class="keyword">if</span> alpha &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=<span class="string">r"$\alpha = &#123;&#125;$"</span>.format(alpha))</span><br><span class="line">    plt.plot(X, y, <span class="string">"b."</span>, linewidth=<span class="number">3</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">"upper left"</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">4</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line"><span class="comment"># Plot ridge regression</span></span><br><span class="line">plot_model(Ridge, polynomial=<span class="literal">False</span>, alphas=(<span class="number">0</span>, <span class="number">10</span>, <span class="number">100</span>), random_state=<span class="number">42</span>)</span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">plot_model(Ridge, polynomial=<span class="literal">True</span>, alphas=(<span class="number">0</span>, <span class="number">10</span>**<span class="number">-5</span>, <span class="number">1</span>), random_state=<span class="number">42</span>)</span><br><span class="line">plt.show()</span><br><span class="line">print(<span class="string">"Training Errors Linear regression with Ridge"</span>,train_errors[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"Validation Errors Linear regression with Ridge"</span>,val_errors[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"Training Errors Polynomial regression with Ridge"</span>,train_errors[<span class="number">3</span>:])</span><br><span class="line">print(<span class="string">"Validation Errors Polinomial regression with Ridge"</span>,val_errors[<span class="number">3</span>:])</span><br></pre></td></tr></table></figure>

<pre><code>Coefficients from &lt;class &apos;sklearn.linear_model.ridge.Ridge&apos;&gt; with alpha = 0
[[0.4176916]]
Coefficients from &lt;class &apos;sklearn.linear_model.ridge.Ridge&apos;&gt; with alpha = 10
[[0.31349263]]
Coefficients from &lt;class &apos;sklearn.linear_model.ridge.Ridge&apos;&gt; with alpha = 100
[[0.09660269]]
Coefficients from &lt;class &apos;sklearn.linear_model.ridge.Ridge&apos;&gt; with alpha = 0
[[ 2.79676282e+01 -8.70301875e+02  1.02393993e+04 -6.14861326e+04
   2.14563424e+05 -4.60902057e+05  6.18449227e+05 -5.05246948e+05
   2.29908470e+05 -4.46826954e+04]]
Coefficients from &lt;class &apos;sklearn.linear_model.ridge.Ridge&apos;&gt; with alpha = 1e-05
[[  0.78077474   2.17580376 -15.03623374  26.89195816  -5.6119261
  -23.29096036  -4.96673855  21.51691618  22.82230017 -24.96631119]]
Coefficients from &lt;class &apos;sklearn.linear_model.ridge.Ridge&apos;&gt; with alpha = 1
[[ 0.32396213  0.04946447  0.02169417  0.04508521  0.05805455  0.04884026
   0.01942467 -0.02537665 -0.08092134 -0.14346268]]</code></pre><p><img src="output_104_1.png" alt="png"></p>
<pre><code>Training Errors Linear regression with Ridge [0.3850221633585592, 0.3931885655081954, 0.462567291650715]
Validation Errors Linear regression with Ridge [0.2931679786457559, 0.3258256737772605, 0.44095925341635367]
Training Errors Polynomial regression with Ridge [0.3252732506662376, 0.35973102994984707, 0.3788227343500468]
Validation Errors Polinomial regression with Ridge [0.28745132128253525, 0.2921066809021208, 0.2793681709990138]</code></pre><p>Ridge regression improves fit of complex polynomial model and worsens the fit of simple linear model.</p>
<h1 id="Lasso-Regression"><a href="#Lasso-Regression" class="headerlink" title="Lasso Regression"></a>Lasso Regression</h1><p>Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the $l_1$ norm of the weight vector instead of half the square of the $l_2$ norm.</p>
<p>$$ J(\theta) = MSE(\theta) + \alpha \frac{1}{2} \sum^m_{i=1} |\theta_i|$$</p>
<p>An important characteristic of Lasso Regression is that it tends to completely eliminate the weights of the<br>least important features (i.e., set them to zero). For example, the dashed line in the right plot on a figure below (with $\alpha$ = $10^{-7}$) looks quadratic, almost linear: all the weights for the high-degree polynomial features are equal to zero. In other words, Lasso Regression automatically performs feature selection and outputs a sparse model (i.e., with few nonzero feature weights).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line">train_errors = []</span><br><span class="line">val_errors = []</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">4</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">plot_model(Lasso, polynomial=<span class="literal">False</span>, alphas=(<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">1</span>), random_state=<span class="number">42</span>)</span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">plot_model(Lasso, polynomial=<span class="literal">True</span>, alphas=(<span class="number">0</span>, <span class="number">0.02</span>, <span class="number">1</span>), tol=<span class="number">1</span>, random_state=<span class="number">42</span>)</span><br><span class="line">plt.show()</span><br><span class="line">print(<span class="string">"Training Errors Linear regression with Lasso"</span>,train_errors[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"Validation Errors Linear regression with Lasso"</span>,val_errors[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"Training Errors Polynomial regression with Lasso"</span>,train_errors[<span class="number">3</span>:])</span><br><span class="line">print(<span class="string">"Validation Errors Polinomial regression with Lasso"</span>,val_errors[<span class="number">3</span>:])</span><br></pre></td></tr></table></figure>

<pre><code>Coefficients from &lt;class &apos;sklearn.linear_model.coordinate_descent.Lasso&apos;&gt; with alpha = 0
[[0.4176916]]
Coefficients from &lt;class &apos;sklearn.linear_model.coordinate_descent.Lasso&apos;&gt; with alpha = 0.1
[0.28473922]
Coefficients from &lt;class &apos;sklearn.linear_model.coordinate_descent.Lasso&apos;&gt; with alpha = 1
[0.]
Coefficients from &lt;class &apos;sklearn.linear_model.coordinate_descent.Lasso&apos;&gt; with alpha = 0
[[ 2.79676282e+01 -8.70301875e+02  1.02393993e+04 -6.14861326e+04
   2.14563424e+05 -4.60902057e+05  6.18449227e+05 -5.05246948e+05
   2.29908470e+05 -4.46826954e+04]]
Coefficients from &lt;class &apos;sklearn.linear_model.coordinate_descent.Lasso&apos;&gt; with alpha = 0.02
[ 0.3569844   0.         -0.         -0.         -0.         -0.
 -0.00764842 -0.00799212 -0.00746378 -0.00698341]
Coefficients from &lt;class &apos;sklearn.linear_model.coordinate_descent.Lasso&apos;&gt; with alpha = 1
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</code></pre><p><img src="output_107_1.png" alt="png"></p>
<pre><code>Training Errors Linear regression with Lasso [0.3850221633585592, 0.3983174004851076, 0.5162468019225317]
Validation Errors Linear regression with Lasso [0.2931679786457559, 0.3374260613813014, 0.5127463561383779]
Training Errors Polynomial regression with Lasso [0.3252732506662376, 0.383267741215846, 0.5162468019225317]
Validation Errors Polinomial regression with Lasso [0.28745132128253525, 0.30262353461319985, 0.5127463561383779]</code></pre><p>In our example Lasso regression worsens the fit.</p>
<p>For example, the dashed line in the right plot on (with $\alpha$ = 0.02) looks almost linear: it is not completely zero, there are positive weights on $X^8$, $X^9$, and $X^10$. In other words, Lasso Regression automatically performs feature selection and outputs asparse model (i.e., with few nonzero feature weights). <br></p>
<h1 id="Elastic-net"><a href="#Elastic-net" class="headerlink" title="Elastic net"></a>Elastic net</h1><p>Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term<br>is a simple mix of both Ridge and Lasso’s regularization terms, and you can control the mix ratio $r$. When<br>$r = 0$, Elastic Net is equivalent to Ridge Regression, and when $r = 1$, it is equivalent to Lasso Regression.</p>
<p>$$ J(\theta) = MSE(\theta) + r \alpha \frac{1}{2} \sum^m_{i=1} |\theta_i| + \frac{1-r}{2} \sum^m_{i=1} \theta_i^2 $$</p>
<p>So when should you use Linear Regression, Ridge, Lasso, or Elastic Net? It is almost always preferable<br>to have at least a little bit of regularization, so generally you should avoid plain Linear Regression.</p>
<ul>
<li>Ridge is a good default,</li>
<li>if you suspect that only a few features are actually useful, you should prefer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to zero.</li>
<li>In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">warn</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.warn = warn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">m = <span class="number">70</span></span><br><span class="line">X = <span class="number">3</span> * np.random.rand(m, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">1</span> + <span class="number">0.5</span> * X + np.random.randn(m, <span class="number">1</span>) / <span class="number">1.5</span></span><br><span class="line">poly_features = PolynomialFeatures(degree=<span class="number">10</span>, include_bias=<span class="literal">False</span>)</span><br><span class="line">X_poly = poly_features.fit_transform(X)</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_poly_std = scaler.fit_transform(X_poly)</span><br><span class="line">valvec = []</span><br><span class="line">elnet = ElasticNet(alpha = <span class="number">0.1</span>, l1_ratio = <span class="number">0.5</span>, max_iter=<span class="number">1000000</span> )</span><br><span class="line">cores = -cross_val_score(elnet, X_poly_std, y, cv=<span class="number">5</span>, scoring=<span class="string">'neg_mean_squared_error'</span>)</span><br><span class="line">alphavec = np.arange(<span class="number">0</span>, <span class="number">1</span>, <span class="number">0.01</span>)</span><br><span class="line">l1ratios = [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.7</span>, <span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> l1rat, style <span class="keyword">in</span> zip(l1ratios, (<span class="string">"b"</span>, <span class="string">"g"</span>, <span class="string">"r"</span>,<span class="string">"k"</span>, <span class="string">"m"</span>)):</span><br><span class="line">    mean_score = []</span><br><span class="line">    min_score = <span class="number">100</span></span><br><span class="line">    <span class="keyword">for</span> alpha <span class="keyword">in</span> alphavec:</span><br><span class="line">        elnet = ElasticNet(alpha = alpha, l1_ratio = l1rat,  normalize=<span class="literal">True</span>) <span class="keyword">if</span> alpha &gt; <span class="number">0</span> <span class="keyword">else</span> LinearRegression()</span><br><span class="line">        score = -cross_val_score(elnet, X_poly, y, cv=<span class="number">5</span>, scoring=<span class="string">'neg_mean_squared_error'</span>)</span><br><span class="line">        mean_score.append(np.mean(score))</span><br><span class="line">        <span class="keyword">if</span> np.mean(score) &lt; min_score:</span><br><span class="line">            min_score = np.mean(score)</span><br><span class="line">            best_alpha = alpha</span><br><span class="line">            best_l1 = l1rat</span><br><span class="line">    plt.plot(alphavec, mean_score, style, label=<span class="string">r"l1_ratio = &#123;&#125;"</span>.format(l1rat))</span><br><span class="line">plt.legend(loc=<span class="string">"bottom right"</span>, fontsize=<span class="number">10</span>)</span><br><span class="line">plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">print(<span class="string">"best model has alpha ="</span>, best_alpha, <span class="string">"l1_ratio ="</span> ,l1rat)</span><br></pre></td></tr></table></figure>

<pre><code>best model has alpha = 0.01 l1_ratio = 1</code></pre><p><img src="output_111_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h1><p>A very different way to regularize iterative learning algorithms such as Gradient Descent is to stop<br>training as soon as the validation error reaches a minimum. This is called early stopping. Figure next<br>shows a complex model (in this case a high-degree Polynomial Regression model) being trained using Batch Gradient Descent. As the epochs go by, the algorithm learns and its prediction error (RMSE) on the training set naturally goes down, and so does its prediction error on the validation set. However, after a while the validation error stops decreasing and actually starts to go back up. This indicates that the model has started to overfit the training data. With early stopping you just stop training as soon as the validation error reaches the minimum. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">m = <span class="number">100</span></span><br><span class="line">X = <span class="number">6</span> * np.random.rand(m, <span class="number">1</span>) - <span class="number">3</span></span><br><span class="line">y = <span class="number">2</span> + X + <span class="number">0.5</span> * X**<span class="number">2</span> + np.random.randn(m, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X_train, X_val, y_train, y_val = train_test_split(X[:<span class="number">50</span>], y[:<span class="number">50</span>].ravel(), test_size=<span class="number">0.5</span>, random_state=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">poly_scaler = Pipeline([</span><br><span class="line">        (<span class="string">"poly_features"</span>, PolynomialFeatures(degree=<span class="number">90</span>, include_bias=<span class="literal">False</span>)),</span><br><span class="line">        (<span class="string">"std_scaler"</span>, StandardScaler()),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">X_train_poly_scaled = poly_scaler.fit_transform(X_train)</span><br><span class="line">X_val_poly_scaled = poly_scaler.transform(X_val)</span><br><span class="line"></span><br><span class="line">sgd_reg = SGDRegressor(max_iter=<span class="number">1</span>,</span><br><span class="line">                       penalty=<span class="literal">None</span>,</span><br><span class="line">                       eta0=<span class="number">0.0005</span>,</span><br><span class="line">                       warm_start=<span class="literal">True</span>,</span><br><span class="line">                       learning_rate=<span class="string">"constant"</span>,</span><br><span class="line">                       random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">n_epochs = <span class="number">500</span></span><br><span class="line">train_errors, val_errors = [], []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">    sgd_reg.fit(X_train_poly_scaled, y_train)</span><br><span class="line">    y_train_predict = sgd_reg.predict(X_train_poly_scaled)</span><br><span class="line">    y_val_predict = sgd_reg.predict(X_val_poly_scaled)</span><br><span class="line">    train_errors.append(mean_squared_error(y_train, y_train_predict))</span><br><span class="line">    val_errors.append(mean_squared_error(y_val, y_val_predict))</span><br><span class="line"></span><br><span class="line">best_epoch = np.argmin(val_errors)</span><br><span class="line">best_val_rmse = np.sqrt(val_errors[best_epoch])</span><br><span class="line"></span><br><span class="line">plt.annotate(<span class="string">'Best model'</span>,</span><br><span class="line">             xy=(best_epoch, best_val_rmse),</span><br><span class="line">             xytext=(best_epoch, best_val_rmse + <span class="number">1</span>),</span><br><span class="line">             ha=<span class="string">"center"</span>,</span><br><span class="line">             arrowprops=dict(facecolor=<span class="string">'black'</span>, shrink=<span class="number">0.05</span>),</span><br><span class="line">             fontsize=<span class="number">16</span>,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">best_val_rmse -= <span class="number">0.03</span>  <span class="comment"># just to make the graph look better</span></span><br><span class="line">plt.plot([<span class="number">0</span>, n_epochs], [best_val_rmse, best_val_rmse], <span class="string">"k:"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.plot(np.sqrt(val_errors), <span class="string">"b-"</span>, linewidth=<span class="number">3</span>, label=<span class="string">"Validation set"</span>)</span><br><span class="line">plt.plot(np.sqrt(train_errors), <span class="string">"r--"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Training set"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper right"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Epoch"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">"RMSE"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_114_0.png" alt="png"></p>
<h1 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h1><p>Some regression algorithms can be used for classification as well (and vice versa). Logistic Regression (also called Logit Regression) is commonly used to estimate the<br>probability that an instance belongs to a particular class (e.g., what is the probability that this email is<br>spam?). If the estimated probability is greater than 50%, then the model predicts that the instance belongs<br>to that class (called the positive class, labeled “1”), or else it predicts that it does not (i.e., it belongs to<br>the negative class, labeled “0”). This makes it a binary classifier</p>
<p>$$ \hat{p} = h_\theta (x) = \sigma(\theta^T \cdot x)$$<br>Logit function is:<br>$$ \sigma(t) = \frac{1}{1 + exp(-t)}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">t = np.linspace(<span class="number">-10</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">sig = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-t))</span><br><span class="line">plt.figure(figsize=(<span class="number">9</span>, <span class="number">3</span>))</span><br><span class="line">plt.plot([<span class="number">-10</span>, <span class="number">10</span>], [<span class="number">0</span>, <span class="number">0</span>], <span class="string">"k-"</span>)</span><br><span class="line">plt.plot([<span class="number">-10</span>, <span class="number">10</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>], <span class="string">"k:"</span>)</span><br><span class="line">plt.plot([<span class="number">-10</span>, <span class="number">10</span>], [<span class="number">1</span>, <span class="number">1</span>], <span class="string">"k:"</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">0</span>], [<span class="number">-1.1</span>, <span class="number">1.1</span>], <span class="string">"k-"</span>)</span><br><span class="line">plt.plot(t, sig, <span class="string">"b-"</span>, linewidth=<span class="number">2</span>, label=<span class="string">r"$\sigma(t) = \frac&#123;1&#125;&#123;1 + e^&#123;-t&#125;&#125;$"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"t"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.axis([<span class="number">-10</span>, <span class="number">10</span>, <span class="number">-0.1</span>, <span class="number">1.1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_117_0.png" alt="png"></p>
<p>if $\hat{p} &lt; 0.5$ then $\hat{y} = 0$ <br><br>if $\hat{p} &gt; 0.5$ then $\hat{y} = 1$</p>
<h1 id="Training-and-cost-function"><a href="#Training-and-cost-function" class="headerlink" title="Training and cost function"></a>Training and cost function</h1><p>The objective of training is to set the parameter vector $\theta$ so that the model estimates high probabilities for positive instances ($y = 1$) and low probabilities for negative instances ($y = 0$). This idea is captured by the cost function shown:<br>$$<br>c(\theta) = \begin{cases}<br> -log(\hat{p})  &amp; \text{if } y = 1\<br> -log(1 - \hat{p})  &amp; \text{if } y = 0<br> \end{cases}<br>$$<br>$-log(\hat{p})$ grows very large as $\hat{p}$ approaches $0$, and  $-log(1 - \hat{p})$ grows very large as  $\hat{p}$ approaches 1. The cost of the training set is the sum of the costs function for all instances:<br>$$<br>J(\theta) = - \frac{1}{m} \sum^m_{i=1}[y^{(i)}log(\hat{p}^{(i)}) + (1 - y^{(i)})log(1-\hat{p}^{(i)})]<br>$$<br>There is no closed form solution. However, the gradient is convex, so we can find global minimum. The partial deriviatives of the cost function with regards to the jth model parameters $\theta_j$ is:<br>$$<br>\frac{\partial J(\theta)}{\partial \theta} =  \frac{1}{m} \sum^m_{i=1})(\sigma(\theta^T \cdot x^{(i)}) - y^{(i)})x^{(i)}_j<br>$$<br>After we have vector of partial deriviatives we can use it in the Batch Gradient Descent<br>algorithm, or mini-Batch or Stochastic.</p>
<h1 id="Decisions-Boundaries"><a href="#Decisions-Boundaries" class="headerlink" title="Decisions Boundaries"></a>Decisions Boundaries</h1><p>Let’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that contains the sepal<br>and petal length and width of 150 iris flowers of three different species: Iris-Setosa, Iris-Versicolor, and<br>Iris-Virginica. We will build a classifier to detect the Iris-Virginica type based only on the petal width feature. First<br>let’s load the data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">list(iris.keys())</span><br></pre></td></tr></table></figure>

<pre><code>[&apos;data&apos;, &apos;target&apos;, &apos;target_names&apos;, &apos;DESCR&apos;, &apos;feature_names&apos;, &apos;filename&apos;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(iris.DESCR)</span><br></pre></td></tr></table></figure>

<pre><code>.. _iris_dataset:

Iris plants dataset
--------------------

**Data Set Characteristics:**

    :Number of Instances: 150 (50 in each of three classes)
    :Number of Attributes: 4 numeric, predictive attributes and the class
    :Attribute Information:
        - sepal length in cm
        - sepal width in cm
        - petal length in cm
        - petal width in cm
        - class:
                - Iris-Setosa
                - Iris-Versicolour
                - Iris-Virginica

    :Summary Statistics:

    ============== ==== ==== ======= ===== ====================
                    Min  Max   Mean    SD   Class Correlation
    ============== ==== ==== ======= ===== ====================
    sepal length:   4.3  7.9   5.84   0.83    0.7826
    sepal width:    2.0  4.4   3.05   0.43   -0.4194
    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)
    ============== ==== ==== ======= ===== ====================

    :Missing Attribute Values: None
    :Class Distribution: 33.3% for each of 3 classes.
    :Creator: R.A. Fisher
    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
    :Date: July, 1988

The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken
from Fisher&apos;s paper. Note that it&apos;s the same as in R, but not as in the UCI
Machine Learning Repository, which has two wrong data points.

This is perhaps the best known database to be found in the
pattern recognition literature.  Fisher&apos;s paper is a classic in the field and
is referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The
data set contains 3 classes of 50 instances each, where each class refers to a
type of iris plant.  One class is linearly separable from the other 2; the
latter are NOT linearly separable from each other.

.. topic:: References

   - Fisher, R.A. &quot;The use of multiple measurements in taxonomic problems&quot;
     Annual Eugenics, 7, Part II, 179-188 (1936); also in &quot;Contributions to
     Mathematical Statistics&quot; (John Wiley, NY, 1950).
   - Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis.
     (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.
   - Dasarathy, B.V. (1980) &quot;Nosing Around the Neighborhood: A New System
     Structure and Classification Rule for Recognition in Partially Exposed
     Environments&quot;.  IEEE Transactions on Pattern Analysis and Machine
     Intelligence, Vol. PAMI-2, No. 1, 67-71.
   - Gates, G.W. (1972) &quot;The Reduced Nearest Neighbor Rule&quot;.  IEEE Transactions
     on Information Theory, May 1972, 431-433.
   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al&quot;s AUTOCLASS II
     conceptual clustering system finds 3 classes in the data.
   - Many, many more ...</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># Set one X - petal width</span></span><br><span class="line">X = iris[<span class="string">"data"</span>][:, <span class="number">3</span>:]  <span class="comment"># petal width</span></span><br><span class="line"><span class="comment"># Y is the classification of Iris-Virginica. </span></span><br><span class="line">y = (iris[<span class="string">"target"</span>] == <span class="number">2</span>).astype(np.int)  <span class="comment"># 1 if Iris-Virginica, else 0</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">log_reg = LogisticRegression(random_state=<span class="number">42</span>)</span><br><span class="line">log_reg.fit(X, y)</span><br></pre></td></tr></table></figure>

<pre><code>LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class=&apos;warn&apos;, n_jobs=None, penalty=&apos;l2&apos;,
                   random_state=42, solver=&apos;warn&apos;, tol=0.0001, verbose=0,
                   warm_start=False)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Green is the probability of Iris-Virginica, blue is the probability of Not Iris-Virginica. </span></span><br><span class="line">X_new = np.linspace(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1000</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">y_proba = log_reg.predict_proba(X_new)</span><br><span class="line"></span><br><span class="line">plt.plot(X_new, y_proba[:, <span class="number">1</span>], <span class="string">"g-"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Iris-Virginica"</span>)</span><br><span class="line">plt.plot(X_new, y_proba[:, <span class="number">0</span>], <span class="string">"b--"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Not Iris-Virginica"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>[&lt;matplotlib.lines.Line2D at 0x24854380d30&gt;]</code></pre><p><img src="output_126_1.png" alt="png"></p>
<p>The figure in the book actually is actually a bit fancier:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">X_new = np.linspace(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1000</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">y_proba = log_reg.predict_proba(X_new)</span><br><span class="line">decision_boundary = X_new[y_proba[:, <span class="number">1</span>] &gt;= <span class="number">0.5</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">3</span>))</span><br><span class="line">plt.plot(X[y==<span class="number">0</span>], y[y==<span class="number">0</span>], <span class="string">"bs"</span>)</span><br><span class="line">plt.plot(X[y==<span class="number">1</span>], y[y==<span class="number">1</span>], <span class="string">"g^"</span>)</span><br><span class="line">plt.plot([decision_boundary, decision_boundary], [<span class="number">-1</span>, <span class="number">2</span>], <span class="string">"k:"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.plot(X_new, y_proba[:, <span class="number">1</span>], <span class="string">"g-"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Iris-Virginica"</span>)</span><br><span class="line">plt.plot(X_new, y_proba[:, <span class="number">0</span>], <span class="string">"b--"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Not Iris-Virginica"</span>)</span><br><span class="line">plt.text(decision_boundary+<span class="number">0.02</span>, <span class="number">0.15</span>, <span class="string">"Decision  boundary"</span>, fontsize=<span class="number">14</span>, color=<span class="string">"k"</span>, ha=<span class="string">"center"</span>)</span><br><span class="line">plt.arrow(decision_boundary, <span class="number">0.08</span>, <span class="number">-0.3</span>, <span class="number">0</span>, head_width=<span class="number">0.05</span>, head_length=<span class="number">0.1</span>, fc=<span class="string">'b'</span>, ec=<span class="string">'b'</span>)</span><br><span class="line">plt.arrow(decision_boundary, <span class="number">0.92</span>, <span class="number">0.3</span>, <span class="number">0</span>, head_width=<span class="number">0.05</span>, head_length=<span class="number">0.1</span>, fc=<span class="string">'g'</span>, ec=<span class="string">'g'</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Petal width (cm)"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Probability"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"center left"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">3</span>, <span class="number">-0.02</span>, <span class="number">1.02</span>])</span><br><span class="line">save_fig(<span class="string">"logistic_regression_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Saving figure logistic_regression_plot</code></pre><p><img src="output_128_1.png" alt="png"></p>
<p>The petal width of Iris-Virginica flowers (represented by triangles) ranges from 1.4 cm to 2.5 cm, while<br>the other iris flowers (represented by squares) generally have a smaller petal width, ranging from 0.1 cm<br>to 1.8 cm. Notice that there is a bit of overlap. Above about 2 cm the classifier is highly confident that the<br>flower is an Iris-Virginica (it outputs a high probability to that class), while below 1 cm it is highly<br>confident that it is not an Iris-Virginica (high probability for the “Not Iris-Virginica” class). In between these extremes, the classifier is unsure. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 50% probability is at</span></span><br><span class="line">decision_boundary</span><br></pre></td></tr></table></figure>

<pre><code>array([1.61561562])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predict with sepal lenths of 1.7 and 1.5.</span></span><br><span class="line">log_reg.predict([[<span class="number">1.7</span>], [<span class="number">1.5</span>]])</span><br></pre></td></tr></table></figure>

<pre><code>array([1, 0])</code></pre><p>Next we will show the same dataset but this time displaying two features: petal width and length. Once<br>trained, the Logistic Regression classifier can estimate the probability that a new flower is an Iris-<br>Virginica based on these two features. The dashed line represents the points where the model estimates a<br>50% probability: this is the model’s decision boundary.  Each parallel line represents the points where the model outputs a specific probability, from 15% (bottom left) to 90% (top right). All the flowers beyond the top-right line have an over 90% chance of being Iris-Virginica according to the model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">X = iris[<span class="string">"data"</span>][:, (<span class="number">2</span>, <span class="number">3</span>)]  <span class="comment"># petal length, petal width</span></span><br><span class="line">y = (iris[<span class="string">"target"</span>] == <span class="number">2</span>).astype(np.int)</span><br><span class="line"><span class="comment"># C is the regularization parameter, we will cover it more in the next lecture</span></span><br><span class="line">log_reg = LogisticRegression(C=<span class="number">10</span>**<span class="number">10</span>, random_state=<span class="number">42</span>)</span><br><span class="line">log_reg.fit(X, y)</span><br><span class="line"></span><br><span class="line">x0, x1 = np.meshgrid(</span><br><span class="line">        np.linspace(<span class="number">2.9</span>, <span class="number">7</span>, <span class="number">500</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">        np.linspace(<span class="number">0.8</span>, <span class="number">2.7</span>, <span class="number">200</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">    )</span><br><span class="line">X_new = np.c_[x0.ravel(), x1.ravel()]</span><br><span class="line"></span><br><span class="line">y_proba = log_reg.predict_proba(X_new)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>], <span class="string">"bs"</span>)</span><br><span class="line">plt.plot(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>], <span class="string">"g^"</span>)</span><br><span class="line"></span><br><span class="line">zz = y_proba[:, <span class="number">1</span>].reshape(x0.shape)</span><br><span class="line">contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">left_right = np.array([<span class="number">2.9</span>, <span class="number">7</span>])</span><br><span class="line">boundary = -(log_reg.coef_[<span class="number">0</span>][<span class="number">0</span>] * left_right + log_reg.intercept_[<span class="number">0</span>]) / log_reg.coef_[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">plt.clabel(contour, inline=<span class="number">1</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.plot(left_right, boundary, <span class="string">"k--"</span>, linewidth=<span class="number">3</span>)</span><br><span class="line">plt.text(<span class="number">3.5</span>, <span class="number">1.5</span>, <span class="string">"Not Iris-Virginica"</span>, fontsize=<span class="number">14</span>, color=<span class="string">"b"</span>, ha=<span class="string">"center"</span>)</span><br><span class="line">plt.text(<span class="number">6.5</span>, <span class="number">2.3</span>, <span class="string">"Iris-Virginica"</span>, fontsize=<span class="number">14</span>, color=<span class="string">"g"</span>, ha=<span class="string">"center"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Petal length"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Petal width"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.axis([<span class="number">2.9</span>, <span class="number">7</span>, <span class="number">0.8</span>, <span class="number">2.7</span>])</span><br><span class="line">save_fig(<span class="string">"logistic_regression_contour_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Saving figure logistic_regression_contour_plot</code></pre><p><img src="output_133_1.png" alt="png"></p>
<h1 id="Multinomial-Logit-Regression"><a href="#Multinomial-Logit-Regression" class="headerlink" title="Multinomial Logit Regression"></a>Multinomial Logit Regression</h1><p>The Logistic Regression model can be generalized to support multiple classes directly, without having to<br>train and combine multiple binary classifiers. This is called Softmax Regression, or Multinomial Logistic Regression.<br>For each instance x the model first computes a score $sk(x)$ for each class $k$, then estimates the probability of each class by applying the softmax function (also called the normalized exponential) to the scores:<br>$$<br>s_k(x) = \theta^T_k \cdot x<br>$$<br>Each class $k$ has it’s own vector of parameters $\theta_k$. For each instance $x$ we compute a probability $\hat{p}<em>k$ of belonging to class $k$. The probability is the ratio of the score $k$ and the sum of score of all other classes:<br>$$<br>\hat{p}_k = \sigma(s(x))_k = \frac{exp(s_k(x))}{\sum</em>{j=1}^K exp(s_j(x))}<br>$$<br>Multinomial logit predict class $k$ according to the highest probability. In the estimation of multinomial logit we minimize a cost function similar to the binomial logit. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># now we estimate multinomial logit using three classes:</span></span><br><span class="line">X = iris[<span class="string">"data"</span>][:, (<span class="number">2</span>, <span class="number">3</span>)]  <span class="comment"># petal length, petal width</span></span><br><span class="line">y = iris[<span class="string">"target"</span>]</span><br><span class="line"></span><br><span class="line">softmax_reg = LogisticRegression(multi_class=<span class="string">"multinomial"</span>,solver=<span class="string">"lbfgs"</span>, C=<span class="number">10</span>, random_state=<span class="number">42</span>)</span><br><span class="line">softmax_reg.fit(X, y)</span><br></pre></td></tr></table></figure>

<pre><code>LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class=&apos;multinomial&apos;, n_jobs=None, penalty=&apos;l2&apos;,
                   random_state=42, solver=&apos;lbfgs&apos;, tol=0.0001, verbose=0,
                   warm_start=False)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">x0, x1 = np.meshgrid(</span><br><span class="line">        np.linspace(<span class="number">0</span>, <span class="number">8</span>, <span class="number">500</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">        np.linspace(<span class="number">0</span>, <span class="number">3.5</span>, <span class="number">200</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">    )</span><br><span class="line">X_new = np.c_[x0.ravel(), x1.ravel()]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y_proba = softmax_reg.predict_proba(X_new)</span><br><span class="line">y_predict = softmax_reg.predict(X_new)</span><br><span class="line"></span><br><span class="line">zz1 = y_proba[:, <span class="number">1</span>].reshape(x0.shape)</span><br><span class="line">zz = y_predict.reshape(x0.shape)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot(X[y==<span class="number">2</span>, <span class="number">0</span>], X[y==<span class="number">2</span>, <span class="number">1</span>], <span class="string">"g^"</span>, label=<span class="string">"Iris-Virginica"</span>)</span><br><span class="line">plt.plot(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>], <span class="string">"bs"</span>, label=<span class="string">"Iris-Versicolor"</span>)</span><br><span class="line">plt.plot(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>], <span class="string">"yo"</span>, label=<span class="string">"Iris-Setosa"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line">custom_cmap = ListedColormap([<span class="string">'#fafab0'</span>,<span class="string">'#9898ff'</span>,<span class="string">'#a0faa0'</span>])</span><br><span class="line"></span><br><span class="line">plt.contourf(x0, x1, zz, cmap=custom_cmap)</span><br><span class="line">contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)</span><br><span class="line">plt.clabel(contour, inline=<span class="number">1</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Petal length"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Petal width"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"center left"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">3.5</span>])</span><br><span class="line">save_fig(<span class="string">"softmax_regression_contour_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Saving figure softmax_regression_contour_plot</code></pre><p><img src="output_136_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">softmax_reg.predict([[<span class="number">5</span>, <span class="number">2</span>]])</span><br></pre></td></tr></table></figure>

<pre><code>array([2])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">softmax_reg.predict_proba([[<span class="number">5</span>, <span class="number">2</span>]])</span><br></pre></td></tr></table></figure>

<pre><code>array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])</code></pre><h1 id="Exercise-solutions"><a href="#Exercise-solutions" class="headerlink" title="Exercise solutions"></a>Exercise solutions</h1><h2 id="1-to-11"><a href="#1-to-11" class="headerlink" title="1. to 11."></a>1. to 11.</h2><p>See appendix A.</p>
<h2 id="12-Batch-Gradient-Descent-with-early-stopping-for-Softmax-Regression"><a href="#12-Batch-Gradient-Descent-with-early-stopping-for-Softmax-Regression" class="headerlink" title="12. Batch Gradient Descent with early stopping for Softmax Regression"></a>12. Batch Gradient Descent with early stopping for Softmax Regression</h2><p>(without using Scikit-Learn)</p>
<p>Let’s start by loading the data. We will just reuse the Iris dataset we loaded earlier.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = iris[<span class="string">"data"</span>][:, (<span class="number">2</span>, <span class="number">3</span>)]  <span class="comment"># petal length, petal width</span></span><br><span class="line">y = iris[<span class="string">"target"</span>]</span><br></pre></td></tr></table></figure>

<p>We need to add the bias term for every instance ($x_0 = 1$):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_with_bias = np.c_[np.ones([len(X), <span class="number">1</span>]), X]</span><br></pre></td></tr></table></figure>

<p>And let’s set the random seed so the output of this exercise solution is reproducible:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">2042</span>)</span><br></pre></td></tr></table></figure>

<p>The easiest option to split the dataset into a training set, a validation set and a test set would be to use Scikit-Learn’s <code>train_test_split()</code> function, but the point of this exercise is to try understand the algorithms by implementing them manually. So here is one possible implementation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">test_ratio = <span class="number">0.2</span></span><br><span class="line">validation_ratio = <span class="number">0.2</span></span><br><span class="line">total_size = len(X_with_bias)</span><br><span class="line"></span><br><span class="line">test_size = int(total_size * test_ratio)</span><br><span class="line">validation_size = int(total_size * validation_ratio)</span><br><span class="line">train_size = total_size - test_size - validation_size</span><br><span class="line"></span><br><span class="line">rnd_indices = np.random.permutation(total_size)</span><br><span class="line"></span><br><span class="line">X_train = X_with_bias[rnd_indices[:train_size]]</span><br><span class="line">y_train = y[rnd_indices[:train_size]]</span><br><span class="line">X_valid = X_with_bias[rnd_indices[train_size:-test_size]]</span><br><span class="line">y_valid = y[rnd_indices[train_size:-test_size]]</span><br><span class="line">X_test = X_with_bias[rnd_indices[-test_size:]]</span><br><span class="line">y_test = y[rnd_indices[-test_size:]]</span><br></pre></td></tr></table></figure>

<p>The targets are currently class indices (0, 1 or 2), but we need target class probabilities to train the Softmax Regression model. Each instance will have target class probabilities equal to 0.0 for all classes except for the target class which will have a probability of 1.0 (in other words, the vector of class probabilities for ay given instance is a one-hot vector). Let’s write a small function to convert the vector of class indices into a matrix containing a one-hot vector for each instance:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_one_hot</span><span class="params">(y)</span>:</span></span><br><span class="line">    n_classes = y.max() + <span class="number">1</span></span><br><span class="line">    m = len(y)</span><br><span class="line">    Y_one_hot = np.zeros((m, n_classes))</span><br><span class="line">    Y_one_hot[np.arange(m), y] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> Y_one_hot</span><br></pre></td></tr></table></figure>

<p>Let’s test this function on the first 10 instances:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>

<pre><code>array([0, 1, 2, 1, 1, 0, 1, 1, 1, 0])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">to_one_hot(y_train[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<pre><code>array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.],
       [0., 1., 0.],
       [0., 1., 0.],
       [1., 0., 0.],
       [0., 1., 0.],
       [0., 1., 0.],
       [0., 1., 0.],
       [1., 0., 0.]])</code></pre><p>Looks good, so let’s create the target class probabilities matrix for the training set and the test set:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Y_train_one_hot = to_one_hot(y_train)</span><br><span class="line">Y_valid_one_hot = to_one_hot(y_valid)</span><br><span class="line">Y_test_one_hot = to_one_hot(y_test)</span><br></pre></td></tr></table></figure>

<p>Now let’s implement the Softmax function. Recall that it is defined by the following equation:</p>
<p>$\sigma\left(\mathbf{s}(\mathbf{x})\right)<em>k = \dfrac{\exp\left(s_k(\mathbf{x})\right)}{\sum\limits</em>{j=1}^{K}{\exp\left(s_j(\mathbf{x})\right)}}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(logits)</span>:</span></span><br><span class="line">    exps = np.exp(logits)</span><br><span class="line">    exp_sums = np.sum(exps, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> exps / exp_sums</span><br></pre></td></tr></table></figure>

<p>We are almost ready to start training. Let’s define the number of inputs and outputs:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n_inputs = X_train.shape[<span class="number">1</span>] <span class="comment"># == 3 (2 features plus the bias term)</span></span><br><span class="line">n_outputs = len(np.unique(y_train))   <span class="comment"># == 3 (3 iris classes)</span></span><br></pre></td></tr></table></figure>

<p>Now here comes the hardest part: training! Theoretically, it’s simple: it’s just a matter of translating the math equations into Python code. But in practice, it can be quite tricky: in particular, it’s easy to mix up the order of the terms, or the indices. You can even end up with code that looks like it’s working but is actually not computing exactly the right thing. When unsure, you should write down the shape of each term in the equation and make sure the corresponding terms in your code match closely. It can also help to evaluate each term independently and print them out. The good news it that you won’t have to do this everyday, since all this is well implemented by Scikit-Learn, but it will help you understand what’s going on under the hood.</p>
<p>So the equations we will need are the cost function:</p>
<p>$J(\mathbf{\Theta}) =</p>
<ul>
<li>\dfrac{1}{m}\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K}{y_k^{(i)}\log\left(\hat{p}_k^{(i)}\right)}$</li>
</ul>
<p>And the equation for the gradients:</p>
<p>$\nabla_{\mathbf{\theta}^{(k)}} , J(\mathbf{\Theta}) = \dfrac{1}{m} \sum\limits_{i=1}^{m}{ \left ( \hat{p}^{(i)}_k - y_k^{(i)} \right ) \mathbf{x}^{(i)}}$</p>
<p>Note that $\log\left(\hat{p}_k^{(i)}\right)$ may not be computable if $\hat{p}_k^{(i)} = 0$. So we will add a tiny value $\epsilon$ to $\log\left(\hat{p}_k^{(i)}\right)$ to avoid getting <code>nan</code> values.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">eta = <span class="number">0.01</span></span><br><span class="line">n_iterations = <span class="number">5001</span></span><br><span class="line">m = len(X_train)</span><br><span class="line">epsilon = <span class="number">1e-7</span></span><br><span class="line"></span><br><span class="line">Theta = np.random.randn(n_inputs, n_outputs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">    logits = X_train.dot(Theta)</span><br><span class="line">    Y_proba = softmax(logits)</span><br><span class="line">    loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=<span class="number">1</span>))</span><br><span class="line">    error = Y_proba - Y_train_one_hot</span><br><span class="line">    <span class="keyword">if</span> iteration % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        print(iteration, loss)</span><br><span class="line">    gradients = <span class="number">1</span>/m * X_train.T.dot(error)</span><br><span class="line">    Theta = Theta - eta * gradients</span><br></pre></td></tr></table></figure>

<pre><code>0 5.446205811872683
500 0.8350062641405651
1000 0.6878801447192402
1500 0.6012379137693313
2000 0.5444496861981873
2500 0.5038530181431525
3000 0.4729228972192248
3500 0.4482424418895776
4000 0.4278651093928793
4500 0.41060071429187134
5000 0.3956780375390373</code></pre><p>And that’s it! The Softmax model is trained. Let’s look at the model parameters:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Theta</span><br></pre></td></tr></table></figure>

<pre><code>array([[ 3.32094157, -0.6501102 , -2.99979416],
       [-1.1718465 ,  0.11706172,  0.10507543],
       [-0.70224261, -0.09527802,  1.4786383 ]])</code></pre><p>Let’s make predictions for the validation set and check the accuracy score:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">logits = X_valid.dot(Theta)</span><br><span class="line">Y_proba = softmax(logits)</span><br><span class="line">y_predict = np.argmax(Y_proba, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">accuracy_score = np.mean(y_predict == y_valid)</span><br><span class="line">accuracy_score</span><br></pre></td></tr></table></figure>

<pre><code>0.9666666666666667</code></pre><p>Well, this model looks pretty good. For the sake of the exercise, let’s add a bit of $\ell_2$ regularization. The following training code is similar to the one above, but the loss now has an additional $\ell_2$ penalty, and the gradients have the proper additional term (note that we don’t regularize the first element of <code>Theta</code> since this corresponds to the bias term). Also, let’s try increasing the learning rate <code>eta</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">eta = <span class="number">0.1</span></span><br><span class="line">n_iterations = <span class="number">5001</span></span><br><span class="line">m = len(X_train)</span><br><span class="line">epsilon = <span class="number">1e-7</span></span><br><span class="line">alpha = <span class="number">0.1</span>  <span class="comment"># regularization hyperparameter</span></span><br><span class="line"></span><br><span class="line">Theta = np.random.randn(n_inputs, n_outputs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">    logits = X_train.dot(Theta)</span><br><span class="line">    Y_proba = softmax(logits)</span><br><span class="line">    xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=<span class="number">1</span>))</span><br><span class="line">    l2_loss = <span class="number">1</span>/<span class="number">2</span> * np.sum(np.square(Theta[<span class="number">1</span>:]))</span><br><span class="line">    loss = xentropy_loss + alpha * l2_loss</span><br><span class="line">    error = Y_proba - Y_train_one_hot</span><br><span class="line">    <span class="keyword">if</span> iteration % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        print(iteration, loss)</span><br><span class="line">    gradients = <span class="number">1</span>/m * X_train.T.dot(error) + np.r_[np.zeros([<span class="number">1</span>, n_outputs]), alpha * Theta[<span class="number">1</span>:]]</span><br><span class="line">    Theta = Theta - eta * gradients</span><br></pre></td></tr></table></figure>

<pre><code>0 6.629842469083912
500 0.5339667976629506
1000 0.503640075014894
1500 0.49468910594603216
2000 0.4912968418075477
2500 0.48989924700933296
3000 0.48929905984511984
3500 0.48903512443978603
4000 0.4889173621830818
4500 0.4888643337449303
5000 0.4888403120738818</code></pre><p>Because of the additional $\ell_2$ penalty, the loss seems greater than earlier, but perhaps this model will perform better? Let’s find out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">logits = X_valid.dot(Theta)</span><br><span class="line">Y_proba = softmax(logits)</span><br><span class="line">y_predict = np.argmax(Y_proba, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">accuracy_score = np.mean(y_predict == y_valid)</span><br><span class="line">accuracy_score</span><br></pre></td></tr></table></figure>

<pre><code>1.0</code></pre><p>Cool, perfect accuracy! We probably just got lucky with this validation set, but still, it’s pleasant.</p>
<p>Now let’s add early stopping. For this we just need to measure the loss on the validation set at every iteration and stop when the error starts growing.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">eta = <span class="number">0.1</span> </span><br><span class="line">n_iterations = <span class="number">5001</span></span><br><span class="line">m = len(X_train)</span><br><span class="line">epsilon = <span class="number">1e-7</span></span><br><span class="line">alpha = <span class="number">0.1</span>  <span class="comment"># regularization hyperparameter</span></span><br><span class="line">best_loss = np.infty</span><br><span class="line"></span><br><span class="line">Theta = np.random.randn(n_inputs, n_outputs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">    logits = X_train.dot(Theta)</span><br><span class="line">    Y_proba = softmax(logits)</span><br><span class="line">    xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=<span class="number">1</span>))</span><br><span class="line">    l2_loss = <span class="number">1</span>/<span class="number">2</span> * np.sum(np.square(Theta[<span class="number">1</span>:]))</span><br><span class="line">    loss = xentropy_loss + alpha * l2_loss</span><br><span class="line">    error = Y_proba - Y_train_one_hot</span><br><span class="line">    gradients = <span class="number">1</span>/m * X_train.T.dot(error) + np.r_[np.zeros([<span class="number">1</span>, n_outputs]), alpha * Theta[<span class="number">1</span>:]]</span><br><span class="line">    Theta = Theta - eta * gradients</span><br><span class="line"></span><br><span class="line">    logits = X_valid.dot(Theta)</span><br><span class="line">    Y_proba = softmax(logits)</span><br><span class="line">    xentropy_loss = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon), axis=<span class="number">1</span>))</span><br><span class="line">    l2_loss = <span class="number">1</span>/<span class="number">2</span> * np.sum(np.square(Theta[<span class="number">1</span>:]))</span><br><span class="line">    loss = xentropy_loss + alpha * l2_loss</span><br><span class="line">    <span class="keyword">if</span> iteration % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        print(iteration, loss)</span><br><span class="line">    <span class="keyword">if</span> loss &lt; best_loss:</span><br><span class="line">        best_loss = loss</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(iteration - <span class="number">1</span>, best_loss)</span><br><span class="line">        print(iteration, loss, <span class="string">"early stopping!"</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<pre><code>0 4.7096017363419875
500 0.5739711987633519
1000 0.5435638529109127
1500 0.5355752782580262
2000 0.5331959249285544
2500 0.5325946767399383
2765 0.5325460966791898
2766 0.5325460971327975 early stopping!</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">logits = X_valid.dot(Theta)</span><br><span class="line">Y_proba = softmax(logits)</span><br><span class="line">y_predict = np.argmax(Y_proba, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">accuracy_score = np.mean(y_predict == y_valid)</span><br><span class="line">accuracy_score</span><br></pre></td></tr></table></figure>

<pre><code>1.0</code></pre><p>Still perfect, but faster.</p>
<p>Now let’s plot the model’s predictions on the whole dataset:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">x0, x1 = np.meshgrid(</span><br><span class="line">        np.linspace(<span class="number">0</span>, <span class="number">8</span>, <span class="number">500</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">        np.linspace(<span class="number">0</span>, <span class="number">3.5</span>, <span class="number">200</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">    )</span><br><span class="line">X_new = np.c_[x0.ravel(), x1.ravel()]</span><br><span class="line">X_new_with_bias = np.c_[np.ones([len(X_new), <span class="number">1</span>]), X_new]</span><br><span class="line"></span><br><span class="line">logits = X_new_with_bias.dot(Theta)</span><br><span class="line">Y_proba = softmax(logits)</span><br><span class="line">y_predict = np.argmax(Y_proba, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">zz1 = Y_proba[:, <span class="number">1</span>].reshape(x0.shape)</span><br><span class="line">zz = y_predict.reshape(x0.shape)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot(X[y==<span class="number">2</span>, <span class="number">0</span>], X[y==<span class="number">2</span>, <span class="number">1</span>], <span class="string">"g^"</span>, label=<span class="string">"Iris-Virginica"</span>)</span><br><span class="line">plt.plot(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>], <span class="string">"bs"</span>, label=<span class="string">"Iris-Versicolor"</span>)</span><br><span class="line">plt.plot(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>], <span class="string">"yo"</span>, label=<span class="string">"Iris-Setosa"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line">custom_cmap = ListedColormap([<span class="string">'#fafab0'</span>,<span class="string">'#9898ff'</span>,<span class="string">'#a0faa0'</span>])</span><br><span class="line"></span><br><span class="line">plt.contourf(x0, x1, zz, cmap=custom_cmap)</span><br><span class="line">contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)</span><br><span class="line">plt.clabel(contour, inline=<span class="number">1</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Petal length"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Petal width"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">3.5</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_178_0.png" alt="png"></p>
<p>And now let’s measure the final model’s accuracy on the test set:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">logits = X_test.dot(Theta)</span><br><span class="line">Y_proba = softmax(logits)</span><br><span class="line">y_predict = np.argmax(Y_proba, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">accuracy_score = np.mean(y_predict == y_test)</span><br><span class="line">accuracy_score</span><br></pre></td></tr></table></figure>

<pre><code>0.9333333333333333</code></pre><p>Our perfect model turns out to have slight imperfections. This variability is likely due to the very small size of the dataset: depending on how you sample the training set, validation set and the test set, you can get quite different results. Try changing the random seed and running the code again a few times, you will see that the results will vary.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/01/Lecture 4 Classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/01/Lecture 4 Classification/" itemprop="url">Hands on Machine Learning-Chapter 3</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-01T19:52:14-04:00">
                2019-10-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/Hands-on-Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Hands-on Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>《Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Technique for Building Intelligent Systems》—-Chapter 3</p>
</blockquote>
<p><em>This notebook contains all the sample code and solutions to the exercises in chapter 3.</em></p>
<p>In previous lesson we predicted values, now we will be predicting classes. </p>
<h1 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h1><p>First, let’s make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># To support both python 2 and python 3</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division, print_function, unicode_literals</span><br><span class="line"></span><br><span class="line"><span class="comment"># Common imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># to make this notebook's output stable across runs</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># To plot pretty figures</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">'axes.labelsize'</span>] = <span class="number">14</span></span><br><span class="line">plt.rcParams[<span class="string">'xtick.labelsize'</span>] = <span class="number">12</span></span><br><span class="line">plt.rcParams[<span class="string">'ytick.labelsize'</span>] = <span class="number">12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Where to save the figures</span></span><br><span class="line">PROJECT_ROOT_DIR = <span class="string">'..'</span></span><br><span class="line">CHAPTER_ID = <span class="string">"classification"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_fig</span><span class="params">(fig_id, tight_layout=True)</span>:</span></span><br><span class="line">    path = os.path.join(PROJECT_ROOT_DIR, <span class="string">"images"</span>, CHAPTER_ID, fig_id + <span class="string">".png"</span>)</span><br><span class="line">    print(<span class="string">"Saving figure"</span>, fig_id)</span><br><span class="line">    <span class="keyword">if</span> tight_layout:</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">    plt.savefig(path, format=<span class="string">'png'</span>, dpi=<span class="number">300</span>)</span><br></pre></td></tr></table></figure>

<h1 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h1><p>We will use data on handwriting of 70,000 digits written by school children and Census employees trying to correctly classify the digits.</p>
<p>As the code ‘fetch_mldata’ is not working anymore, my professor set a new function ‘fetch_mnist’ to change the url and the path of MNIST data.</p>
<blockquote>
<p><code>fetch_mldata</code> is <a href="https://scikit-learn.org/0.20/modules/generated/sklearn.datasets.fetch_mldata.html" target="_blank" rel="noopener">deprecated</a> since scikit-learn v0.20, and replaced with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html" target="_blank" rel="noopener"><code>fetch_openml</code></a>;<br>from <a href="https://stackoverflow.com/questions/57061437/why-am-i-getting-the-following-connectionreseterror-for-mnist-fetch-mldata" target="_blank" rel="noopener">stack overflow</a></p>
<p>Actually, the code to fetch MNIST is :</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_openml</span><br><span class="line">&gt; X, y = fetch_openml(<span class="string">'mnist_784'</span>, version=<span class="number">1</span>, return_X_y=<span class="literal">True</span>) </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p><strong>But, in this course, professor uses a different way to do the same thing. And in this note I will follow the professor’s version as it is a class note.</strong></p>
</blockquote>
<p>Define a function to copy MNIST from Github.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run this first </span></span><br><span class="line"><span class="keyword">from</span> shutil <span class="keyword">import</span> copyfileobj</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.base <span class="keyword">import</span> get_data_home</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_mnist</span><span class="params">(data_home=None)</span>:</span></span><br><span class="line">    mnist_alternative_url = <span class="string">"https://github.com/amplab/datascience-sp14/raw/master/lab7/mldata/mnist-original.mat"</span></span><br><span class="line">    data_home = get_data_home(data_home=data_home)</span><br><span class="line">    data_home = os.path.join(data_home, <span class="string">'mldata'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(data_home):</span><br><span class="line">        os.makedirs(data_home)</span><br><span class="line">    mnist_save_path = os.path.join(data_home, <span class="string">"mnist-original.mat"</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(mnist_save_path):</span><br><span class="line">        mnist_url = urllib.request.urlopen(mnist_alternative_url)</span><br><span class="line">        <span class="keyword">with</span> open(mnist_save_path, <span class="string">"wb"</span>) <span class="keyword">as</span> matlab_file:</span><br><span class="line">            copyfileobj(mnist_url, matlab_file)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">    path = <span class="string">'../../data/mnist.pkl.gz'</span></span><br><span class="line">    f = gzip.open(path, <span class="string">'rb'</span>)</span><br><span class="line">    training_data, validation_data, test_data = Pickle.load(f)</span><br><span class="line">    f.close()</span><br><span class="line"></span><br><span class="line">    X_train, y_train = training_data[<span class="number">0</span>], training_data[<span class="number">1</span>]</span><br><span class="line">    print(X_train.shape, y_train.shape)</span><br><span class="line">    <span class="comment"># (50000L, 784L) (50000L,)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># get the first image and it's label</span></span><br><span class="line">    img1_arr, img1_label = X_train[<span class="number">0</span>], y_train[<span class="number">0</span>]</span><br><span class="line">    print(img1_arr.shape, img1_label)</span><br><span class="line">    <span class="comment"># (784L,) , 5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># reshape first image(1 D vector) to 2D dimension image</span></span><br><span class="line">    img1_2d = np.reshape(img1_arr, (<span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">    <span class="comment"># show it</span></span><br><span class="line">    plt.subplot(<span class="number">111</span>)</span><br><span class="line">    plt.imshow(img1_2d, cmap=plt.get_cmap(<span class="string">'gray'</span>))</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fetch the data</span></span><br><span class="line">fetch_mnist()</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_mldata</span><br><span class="line">mnist = fetch_mldata(<span class="string">"MNIST original"</span>)</span><br><span class="line">mnist</span><br></pre></td></tr></table></figure>

<p>Result:</p>
<pre><code>{&apos;DESCR&apos;: &apos;mldata.org dataset: mnist-original&apos;,
 &apos;COL_NAMES&apos;: [&apos;label&apos;, &apos;data&apos;],
 &apos;target&apos;: array([0., 0., 0., ..., 9., 9., 9.]),
 &apos;data&apos;: array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}</code></pre><p>Scikit-learn data have a dictionary (DESCR), “data” for features and “target” for labels</p>
<blockquote>
<p>Skleran 数据集具有类似dictionary的结构：</p>
<ol>
<li>DESCR键：描述数据集</li>
<li>data键：含有一个数组，每一个实例为1行，每个特征为1列</li>
<li>target键 ：一个标签的数组</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X, y = mnist[<span class="string">"data"</span>], mnist[<span class="string">"target"</span>]</span><br><span class="line">X.shape</span><br></pre></td></tr></table></figure>

<p>OUT:(70000, 784)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.shape</span><br></pre></td></tr></table></figure>

<p>OUT:(70000,)</p>
<p>Each images has 28 by 28 pixels, with each pixed containing information on color intensity from 0 (white) to 255 (black). Let’s plot it.</p>
<blockquote>
<p> 数据X共有7万张图片，每张图片有784个特征。因为图片是28×28像素，每个特征代表了一个像素点的强度，从0（白色）到255（黑色），X[36000]的数字如下，通过“y[36000]”查看其标签为“5”。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#plot X 36000</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># Find the image number 36000</span></span><br><span class="line">some_digit = X[<span class="number">36000</span>]</span><br><span class="line"><span class="comment"># Reshape vector into a matrix</span></span><br><span class="line">some_digit_image = some_digit.reshape(<span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="comment"># Plot the image. matplotlib.cm.binary is the black-white coloring scheme. </span></span><br><span class="line"><span class="comment"># Interpolation is the smoothing of colors</span></span><br><span class="line">plt.imshow(some_digit_image, cmap = matplotlib.cm.binary,</span><br><span class="line">           interpolation=<span class="string">"nearest"</span>)</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line"></span><br><span class="line">save_fig(<span class="string">"some_digit_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Saving figure some_digit_plot</code></pre><p><img src="output_16_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># As you might have guessed the correct number is:</span></span><br><span class="line">y[<span class="number">36000</span>]</span><br></pre></td></tr></table></figure>

<p>OUT: 5.0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a function to draw the picture above.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_digit</span><span class="params">(data)</span>:</span></span><br><span class="line">    image = data.reshape(<span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    plt.imshow(image, cmap = matplotlib.cm.binary,</span><br><span class="line">               interpolation=<span class="string">"nearest"</span>)</span><br><span class="line">    plt.axis(<span class="string">"off"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXTRA Let's plot 10 by 10 graph</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_digits</span><span class="params">(instances, images_per_row=<span class="number">10</span>, **options)</span>:</span></span><br><span class="line">    <span class="comment"># image size</span></span><br><span class="line">    size = <span class="number">28</span></span><br><span class="line">    <span class="comment"># number if images per row is 10 or less </span></span><br><span class="line">    images_per_row = min(len(instances), images_per_row)</span><br><span class="line">    <span class="comment"># reshape data into 28 by 28 matrix</span></span><br><span class="line">    images = [instance.reshape(size,size) <span class="keyword">for</span> instance <span class="keyword">in</span> instances]</span><br><span class="line">    <span class="comment"># Number of rows is a floor (min = 1)</span></span><br><span class="line">    n_rows = (len(instances) - <span class="number">1</span>) // images_per_row + <span class="number">1</span></span><br><span class="line">    row_images = []</span><br><span class="line">    n_empty = n_rows * images_per_row - len(instances)</span><br><span class="line">    images.append(np.zeros((size, size * n_empty)))</span><br><span class="line">    <span class="comment"># loop filling the rows of images</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> range(n_rows):</span><br><span class="line">        <span class="comment"># form each row out of images per wor</span></span><br><span class="line">        rimages = images[row * images_per_row : (row + <span class="number">1</span>) * images_per_row]</span><br><span class="line">        <span class="comment"># append images to form row</span></span><br><span class="line">        row_images.append(np.concatenate(rimages, axis=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># append rows to form matrix</span></span><br><span class="line">    image = np.concatenate(row_images, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment">#plot the image</span></span><br><span class="line">    plt.imshow(image, cmap = matplotlib.cm.binary, **options)</span><br><span class="line">    plt.axis(<span class="string">"off"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">9</span>,<span class="number">9</span>))</span><br><span class="line">example_images = np.r_[X[:<span class="number">12000</span>:<span class="number">600</span>], X[<span class="number">13000</span>:<span class="number">30600</span>:<span class="number">600</span>], X[<span class="number">30600</span>:<span class="number">60000</span>:<span class="number">590</span>]]</span><br><span class="line">print(len(example_images))</span><br><span class="line">plot_digits(example_images, images_per_row=<span class="number">10</span>)</span><br><span class="line">save_fig(<span class="string">"more_digits_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>100
Saving figure more_digits_plot</code></pre><p><img src="output_20_1.png" alt="png"></p>
<h2 id="Set-data"><a href="#Set-data" class="headerlink" title="Set data"></a>Set data</h2><p>Training: the first 60000 images; Testing: the last 10000 images.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We randomly assign 60000 obs to training data and the rest to the testing data</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X_train, X_test, y_train, y_test = X[:<span class="number">60000</span>], X[<span class="number">60000</span>:], y[:<span class="number">60000</span>], y[<span class="number">60000</span>:]</span><br><span class="line">shuffle_index = np.random.permutation(<span class="number">60000</span>)</span><br><span class="line">X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]</span><br></pre></td></tr></table></figure>

<h1 id="Training-a-Binary-classifier"><a href="#Training-a-Binary-classifier" class="headerlink" title="Training a Binary classifier"></a>Training a Binary classifier</h1><p>Let’s start with a simple task to classify one digit (5). The classifier is true for all 5s, False for all other digits.</p>
<blockquote>
<p>现在先简化问题，只尝试识别一个数字——比如数字5。那么这个“数字5检测器”就是一个二元分类器的例子，它只能区分两个类别：5和非5。先为此分类任务创建目标向量(将数字标签转换为bool型标签true代表 5，false代表 非5)：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_train_5 = (y_train == <span class="number">5</span>)</span><br><span class="line">y_test_5 = (y_test == <span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<p>Let’s start with a Stochastic Gradient Descent (SGD) classifier, using Scikit-Learn’s SGDClassifier class. This classifier has the advantage of being capable of handling very large datasets efficiently.<br>This is in part because SGD deals with training instances independently, one at a time<br>(which also makes SGD well suited for online learning), as we will see later. Let’s create<br>an SGDClassifier and train it on the whole training set:</p>
<blockquote>
<p>接着挑选一个分类器并开始训练。一个好的初始选择是随机梯度下降（SGD）分类器，使用Scikit-Learn的SGDClassifier类即可。这个分类器的优势是，能够有效处理非常大型的数据集。这部分是因为SGD独立处理训练实例，一次一个。此时先创建一个SGDClassifier并在整个训练集上进行训练：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train_5</span><br></pre></td></tr></table></figure>

<p>OUT: array([False, False, False, …, False, False, False])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The alhorithm relies on randomness and for reproducibility requires random_state parameter.</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"></span><br><span class="line">sgd_clf = SGDClassifier(max_iter=<span class="number">5</span>, random_state=<span class="number">42</span>) <span class="comment">#max_iter is the max number of passes the training data (aka epochs)</span></span><br><span class="line">sgd_clf.fit(X_train, y_train_5)</span><br></pre></td></tr></table></figure>

<pre><code>SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
              l1_ratio=0.15, learning_rate=&apos;optimal&apos;, loss=&apos;hinge&apos;, max_iter=5,
              n_iter_no_change=5, n_jobs=None, penalty=&apos;l2&apos;, power_t=0.5,
              random_state=42, shuffle=True, tol=0.001, validation_fraction=0.1,
              verbose=0, warm_start=False)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test for our digit.</span></span><br><span class="line">sgd_clf.predict([some_digit])</span><br></pre></td></tr></table></figure>

<p>OUT: array([ True])</p>
<h1 id="Performance-Measure"><a href="#Performance-Measure" class="headerlink" title="Performance Measure"></a>Performance Measure</h1><p>Evaluating a classifier is often significantly trickier than evaluating a regressor, so we will spend a large<br>part of this chapter on this topic.</p>
<h2 id="Measuring-Accuracy-Using-Cross-Validation"><a href="#Measuring-Accuracy-Using-Cross-Validation" class="headerlink" title="Measuring Accuracy Using Cross-Validation"></a>Measuring Accuracy Using Cross-Validation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's start with accuracy</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">cross_val_score(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure>

<p>OUT: array([0.96225, 0.9645 , 0.94765])</p>
<p>Wow! Above 95% accuracy (ratio of correct predictions) on all cross-validation folds?<br>This looks amazing, doesn’t it? Well, before you get too excited, let’s look at a very dumb classifier that just classifies every single image in the “not-5” class:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's look at a simple binary classifier: correctly classify 5 vs. not-5 digits.</span></span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Never5Classifier</span><span class="params">(BaseEstimator)</span>:</span></span><br><span class="line">    <span class="comment"># We don't really fit anything</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># We return a vector of zeros effectively predicting that all digits are not-5</span></span><br><span class="line">        <span class="keyword">return</span> np.zeros((len(X), <span class="number">1</span>), dtype=bool)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">never_5_clf = Never5Classifier()</span><br><span class="line">cross_val_score(never_5_clf, X_train, y_train_5, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>array([0.909  , 0.90715, 0.9128 ])</code></pre><p>Now 95% accuracy does not look that impressive. This is simply because only about 10% of the images are 5s, so if you always guess that an image is not a 5, you will be right about 90% of the time.<br>This demonstrates why accuracy is generally not the preferred performance measure for classifiers, especially when you are dealing with skewed datasets (i.e., when some classes are much more frequent than others).</p>
<blockquote>
<p>所有折叠交叉验证的准确率（正确预测的比率）超过95%？这是因为只有大约10%的图像是数字5，所以如果你猜一张图不是5，90%的情况你都是正确的，这说明准确率通常无法成为分类器的首要性能指标，特别是当你处理偏斜数据集（skewed dataset）的时候（即某些类比其他类更为频繁）。</p>
</blockquote>
<p><code>Cross_val_predict</code> predicts evaluation score for each fold. The model estimated in each prediction is based on a different fold.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_predict</span><br><span class="line">y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>)</span><br><span class="line">y_train_pred</span><br></pre></td></tr></table></figure>

<p>OUT: array([False, False, False, …, False, False, False])</p>
<h2 id="Confusion-Matrix"><a href="#Confusion-Matrix" class="headerlink" title="Confusion Matrix"></a>Confusion Matrix</h2><table>
<thead>
<tr>
<th></th>
<th>Predict No</th>
<th align="left">Peridict Yes</th>
</tr>
</thead>
<tbody><tr>
<td>Actual No</td>
<td>True Negatives</td>
<td align="left">False Positives</td>
</tr>
<tr>
<td>Actual Yes</td>
<td>False Negatives</td>
<td align="left">True Positives</td>
</tr>
</tbody></table>
<p>现在，可以使用confusion_matrix（）函数来获取混淆矩阵了。只需要给出目标类别（y_train_5）和预测类别（y_train_pred）即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line">confusion_matrix(y_train_5, y_train_pred)</span><br></pre></td></tr></table></figure>

<p>OUT: array([[53417,  1162],<br>                      [ 1350,  4071]])</p>
<blockquote>
<p>混淆矩阵中的行表示实际类别，列表示预测类别。本例中第一行表示所有“非5”（负类）的图片中：53417张被正确地分为“非5”类别（真负类），1350张被错误地分类成了“5”（假正类）；第二行表示所有“5”（正类）的图片中：1162张被错误地分为“非5”类别（假负类），4071张被正确地分在了“5”这一类别（真正类）。</p>
</blockquote>
<p>We have 10 times as many false positive as false negatives. This is because our signal (5) is rare relative to the noise (not-5), so we are much more likely to get false positives. </p>
<p><strong>Example of perfect prediction. We made a table from two identical vectors. No errors!</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_train_perfect_predictions = y_train_5</span><br><span class="line">confusion_matrix(y_train_5, y_train_perfect_predictions)</span><br></pre></td></tr></table></figure>

<p>OUT: array([[54579,     0],<br>                      [    0,  5421]])</p>
<h2 id="Precision-amp-Recall"><a href="#Precision-amp-Recall" class="headerlink" title="Precision &amp; Recall"></a>Precision &amp; Recall</h2><p>An interesting one to look at is the accuracy of the positive predictions this is called the precision(精度) of the classifier. </p>
<p>TP is the number of <strong>true positives</strong>, and FP is the number of <strong>false positives</strong>.</p>
<p>A trivial way to have perfect precision is to make one single positive prediction and ensure it is correct (precision = 1/1 = 100%). This would not be very useful since the classifier would ignore all but one positive instance. </p>
<p>So precision is typically used along with another metric named recall(召回率）, also called sensitivity(灵敏度) or true positive rate (TPR)(真正类率) : this is the ratio of positive instances that are correctly detected by the classifier .</p>
<p>FN is of course the number of false negatives.<br>Precision is the share of correctly identified positive values.  <br>Precision = $\frac{TP}{TP + FP}$ <br><br>Recall is the fraction of the true positive values identified.  <br><br>Recall = $\frac{TP}{TP + FN}$  <br><br>Accuracy is the total error rate. It works if the positive and negative value are roughly balanced.  <br><br>Accuracy = $\frac{TP + TN}{P + N}$  <br></p>
<blockquote>
<p>An illustrated confusion matrix</p>
</blockquote>
<p>![1570221657909](D:\HexoBlog\source_posts\Lecture 4 Classification\confusion matrix.png)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score</span><br><span class="line">precision_score(y_train_5, y_train_pred) </span><br><span class="line"><span class="comment"># Our of all digits we classified as fives, 77% are really fives.</span></span><br></pre></td></tr></table></figure>

<p>OUT: 0.7779476399770686</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Remember precision is TP / (TP + FP)</span></span><br><span class="line"><span class="number">4344</span> / (<span class="number">4344</span> + <span class="number">1307</span>)</span><br></pre></td></tr></table></figure>

<p>OUT: 0.7687135020350381</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Out of all fives in the data we classified as fives 80%</span></span><br><span class="line">recall_score(y_train_5, y_train_pred)</span><br></pre></td></tr></table></figure>

<p>OUT: 0.7509684560044272</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">4344</span> / (<span class="number">4344</span> + <span class="number">1077</span>)</span><br></pre></td></tr></table></figure>

<p>OUT: 0.801328168234643</p>
<p>Now your 5-detector does not look as shiny as it did when you looked at its accuracy. When it claims an image represents a 5, it is correct only 77% of the time. Moreover, it only detects 79% of the 5s.</p>
<p>It is often convenient to combine precision and recall into a single metric called the F1 score, in particular if you need a simple way to compare two classifiers. The F1 score is the harmonic mean of precision and recall . Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only get a high F1 score if both recall and precision arehigh. <br> <br><br>$$<br> F1 = \frac{2}{\frac{1}{Precision} + \frac{1}{Recall}} =<br>2 * \frac{Precision * Recall}{Precision + Recall} = \frac{TP}{TP + \frac{FN + FP}{2}}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line">f1_score(y_train_5, y_train_pred)</span><br><span class="line"><span class="comment"># F1 is a good choice for data where the share of positives is greatly different from 0.5</span></span><br></pre></td></tr></table></figure>

<p>OUT: 0.7642200112633752</p>
<h2 id="Precision-Recall-Tradeoff"><a href="#Precision-Recall-Tradeoff" class="headerlink" title="Precision\Recall Tradeoff"></a>Precision\Recall Tradeoff</h2><p>The F1 score favors classifiers that have similar precision and recall. <br><br>This is not always what you want: in some contexts you mostly care about precision, and in other contexts you really care about recall.<br></p>
<p>For example, if you trained a classifier to detect videos that are safe for kids, you would probably prefer a classifier that rejects many good videos (low recall) but keeps only safe ones (high precision), rather than a classifier<br>that has a much higher recall but lets a few really bad videos show up in your<br>product. <br><br>On the other hand, suppose you train a classifier to detect shoplifters on surveillance images: it is probably fine if your classifier has only 30% precision as long as it has 99% recall (sure, the security guards will get a few false alerts, but almost all shoplifters will get caught). <br></p>
<p>*<em>Unfortunately, you can’t have it both ways: increasing precision reduces recall, and vice versa. This is called the precision/recall tradeoff. *</em><br></p>
<p>Imagine your algorithm produces a score how much each digit resembles digit 5. Let’s the algorithm decide that it is optimal to label all digits with a score greater than 0.8 as digit 5. If we artificially increase the benchmark to 0.9, we will increase our precision (almost all digits we classify as fives would be actually fives), but we would reduce recall (we would catch fewer fives in the data).  Lowering the benchmark to 0.5 would produce an opposite effect. </p>
<p>Decision function of the SGD algorithm produces a similarity score used in the classification. By default if the score it greater than 0 the observation is classified as positive.</p>
<p>Scikit-Learn不允许直接设置阈值，但是可以访问它用于预测的决策分数。不是调用分类器的predict()方法，而是调用decision_function（）方法，这个方法返回每个实例的分数，然后就可以根据这些分数，使用任意阈值进行预测了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y_scores = sgd_clf.decision_function([some_digit])</span><br><span class="line">y_scores</span><br><span class="line"><span class="comment"># In this case the score is greteer than 0, so the digit is classified as 5.</span></span><br><span class="line">OUT: array([<span class="number">150526.40944343</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">threshold = <span class="number">0</span></span><br><span class="line">y_some_digit_pred = (y_scores &gt; threshold)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_some_digit_pred</span><br><span class="line">OUT: array([ <span class="literal">True</span>])</span><br></pre></td></tr></table></figure>

<p>SGDClassifier分类器使用的阈值是0，所以前面的代码返回结果与predict（）方法一样（也就是True）。我们来试试提升阈值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If we increase a threshhold greater than 161855..</span></span><br><span class="line">threshold = <span class="number">200000</span></span><br><span class="line">y_some_digit_pred = (y_scores &gt; threshold)</span><br><span class="line">y_some_digit_pred</span><br><span class="line">OUT: array([<span class="literal">False</span>])</span><br></pre></td></tr></table></figure>

<p>这证明了提高阈值确实可以降低召回率。这张图确实是5，当阈值为0时，分类器可以检测到该图，但是当阈值提高到200000时，就错过了这张图。 </p>
<p>那么要如何决定使用什么阈值呢？首先，使用cross_val_predict（）函数获取训练集中所有实例的分数，但是这次需要它返回的是决策分数而不是预测结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get the secision scores</span></span><br><span class="line">y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>,</span><br><span class="line">                             method=<span class="string">"decision_function"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Vector of decision score for the whole training datasets</span></span><br><span class="line">y_scores.shape</span><br><span class="line">OUT:(<span class="number">60000</span>,)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hack to work around issue #9589 in Scikit-Learn 0.19.0</span></span><br><span class="line"><span class="keyword">if</span> y_scores.ndim == <span class="number">2</span>:</span><br><span class="line">    y_scores = y_scores[:, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<p>.有了这些分数，可以使用precision_recall_curve（）函数来计算所有可能的阈值的精度和召回率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve</span><br><span class="line">precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)</span><br></pre></td></tr></table></figure>

<p>The algorithm picked a threshold that maximizes both prevision and recall. Lowering the threshold increases recall and decreases precision. Increasing the threshold leads to the opposite effect.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_precision_recall_vs_threshold</span><span class="params">(precisions, recalls, thresholds)</span>:</span></span><br><span class="line">    plt.plot(thresholds, precisions[:<span class="number">-1</span>], <span class="string">"b--"</span>, label=<span class="string">"Precision"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.plot(thresholds, recalls[:<span class="number">-1</span>], <span class="string">"g-"</span>, label=<span class="string">"Recall"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Threshold"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">"upper left"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.ylim([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">plot_precision_recall_vs_threshold(precisions, recalls, thresholds)</span><br><span class="line">plt.xlim([<span class="number">-700000</span>, <span class="number">700000</span>])</span><br><span class="line">save_fig(<span class="string">"precision_recall_vs_threshold_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_63_1.png" alt="png"></p>
<p>Precision is more bumpy. Generally there is an inverse relationship between precision and recall. However, there are exceptions. Consider the following:     2 2 3 4 5 2 4 5 | 4  5 5 5. Current precision is 3/4 = 0.75, recall 3/5 = 0.6. If we move the threshold to 2 2 3 4 5 2 4 | 5 4 5 5 5 we caught one more five, it increases both precision 4/5 = 0.8 and recall 4/5 = 0.8. This behavior is more likely in the right tail of threshold distribution when we are likely to catch 5 if we move the threshold to the right.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(y_train_pred == (y_scores &gt; <span class="number">0</span>)).all()</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If you aim for a 90% precision, which occurs around threshold = 105000</span></span><br><span class="line">y_train_pred_90 = (y_scores &gt; <span class="number">105000</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">precision_score(y_train_5, y_train_pred_90)</span><br><span class="line">OUT:<span class="number">0.9007936507936508</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">recall_score(y_train_5, y_train_pred_90)</span><br><span class="line">OUT:<span class="number">0.586238701346615</span></span><br></pre></td></tr></table></figure>

<p>现在，就可以通过轻松选择阈值来实现最佳的精度/召回率权衡了。还有一种找到好的精度/召回率权衡的方法是直接绘制精度和召回率的函数图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_precision_vs_recall</span><span class="params">(precisions, recalls)</span>:</span></span><br><span class="line">    plt.plot(recalls, precisions, <span class="string">"b-"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Recall"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Precision"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plot_precision_vs_recall(precisions, recalls)</span><br><span class="line">save_fig(<span class="string">"precision_vs_recall_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_69_1.png" alt="png"></p>
<h2 id="ROC-Curves"><a href="#ROC-Curves" class="headerlink" title="ROC Curves"></a>ROC Curves</h2><p>The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It plots the true positive rate (another name for recall) against the false positive rate. <br><br>$TPR = \frac{TP}{TP + FN}$ <br><br>$FPR = \frac{FP}{FP + TN}$</p>
<p>还有一种经常与二元分类器一起使用的工具，叫作受试者工作特征曲线（简称ROC）。它与精度/召回率曲线非常相似，但绘制的不是精度和召回率，而是真正类率（召回率的另一名称）和假正类率（FPR）。FPR是被错误分为正类的负类实例比率。它等于1减去真负类率（TNR），后者是被正确分类为负类的负类实例比率，也称为特异度。因此，ROC曲线绘制的是灵敏度和（1-特异度）的关系。</p>
<p>Use roc_curve() to calculate TPR and FPR for various shreshold values:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_roc_curve</span><span class="params">(fpr, tpr, label=None)</span>:</span></span><br><span class="line">    plt.plot(fpr, tpr, linewidth=<span class="number">2</span>, label=label)</span><br><span class="line">    plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">'k--'</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    plt.xlabel(<span class="string">'False Positive Rate'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'True Positive Rate'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plot_roc_curve(fpr, tpr)</span><br><span class="line">save_fig(<span class="string">"roc_curve_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_73_1.png" alt="png"></p>
<p>Tradeoff: the higher the recall (TPR), the more false positives (FPR) the classifier produces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner). <br> <br></p>
<p>One way to compare classifiers is to measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC AUC. Hi AUC works for many different TPR-FPR choice. </p>
<blockquote>
<p>虚线表示纯随机分类器的ROC曲线；一个优秀的分类器应该离这条线越远越好（向左上角）。有一种比较分类器的方法是测量曲线下面积（AUC）。完美的分类器的ROC AUC等于1，而纯随机分类器的ROC AUC等于0.5。</p>
</blockquote>
<p>Use PR curve when the positive class is rare and you care more about false positives than about false negatives. Use ROC, AUC otherwise. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">roc_auc_score(y_train_5, y_scores)</span><br><span class="line">OUT:<span class="number">0.9562435587387078</span></span><br></pre></td></tr></table></figure>

<p>Let’s train a RandomForestClassifier and compare its ROC curve and ROC AUC<br>score to the SGDClassifier. <br></p>
<p>We calculate score for each instance in the training data. RandomForestClassifier class does not have a decision_function(),  instead it has predict_proba(). Each instance is assigned a probability of belonging to each class.</p>
<blockquote>
<p>训练一个RandomForestClassifier分类器，并比较它和SGDClassifier分类器的ROC曲线和ROC AUC分数。首先，获取训练集中每个实例的分数。但是由于它的工作方式不同（参见第7章），RandomForestClassifier类没有decision_function（）方法，相反，它有的是dict_proba（）方法。Scikit-Learn的分类器通常都会有这两种方法的其中一种。dict_proba（）方法会返回一个数组，其中每行为一个实例，每列代表一个类别，意思是某个给定实例属于某个给定类别的概率（例如，这张图片有70%的可能是数字5）：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">forest_clf = RandomForestClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=<span class="number">3</span>,</span><br><span class="line">                                    method=<span class="string">"predict_proba"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_scores_forest = y_probas_forest[:, <span class="number">1</span>] <span class="comment"># score = probability of positive class</span></span><br><span class="line"><span class="comment"># Get fit data from the random forest</span></span><br><span class="line">fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(fpr, tpr, <span class="string">"b:"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"SGD"</span>)</span><br><span class="line">plot_roc_curve(fpr_forest, tpr_forest, <span class="string">"Random Forest"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">save_fig(<span class="string">"roc_curve_comparison_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Saving figure roc_curve_comparison_plot</code></pre><p><img src="output_79_1.png" alt="png"></p>
<p>Random forest has very good fit statistics</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">roc_auc_score(y_train_5, y_scores_forest)</span><br><span class="line">OUT:<span class="number">0.9931243366003829</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=<span class="number">3</span>)</span><br><span class="line">precision_score(y_train_5, y_train_pred_forest)</span><br><span class="line">OUT:<span class="number">0.9852973447443494</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">recall_score(y_train_5, y_train_pred_forest)</span><br><span class="line">OUT:<span class="number">0.8282604685482383</span></span><br></pre></td></tr></table></figure>

<p>由于ROC曲线与精度/召回率（或PR）曲线非常相似，因此你可能会问如何决定使用哪种曲线。有一个经验法则是，当正类非常少见或者你更关注假正类而不是假负类时，你应该选择PR曲线，反之则是ROC曲线。例如，看前面的ROC曲线图（以及ROC  AUC分数），你可能会觉得分类器真不错。但这主要是因为跟负类（非5）相比，正类（数字5）的数量真得很少。相比之下，PR曲线清楚地说明分类器还有改进的空间（曲线还可以更接近右上角）。</p>
<h1 id="Multiclass-classification"><a href="#Multiclass-classification" class="headerlink" title="Multiclass classification"></a>Multiclass classification</h1><p>Whereas binary classifiers distinguish between two classes, multiclass classifiers can distinguish between more than two classes.Some algorithms (such as Random Forest classifiers or naive Bayes classifiers) are capable of handling multiple classes directly. Others (such as Support Vector Machine classifiers or Linear classifiers) are strictly binary classifiers. <br> </p>
<p>You can use binary classifiers to estimate multiclass classification: <br></p>
<ol>
<li><p>Create 10 binary classifier for each digit versus the rest of the digits, similar to our five or not-five classifiers. After we estimate 10 classifiers for each digit we set a label for the classifier that produced the highest score. This is called the one-versus-all (OvA) strategy (also called one-versus-the-rest). <br></p>
</li>
<li><p>Another strategy is o train binary classifier for each paid of digits: 0s vs 1s, 0 vs 4s, etc. This is called the one-versus-one (OvO) strategy. If there are N classes, you need to train N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45 binary classifiers! For each digit we pick the label with the highest average score against 9 other digits. The main advantage of OvO is that each classifier only needs to be trained on the part of the training set for the two classes that it must distinguish. <br></p>
</li>
</ol>
<p>Algorithms that scales poorly (like SVM) are preferable for OVO, which reduce the data used in the estimation. However, most often  OvA is preferred and it is a usually the default option for binary classifiers other than SVM.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's try SGD algorithm.</span></span><br><span class="line">sgd_clf.fit(X_train, y_train) <span class="comment">#y_train not y_train_5</span></span><br><span class="line">sgd_clf.predict([some_digit])</span><br><span class="line">OUT: array([<span class="number">5.</span>])</span><br></pre></td></tr></table></figure>

<p>这段代码使用原始目标类别0到9（y_train）在训练集上对SGDClassifier进行训练，而不是以“5”和“剩余”作为目标类别（y_train_5）。然后做出预测（在本例中预测正确）。而在内部，Scikit-Learn实际上训练了10个二元分类器，获得它们对图片的决策分数，然后选择了分数最高的类别。</p>
<p>想要知道是不是这样，可以调用decision_function（）方法。它会返回10个分数，每个类别1个，而不再是每个实例返回1个分数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's look at the 10 estimated scores for the digit. 5 has the higher score  161855.74572176.</span></span><br><span class="line">some_digit_scores = sgd_clf.decision_function([some_digit])</span><br><span class="line">some_digit_scores</span><br></pre></td></tr></table></figure>

<pre><code>OUT:array([[-152619.46799791, -441052.22074349, -249930.3138537 ,
        -237258.35168498, -447251.81933158,  120565.05820991,
        -834139.15404835, -188142.48490477, -555223.79499145,
        -536978.92518594]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Which vector has the highest score</span></span><br><span class="line">np.argmax(some_digit_scores)</span><br><span class="line">OUT: <span class="number">5</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sgd_clf.classes_</span><br><span class="line">OUT: array([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sgd_clf.classes_[<span class="number">5</span>]</span><br><span class="line">OUT: <span class="number">5.0</span></span><br></pre></td></tr></table></figure>

<p>当训练分类器时，目标类别的列表会存储在classes_这个属性中，按值的大小排序。在本例里，classes_数组中每个类别的索引正好对应其类别本身（例如，索引上第5个类别正好是数字5这个类别），但是一般来说，不会这么恰巧。</p>
<p> 如果想要强制Scikit-Learn使用一对一或者一对多策略，可以使用OneVsOne Classifier或OneVsRestClassifier类。只需要创建一个实例，然后将二元分类器传给其构造函数。例如，下面这段代码使用OvO策略，基于SGDClassifier创建了一个多类别分类器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Here is the example of the OVO classifier.</span></span><br><span class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsOneClassifier</span><br><span class="line">ovo_clf = OneVsOneClassifier(SGDClassifier(max_iter=<span class="number">5</span>, random_state=<span class="number">42</span>))</span><br><span class="line">ovo_clf.fit(X_train, y_train)</span><br><span class="line">ovo_clf.predict([some_digit])</span><br><span class="line">OUT: array([<span class="number">5.</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># As expected we estimated 45 binary classfiers</span></span><br><span class="line">len(ovo_clf.estimators_)</span><br></pre></td></tr></table></figure>

<pre><code>45</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">forest_clf.fit(X_train, y_train)</span><br><span class="line">forest_clf.predict([some_digit])</span><br></pre></td></tr></table></figure>

<pre><code>array([5.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">forest_clf.predict_proba([some_digit])</span><br><span class="line"><span class="comment"># This classifier returns probabilities. The probability of digit 5 is 80%, 10% that it is 0 and 10% that it is 3.</span></span><br></pre></td></tr></table></figure>

<pre><code>array([[0.1, 0. , 0. , 0.1, 0. , 0.8, 0. , 0. , 0. , 0. ]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train</span><br></pre></td></tr></table></figure>

<pre><code>array([1., 6., 6., ..., 0., 2., 9.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Perfomance of OVA SGD.</span></span><br><span class="line">cross_val_score(sgd_clf, X_train, y_train, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br><span class="line"><span class="comment"># Accuracy of three folds</span></span><br></pre></td></tr></table></figure>

<pre><code>array([0.84993001, 0.81769088, 0.84707706])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Usually scaling the data improves the accracy</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))</span><br><span class="line">cross_val_score(sgd_clf, X_train_scaled, y_train, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>array([0.91211758, 0.9099955 , 0.90643597])</code></pre><h1 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h1><p>Real life project has multiple steps such as exploring data preparation options, trying out multiple models, shortlisting the best ones and fine-tuning their hyperparameters using GridSearchCV, and automating as much as possible.</p>
<p>Here, we will assume that you have found a promising model and you want to find ways to improve it. One way to do this is to analyze the types of errors it makes.</p>
<p>首先，看看混淆矩阵。就像之前做的，使用cross_val_predict（）函数进行预测，然后调用confusion_matrix（）函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's look at the confusion matrix</span></span><br><span class="line">y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=<span class="number">3</span>)</span><br><span class="line">conf_mx = confusion_matrix(y_train, y_train_pred)</span><br><span class="line">conf_mx</span><br></pre></td></tr></table></figure>

<pre><code>array([[5749,    4,   22,   11,   11,   40,   36,   11,   36,    3],
       [   2, 6490,   43,   24,    6,   41,    8,   12,  107,    9],
       [  53,   42, 5330,   99,   87,   24,   89,   58,  159,   17],
       [  46,   41,  126, 5361,    1,  241,   34,   59,  129,   93],
       [  20,   30,   35,   10, 5369,    8,   48,   38,   76,  208],
       [  73,   45,   30,  194,   64, 4614,  106,   30,  170,   95],
       [  41,   30,   46,    2,   44,   91, 5611,    9,   43,    1],
       [  26,   18,   73,   30,   52,   11,    4, 5823,   14,  214],
       [  63,  159,   69,  168,   15,  172,   54,   26, 4997,  128],
       [  39,   39,   27,   90,  177,   40,    2,  230,   78, 5227]])</code></pre><p>Digit 5 has the lower correct prediction 4,582, digit 1 has the highest correct prediction 6,493. Let’s look at the graphical representation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_confusion_matrix</span><span class="params">(matrix)</span>:</span></span><br><span class="line">    <span class="string">"""If you prefer color and a colorbar"""</span></span><br><span class="line">    <span class="comment"># Plot 8 by 8 matrix</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    <span class="comment"># Color intensity comes from the matrix.</span></span><br><span class="line">    cax = ax.matshow(matrix)</span><br><span class="line">    fig.colorbar(cax)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Digit five is darker, digit 1 is ligher. </span></span><br><span class="line">plt.matshow(conf_mx, cmap=plt.cm.gray)</span><br><span class="line">plt.xlabel(<span class="string">"Predicted"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Actual"</span>)</span><br><span class="line">save_fig(<span class="string">"confusion_matrix_plot"</span>, tight_layout=<span class="literal">False</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_103_1.png" alt="png"></p>
<p>混淆矩阵看起来很不错，因为大多数图片都在主对角线上，这说明它们被正确分类。数字5看起来比其他数字稍稍暗一些，这可能意味着数据集中数字5的图片较少，也可能是分类器在数字5上的执行效果不如在其他数字上好。实际上，你可能会验证这两者都属实。 让我们把焦点放在错误上。首先，你需要将混淆矩阵中的每个值除以相应类别中的图片数量，这样你比较的就是错误率而不是错误的绝对值（后者对图片数量较多的类别不公平）：</p>
<p>Off-diagonal elements are the errors. We can look at most common misclassifications. From the matrix, we can see that the most common error is the confusion of digits 7 and 9 (236, 223).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">row_sums = conf_mx.sum(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">norm_conf_mx = conf_mx / row_sums</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We will no plot a diagal elements, otherwise their intensity would dominated the chart. </span></span><br><span class="line">np.fill_diagonal(norm_conf_mx, <span class="number">0</span>) <span class="comment">#用0填充对角线，只保留错误，重新绘制结果： </span></span><br><span class="line">plt.matshow(norm_conf_mx, cmap=plt.cm.gray)</span><br><span class="line">plt.xlabel(<span class="string">"Predicted"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Actual"</span>)</span><br><span class="line">save_fig(<span class="string">"confusion_matrix_errors_plot"</span>, tight_layout=<span class="literal">False</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_106_1.png" alt="png"></p>
<p>Largest error: symmetric confusion of 7/9, and 3/5. Other errors are asymmetric: the classification confuses 6 for 8, but not the 8 for 6. <br> </p>
<p>Solutions: <br><br>1 gather more training data for the confusing digits <br><br>2 setup closed loops that would run the algorithm until the desired error rate is achieved <br><br>3 Preprocess the data engineering new features <br><br>Analyzing individual errors can also be a good way to gain insights on what your classifier is doing and why it is failing, but it is more difficult and time-consuming. Let’s plot examples of 3s and 5s.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cl_a, cl_b = <span class="number">3</span>, <span class="number">5</span></span><br><span class="line">X_aa = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_a)]</span><br><span class="line">X_ab = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_b)]</span><br><span class="line">X_ba = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_a)]</span><br><span class="line">X_bb = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_b)]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">plt.subplot(<span class="number">221</span>); plot_digits(X_aa[:<span class="number">25</span>], images_per_row=<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">222</span>); plot_digits(X_ab[:<span class="number">25</span>], images_per_row=<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">223</span>); plot_digits(X_ba[:<span class="number">25</span>], images_per_row=<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">224</span>); plot_digits(X_bb[:<span class="number">25</span>], images_per_row=<span class="number">5</span>)</span><br><span class="line">save_fig(<span class="string">"error_analysis_digits_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Saving figure error_analysis_digits_plot</code></pre><p><img src="output_108_1.png" alt="png"></p>
<p>Top-left are the correcly classified 3s, top-right are 3s misclassified for 5s. Bottom-left are 5s misclassified as 3s, bottom-right are the correctly classified 5s.  <br></p>
<p>Humans can easily recognize most of the errors. SGDClassifier assigns a weight to each pixel intensity, as 3s and 5s are differ by only few pixel it’s easy to confuse them by shifting and rotation of the small line that makes a difference between 3 and 5. </p>
<p>Another solution: center and rotate all digits in the same way.</p>
<h1 id="Multilabel-classification"><a href="#Multilabel-classification" class="headerlink" title="Multilabel classification"></a>Multilabel classification</h1><p>Mutlilabel classification marks each object relative to several classes. For example, if we are looking for three people in the picture Alice, Bob, and Charlie, then the classification would return  [1, 0, 1]  if it thinks that Alice and Charlie is in the picture, but not the Bob. Usually restriction on the multiple label help us to estimate this system. For example, if Bob and Charlie look alike we just need to find out two objects that resembles [Bob, Charlie], this is often easier that trying to distinguish similar objects. </p>
<p>We create two classes of digits large (&gt;6) and odd. We use the KNN (Kth nearest neighbor) , which is a weighted average of classes for the K nearest neighbors. </p>
<p>为了阐释清楚，这里不讨论面部识别，让我们来看一个更为简单的例子：</p>
<p>在某些情况下，你希望分类器为每个实例产出多个类别。下面的代码会创建一个y_multilabel数组，其中包含两个数字图片的目标标签：第一个表示数字是否是大数（7、8、9），第二个表示是否为奇数。下一行创建一个KNeighborsClassifier实例（它支持多标签分类，不是所有的分类器都支持），然后使用多个目标数组对它进行训练。现在用它做一个预测，注意它输出的两个标签：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">y_train_large = (y_train &gt;= <span class="number">7</span>)</span><br><span class="line">y_train_odd = (y_train % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line">y_multilabel = np.c_[y_train_large, y_train_odd]</span><br><span class="line"></span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line">knn_clf.fit(X_train, y_multilabel)</span><br></pre></td></tr></table></figure>

<pre><code>KNeighborsClassifier(algorithm=&apos;auto&apos;, leaf_size=30, metric=&apos;minkowski&apos;,
                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,
                     weights=&apos;uniform&apos;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn_clf.predict([some_digit])</span><br></pre></td></tr></table></figure>

<pre><code>OUT:array([[False,  True]])</code></pre><p>And it gets it right! The digit 5 is indeed not large (False) and odd (True).</p>
<p><strong>Warning</strong>: the following cell may take a very long time (possibly hours depending on your hardware). KNN can be a very slow algorithm. F1 can weight classes by importance. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=<span class="number">3</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">f1_score(y_multilabel, y_train_knn_pred, average=<span class="string">"macro"</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Multioutput-Classification"><a href="#Multioutput-Classification" class="headerlink" title="Multioutput Classification"></a>Multioutput Classification</h1><p>我们即将讨论的最后一种分类任务叫作多输出-多类别分类（或简单地称为多输出分类）。简单来说，它是多标签分类的泛化，其标签也可以是多种类别的（比如它可以有两个以上可能的值）。<br>     为了说明这一点，构建一个系统用来去除图片中的噪声。给它输入一张有噪声的图片，它将输出一张干净的数字图片，跟其他MNIST图片一样，以像素强度的一个数组作为呈现方式。请注意，这个分类器的输出是多个标签（一个像素点一个标签），每个标签可以有多个值（像素强度范围为0到225）。所以这是个多输出分类器系统的例子。</p>
<p>先从创建训练集和测试集开始，使用NumPy的randint（）函数为MNIST图片的像素强度增加噪声：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">noise = np.random.randint(0, 100, (len(X_train), 784))</span><br><span class="line">X_train_mod = X_train + noise</span><br><span class="line">noise = np.random.randint(0, 100, (len(X_test), 784))</span><br><span class="line">X_test_mod = X_test + noise</span><br><span class="line">y_train_mod = X_train</span><br><span class="line">y_test_mod = X_test</span><br><span class="line"> </span><br><span class="line">#观察一下其中一个数据</span><br><span class="line">some_index = 5500</span><br><span class="line">plt.subplot(121); plot_digit(X_test_mod[some_index])</span><br><span class="line">plt.subplot(122); plot_digit(y_test_mod[some_index])</span><br><span class="line">save_fig(&quot;noisy_digit_example_plot&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>![img](D:\HexoBlog\source_posts\Lecture 4 Classification\multioutput.png)</p>
<p>左边是有噪声的输入图片，右边是干净的目标图片。现在通过训练分类器，清洗这张图片</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">knn_clf.fit(X_train_mod, y_train_mod)</span><br><span class="line">clean_digit = knn_clf.predict([X_test_mod[some_index]])</span><br><span class="line">plot_digit(clean_digit)</span><br><span class="line">save_fig(&quot;cleaned_digit_example_plot&quot;)</span><br></pre></td></tr></table></figure>

<p>![img](D:\HexoBlog\source_posts\Lecture 4 Classification\multioutput_output.png)</p>
<h1 id="Extra-material"><a href="#Extra-material" class="headerlink" title="Extra material"></a>Extra material</h1><h2 id="Dummy-ie-random-classifier"><a href="#Dummy-ie-random-classifier" class="headerlink" title="Dummy (ie. random) classifier"></a>Dummy (ie. random) classifier</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.dummy <span class="keyword">import</span> DummyClassifier</span><br><span class="line">dmy_clf = DummyClassifier()</span><br><span class="line">y_probas_dmy = cross_val_predict(dmy_clf, X_train, y_train_5, cv=<span class="number">3</span>, method=<span class="string">"predict_proba"</span>)</span><br><span class="line">y_scores_dmy = y_probas_dmy[:, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fprr, tprr, thresholdsr = roc_curve(y_train_5, y_scores_dmy)</span><br><span class="line">plot_roc_curve(fprr, tprr)</span><br></pre></td></tr></table></figure>

<h2 id="KNN-classifier"><a href="#KNN-classifier" class="headerlink" title="KNN classifier"></a>KNN classifier</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">knn_clf = KNeighborsClassifier(n_jobs=<span class="number">-1</span>, weights=<span class="string">'distance'</span>, n_neighbors=<span class="number">4</span>)</span><br><span class="line">knn_clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_knn_pred = knn_clf.predict(X_test)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">accuracy_score(y_test, y_knn_pred)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.ndimage.interpolation <span class="keyword">import</span> shift</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shift_digit</span><span class="params">(digit_array, dx, dy, new=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> shift(digit_array.reshape(<span class="number">28</span>, <span class="number">28</span>), [dy, dx], cval=new).reshape(<span class="number">784</span>)</span><br><span class="line"></span><br><span class="line">plot_digit(shift_digit(some_digit, <span class="number">5</span>, <span class="number">1</span>, new=<span class="number">100</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_train_expanded = [X_train]</span><br><span class="line">y_train_expanded = [y_train]</span><br><span class="line"><span class="keyword">for</span> dx, dy <span class="keyword">in</span> ((<span class="number">1</span>, <span class="number">0</span>), (<span class="number">-1</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">-1</span>)):</span><br><span class="line">    shifted_images = np.apply_along_axis(shift_digit, axis=<span class="number">1</span>, arr=X_train, dx=dx, dy=dy)</span><br><span class="line">    X_train_expanded.append(shifted_images)</span><br><span class="line">    y_train_expanded.append(y_train)</span><br><span class="line"></span><br><span class="line">X_train_expanded = np.concatenate(X_train_expanded)</span><br><span class="line">y_train_expanded = np.concatenate(y_train_expanded)</span><br><span class="line">X_train_expanded.shape, y_train_expanded.shape</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn_clf.fit(X_train_expanded, y_train_expanded)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_knn_expanded_pred = knn_clf.predict(X_test)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy_score(y_test, y_knn_expanded_pred)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ambiguous_digit = X_test[<span class="number">2589</span>]</span><br><span class="line">knn_clf.predict_proba([ambiguous_digit])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_digit(ambiguous_digit)</span><br></pre></td></tr></table></figure>

<h1 id="Exercise-solutions"><a href="#Exercise-solutions" class="headerlink" title="Exercise solutions"></a>Exercise solutions</h1><h2 id="1-An-MNIST-Classifier-With-Over-97-Accuracy"><a href="#1-An-MNIST-Classifier-With-Over-97-Accuracy" class="headerlink" title="1. An MNIST Classifier With Over 97% Accuracy"></a>1. An MNIST Classifier With Over 97% Accuracy</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">param_grid = [&#123;<span class="string">'weights'</span>: [<span class="string">"uniform"</span>, <span class="string">"distance"</span>], <span class="string">'n_neighbors'</span>: [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]&#125;]</span><br><span class="line"></span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line">grid_search = GridSearchCV(knn_clf, param_grid, cv=<span class="number">5</span>, verbose=<span class="number">3</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">grid_search.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid_search.best_params_</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid_search.best_score_</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">y_pred = grid_search.predict(X_test)</span><br><span class="line">accuracy_score(y_test, y_pred)</span><br></pre></td></tr></table></figure>

<h2 id="2-Data-Augmentation"><a href="#2-Data-Augmentation" class="headerlink" title="2. Data Augmentation"></a>2. Data Augmentation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.ndimage.interpolation <span class="keyword">import</span> shift</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shift_image</span><span class="params">(image, dx, dy)</span>:</span></span><br><span class="line">    image = image.reshape((<span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">    shifted_image = shift(image, [dy, dx], cval=<span class="number">0</span>, mode=<span class="string">"constant"</span>)</span><br><span class="line">    <span class="keyword">return</span> shifted_image.reshape([<span class="number">-1</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">image = X_train[<span class="number">1000</span>]</span><br><span class="line">shifted_image_down = shift_image(image, <span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">shifted_image_left = shift_image(image, <span class="number">-5</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">3</span>))</span><br><span class="line">plt.subplot(<span class="number">131</span>)</span><br><span class="line">plt.title(<span class="string">"Original"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.imshow(image.reshape(<span class="number">28</span>, <span class="number">28</span>), interpolation=<span class="string">"nearest"</span>, cmap=<span class="string">"Greys"</span>)</span><br><span class="line">plt.subplot(<span class="number">132</span>)</span><br><span class="line">plt.title(<span class="string">"Shifted down"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.imshow(shifted_image_down.reshape(<span class="number">28</span>, <span class="number">28</span>), interpolation=<span class="string">"nearest"</span>, cmap=<span class="string">"Greys"</span>)</span><br><span class="line">plt.subplot(<span class="number">133</span>)</span><br><span class="line">plt.title(<span class="string">"Shifted left"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.imshow(shifted_image_left.reshape(<span class="number">28</span>, <span class="number">28</span>), interpolation=<span class="string">"nearest"</span>, cmap=<span class="string">"Greys"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_train_augmented = [image <span class="keyword">for</span> image <span class="keyword">in</span> X_train]</span><br><span class="line">y_train_augmented = [label <span class="keyword">for</span> label <span class="keyword">in</span> y_train]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> dx, dy <span class="keyword">in</span> ((<span class="number">1</span>, <span class="number">0</span>), (<span class="number">-1</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">-1</span>)):</span><br><span class="line">    <span class="keyword">for</span> image, label <span class="keyword">in</span> zip(X_train, y_train):</span><br><span class="line">        X_train_augmented.append(shift_image(image, dx, dy))</span><br><span class="line">        y_train_augmented.append(label)</span><br><span class="line"></span><br><span class="line">X_train_augmented = np.array(X_train_augmented)</span><br><span class="line">y_train_augmented = np.array(y_train_augmented)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">shuffle_idx = np.random.permutation(len(X_train_augmented))</span><br><span class="line">X_train_augmented = X_train_augmented[shuffle_idx]</span><br><span class="line">y_train_augmented = y_train_augmented[shuffle_idx]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn_clf = KNeighborsClassifier(**grid_search.best_params_)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn_clf.fit(X_train_augmented, y_train_augmented)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred = knn_clf.predict(X_test)</span><br><span class="line">accuracy_score(y_test, y_pred)</span><br></pre></td></tr></table></figure>

<p>By simply augmenting the data, we got a 0.5% accuracy boost. :)</p>
<h2 id="3-Tackle-the-Titanic-dataset"><a href="#3-Tackle-the-Titanic-dataset" class="headerlink" title="3. Tackle the Titanic dataset"></a>3. Tackle the Titanic dataset</h2><p>The goal is to predict whether or not a passenger survived based on attributes such as their age, sex, passenger class, where they embarked and so on.</p>
<p>First, login to <a href="https://www.kaggle.com/" target="_blank" rel="noopener">Kaggle</a> and go to the <a href="https://www.kaggle.com/c/titanic" target="_blank" rel="noopener">Titanic challenge</a> to download <code>train.csv</code> and <code>test.csv</code>. Save them to the <code>datasets/titanic</code> directory.</p>
<p>Next, let’s load the data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">TITANIC_PATH = os.path.join(<span class="string">"datasets"</span>, <span class="string">"titanic"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data = load_titanic_data(<span class="string">"train.csv"</span>)</span><br><span class="line">test_data = load_titanic_data(<span class="string">"test.csv"</span>)</span><br></pre></td></tr></table></figure>

<p>The data is already split into a training set and a test set. However, the test data does <em>not</em> contain the labels: your goal is to train the best model you can using the training data, then make your predictions on the test data and upload them to Kaggle to see your final score.</p>
<p>Let’s take a peek at the top few rows of the training set:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.head()</span><br></pre></td></tr></table></figure>

<p>The attributes have the following meaning:</p>
<ul>
<li><strong>Survived</strong>: that’s the target, 0 means the passenger did not survive, while 1 means he/she survived.</li>
<li><strong>Pclass</strong>: passenger class.</li>
<li><strong>Name</strong>, <strong>Sex</strong>, <strong>Age</strong>: self-explanatory</li>
<li><strong>SibSp</strong>: how many siblings &amp; spouses of the passenger aboard the Titanic.</li>
<li><strong>Parch</strong>: how many children &amp; parents of the passenger aboard the Titanic.</li>
<li><strong>Ticket</strong>: ticket id</li>
<li><strong>Fare</strong>: price paid (in pounds)</li>
<li><strong>Cabin</strong>: passenger’s cabin number</li>
<li><strong>Embarked</strong>: where the passenger embarked the Titanic</li>
</ul>
<p>Let’s get more info to see how much data is missing:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.info()</span><br></pre></td></tr></table></figure>

<p>Okay, the <strong>Age</strong>, <strong>Cabin</strong> and <strong>Embarked</strong> attributes are sometimes null (less than 891 non-null), especially the <strong>Cabin</strong> (77% are null). We will ignore the <strong>Cabin</strong> for now and focus on the rest. The <strong>Age</strong> attribute has about 19% null values, so we will need to decide what to do with them. Replacing null values with the median age seems reasonable.</p>
<p>The <strong>Name</strong> and <strong>Ticket</strong> attributes may have some value, but they will be a bit tricky to convert into useful numbers that a model can consume. So for now, we will ignore them.</p>
<p>Let’s take a look at the numerical attributes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.describe()</span><br></pre></td></tr></table></figure>

<ul>
<li>Yikes, only 38% <strong>Survived</strong>. :(  That’s close enough to 40%, so accuracy will be a reasonable metric to evaluate our model.</li>
<li>The mean <strong>Fare</strong> was £32.20, which does not seem so expensive (but it was probably a lot of money back then).</li>
<li>The mean <strong>Age</strong> was less than 30 years old.</li>
</ul>
<p>Let’s check that the target is indeed 0 or 1:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"Survived"</span>].value_counts()</span><br></pre></td></tr></table></figure>

<p>Now let’s take a quick look at all the categorical attributes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"Pclass"</span>].value_counts()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"Sex"</span>].value_counts()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"Embarked"</span>].value_counts()</span><br></pre></td></tr></table></figure>

<p>The Embarked attribute tells us where the passenger embarked: C=Cherbourg, Q=Queenstown, S=Southampton.</p>
<p>Now let’s build our preprocessing pipelines. We will reuse the <code>DataframeSelector</code> we built in the previous chapter to select specific attributes from the <code>DataFrame</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"></span><br><span class="line"><span class="comment"># A class to select numerical or categorical columns </span></span><br><span class="line"><span class="comment"># since Scikit-Learn doesn't handle DataFrames yet</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataFrameSelector</span><span class="params">(BaseEstimator, TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, attribute_names)</span>:</span></span><br><span class="line">        self.attribute_names = attribute_names</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> X[self.attribute_names]</span><br></pre></td></tr></table></figure>

<p>Let’s build the pipeline for the numerical attributes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line"></span><br><span class="line">imputer = Imputer(strategy=<span class="string">"median"</span>)</span><br><span class="line"></span><br><span class="line">num_pipeline = Pipeline([</span><br><span class="line">        (<span class="string">"select_numeric"</span>, DataFrameSelector([<span class="string">"Age"</span>, <span class="string">"SibSp"</span>, <span class="string">"Parch"</span>, <span class="string">"Fare"</span>])),</span><br><span class="line">        (<span class="string">"imputer"</span>, Imputer(strategy=<span class="string">"median"</span>)),</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_pipeline.fit_transform(train_data)</span><br></pre></td></tr></table></figure>

<p>We will also need an imputer for the string categorical columns (the regular <code>Imputer</code> does not work on those):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inspired from stackoverflow.com/questions/25239958</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MostFrequentImputer</span><span class="params">(BaseEstimator, TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        self.most_frequent_ = pd.Series([X[c].value_counts().index[<span class="number">0</span>] <span class="keyword">for</span> c <span class="keyword">in</span> X],</span><br><span class="line">                                        index=X.columns)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> X.fillna(self.most_frequent_)</span><br></pre></td></tr></table></figure>

<p>We can convert each categorical value to a one-hot vector using a <code>OneHotEncoder</code>. Right now this class can only handle integer categorical inputs, but in Scikit-Learn 0.20 it will also handle string categorical inputs (see <a href="https://github.com/scikit-learn/scikit-learn/issues/10521" target="_blank" rel="noopener">PR #10521</a>). So for now we import it from <code>future_encoders.py</code>, but when Scikit-Learn 0.20 is released, you can import it from <code>sklearn.preprocessing</code> instead:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> future_encoders <span class="keyword">import</span> OneHotEncoder</span><br></pre></td></tr></table></figure>

<p>Now we can build the pipeline for the categorical attributes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat_pipeline = Pipeline([</span><br><span class="line">        (<span class="string">"select_cat"</span>, DataFrameSelector([<span class="string">"Pclass"</span>, <span class="string">"Sex"</span>, <span class="string">"Embarked"</span>])),</span><br><span class="line">        (<span class="string">"imputer"</span>, MostFrequentImputer()),</span><br><span class="line">        (<span class="string">"cat_encoder"</span>, OneHotEncoder(sparse=<span class="literal">False</span>)),</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat_pipeline.fit_transform(train_data)</span><br></pre></td></tr></table></figure>

<p>Finally, let’s join the numerical and categorical pipelines:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> FeatureUnion</span><br><span class="line">preprocess_pipeline = FeatureUnion(transformer_list=[</span><br><span class="line">        (<span class="string">"num_pipeline"</span>, num_pipeline),</span><br><span class="line">        (<span class="string">"cat_pipeline"</span>, cat_pipeline),</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>

<p>Cool! Now we have a nice preprocessing pipeline that takes the raw data and outputs numerical input features that we can feed to any Machine Learning model we want.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train = preprocess_pipeline.fit_transform(train_data)</span><br><span class="line">X_train</span><br></pre></td></tr></table></figure>

<p>Let’s not forget to get the labels:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train = train_data[<span class="string">"Survived"</span>]</span><br></pre></td></tr></table></figure>

<p>We are now ready to train a classifier. Let’s start with an <code>SVC</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">svm_clf = SVC()</span><br><span class="line">svm_clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<p>Great, our model is trained, let’s use it to make predictions on the test set:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_test = preprocess_pipeline.transform(test_data)</span><br><span class="line">y_pred = svm_clf.predict(X_test)</span><br></pre></td></tr></table></figure>

<p>And now we could just build a CSV file with these predictions (respecting the format excepted by Kaggle), then upload it and hope for the best. But wait! We can do better than hope. Why don’t we use cross-validation to have an idea of how good our model is?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line">svm_scores = cross_val_score(svm_clf, X_train, y_train, cv=<span class="number">10</span>)</span><br><span class="line">svm_scores.mean()</span><br></pre></td></tr></table></figure>

<p>Okay, over 73% accuracy, clearly better than random chance, but it’s not a great score. Looking at the <a href="https://www.kaggle.com/c/titanic/leaderboard" target="_blank" rel="noopener">leaderboard</a> for the Titanic competition on Kaggle, you can see that you need to reach above 80% accuracy to be within the top 10% Kagglers. Some reached 100%, but since you can easily find the <a href="https://www.encyclopedia-titanica.org/titanic-victims/" target="_blank" rel="noopener">list of victims</a> of the Titanic, it seems likely that there was little Machine Learning involved in their performance! ;-) So let’s try to build a model that reaches 80% accuracy.</p>
<p>Let’s try a <code>RandomForestClassifier</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line">forest_clf = RandomForestClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=<span class="number">10</span>)</span><br><span class="line">forest_scores.mean()</span><br></pre></td></tr></table></figure>

<p>That’s much better!</p>
<p>Instead of just looking at the mean accuracy across the 10 cross-validation folds, let’s plot all 10 scores for each model, along with a box plot highlighting the lower and upper quartiles, and “whiskers” showing the extent of the scores (thanks to Nevin Yilmaz for suggesting this visualization). Note that the <code>boxplot()</code> function detects outliers (called “fliers”) and does not include them within the whiskers. Specifically, if the lower quartile is $Q_1$ and the upper quartile is $Q_3$, then the interquartile range $IQR = Q_3 - Q_1$ (this is the box’s height), and any score lower than $Q_1 - 1.5 \times IQR$ is a flier, and so is any score greater than $Q3 + 1.5 \times IQR$.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot([<span class="number">1</span>]*<span class="number">10</span>, svm_scores, <span class="string">"."</span>)</span><br><span class="line">plt.plot([<span class="number">2</span>]*<span class="number">10</span>, forest_scores, <span class="string">"."</span>)</span><br><span class="line">plt.boxplot([svm_scores, forest_scores], labels=(<span class="string">"SVM"</span>,<span class="string">"Random Forest"</span>))</span><br><span class="line">plt.ylabel(<span class="string">"Accuracy"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>To improve this result further, you could:</p>
<ul>
<li>Compare many more models and tune hyperparameters using cross validation and grid search,</li>
<li>Do more feature engineering, for example:<ul>
<li>replace <strong>SibSp</strong> and <strong>Parch</strong> with their sum,</li>
<li>try to identify parts of names that correlate well with the <strong>Survived</strong> attribute (e.g. if the name contains “Countess”, then survival seems more likely),</li>
</ul>
</li>
<li>try to convert numerical attributes to categorical attributes: for example, different age groups had very different survival rates (see below), so it may help to create an age bucket category and use it instead of the age. Similarly, it may be useful to have a special category for people traveling alone since only 30% of them survived (see below).</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"AgeBucket"</span>] = train_data[<span class="string">"Age"</span>] // <span class="number">15</span> * <span class="number">15</span></span><br><span class="line">train_data[[<span class="string">"AgeBucket"</span>, <span class="string">"Survived"</span>]].groupby([<span class="string">'AgeBucket'</span>]).mean()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"RelativesOnboard"</span>] = train_data[<span class="string">"SibSp"</span>] + train_data[<span class="string">"Parch"</span>]</span><br><span class="line">train_data[[<span class="string">"RelativesOnboard"</span>, <span class="string">"Survived"</span>]].groupby([<span class="string">'RelativesOnboard'</span>]).mean()</span><br></pre></td></tr></table></figure>

<h2 id="4-Spam-classifier"><a href="#4-Spam-classifier" class="headerlink" title="4. Spam classifier"></a>4. Spam classifier</h2><p>First, let’s fetch the data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tarfile</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> urllib</span><br><span class="line"></span><br><span class="line">DOWNLOAD_ROOT = <span class="string">"http://spamassassin.apache.org/old/publiccorpus/"</span></span><br><span class="line">HAM_URL = DOWNLOAD_ROOT + <span class="string">"20030228_easy_ham.tar.bz2"</span></span><br><span class="line">SPAM_URL = DOWNLOAD_ROOT + <span class="string">"20030228_spam.tar.bz2"</span></span><br><span class="line">SPAM_PATH = os.path.join(<span class="string">"datasets"</span>, <span class="string">"spam"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_spam_data</span><span class="params">(spam_url=SPAM_URL, spam_path=SPAM_PATH)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(spam_path):</span><br><span class="line">        os.makedirs(spam_path)</span><br><span class="line">    <span class="keyword">for</span> filename, url <span class="keyword">in</span> ((<span class="string">"ham.tar.bz2"</span>, HAM_URL), (<span class="string">"spam.tar.bz2"</span>, SPAM_URL)):</span><br><span class="line">        path = os.path.join(spam_path, filename)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(path):</span><br><span class="line">            urllib.request.urlretrieve(url, path)</span><br><span class="line">        tar_bz2_file = tarfile.open(path)</span><br><span class="line">        tar_bz2_file.extractall(path=SPAM_PATH)</span><br><span class="line">        tar_bz2_file.close()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fetch_spam_data()</span><br></pre></td></tr></table></figure>

<p>Next, let’s load all the emails:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HAM_DIR = os.path.join(SPAM_PATH, <span class="string">"easy_ham"</span>)</span><br><span class="line">SPAM_DIR = os.path.join(SPAM_PATH, <span class="string">"spam"</span>)</span><br><span class="line">ham_filenames = [name <span class="keyword">for</span> name <span class="keyword">in</span> sorted(os.listdir(HAM_DIR)) <span class="keyword">if</span> len(name) &gt; <span class="number">20</span>]</span><br><span class="line">spam_filenames = [name <span class="keyword">for</span> name <span class="keyword">in</span> sorted(os.listdir(SPAM_DIR)) <span class="keyword">if</span> len(name) &gt; <span class="number">20</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(ham_filenames)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(spam_filenames)</span><br></pre></td></tr></table></figure>

<p>We can use Python’s <code>email</code> module to parse these emails (this handles headers, encoding, and so on):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> email</span><br><span class="line"><span class="keyword">import</span> email.policy</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_email</span><span class="params">(is_spam, filename, spam_path=SPAM_PATH)</span>:</span></span><br><span class="line">    directory = <span class="string">"spam"</span> <span class="keyword">if</span> is_spam <span class="keyword">else</span> <span class="string">"easy_ham"</span></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(spam_path, directory, filename), <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> email.parser.BytesParser(policy=email.policy.default).parse(f)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ham_emails = [load_email(is_spam=<span class="literal">False</span>, filename=name) <span class="keyword">for</span> name <span class="keyword">in</span> ham_filenames]</span><br><span class="line">spam_emails = [load_email(is_spam=<span class="literal">True</span>, filename=name) <span class="keyword">for</span> name <span class="keyword">in</span> spam_filenames]</span><br></pre></td></tr></table></figure>

<p>Let’s look at one example of ham and one example of spam, to get a feel of what the data looks like:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(ham_emails[<span class="number">1</span>].get_content().strip())</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(spam_emails[<span class="number">6</span>].get_content().strip())</span><br></pre></td></tr></table></figure>

<p>Some emails are actually multipart, with images and attachments (which can have their own attachments). Let’s look at the various types of structures we have:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_email_structure</span><span class="params">(email)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(email, str):</span><br><span class="line">        <span class="keyword">return</span> email</span><br><span class="line">    payload = email.get_payload()</span><br><span class="line">    <span class="keyword">if</span> isinstance(payload, list):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"multipart(&#123;&#125;)"</span>.format(<span class="string">", "</span>.join([</span><br><span class="line">            get_email_structure(sub_email)</span><br><span class="line">            <span class="keyword">for</span> sub_email <span class="keyword">in</span> payload</span><br><span class="line">        ]))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> email.get_content_type()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">structures_counter</span><span class="params">(emails)</span>:</span></span><br><span class="line">    structures = Counter()</span><br><span class="line">    <span class="keyword">for</span> email <span class="keyword">in</span> emails:</span><br><span class="line">        structure = get_email_structure(email)</span><br><span class="line">        structures[structure] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> structures</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">structures_counter(ham_emails).most_common()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">structures_counter(spam_emails).most_common()</span><br></pre></td></tr></table></figure>

<p>It seems that the ham emails are more often plain text, while spam has quite a lot of HTML. Moreover, quite a few ham emails are signed using PGP, while no spam is. In short, it seems that the email structure is useful information to have.</p>
<p>Now let’s take a look at the email headers:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> header, value <span class="keyword">in</span> spam_emails[<span class="number">0</span>].items():</span><br><span class="line">    print(header,<span class="string">":"</span>,value)</span><br></pre></td></tr></table></figure>

<p>There’s probably a lot of useful information in there, such as the sender’s email address (<a href="mailto:12a1mailbot1@web.de" target="_blank" rel="noopener">12a1mailbot1@web.de</a> looks fishy), but we will just focus on the <code>Subject</code> header:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spam_emails[<span class="number">0</span>][<span class="string">"Subject"</span>]</span><br></pre></td></tr></table></figure>

<p>Okay, before we learn too much about the data, let’s not forget to split it into a training set and a test set:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X = np.array(ham_emails + spam_emails)</span><br><span class="line">y = np.array([<span class="number">0</span>] * len(ham_emails) + [<span class="number">1</span>] * len(spam_emails))</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<p>Okay, let’s start writing the preprocessing functions. First, we will need a function to convert HTML to plain text. Arguably the best way to do this would be to use the great <a href="https://www.crummy.com/software/BeautifulSoup/" target="_blank" rel="noopener">BeautifulSoup</a> library, but I would like to avoid adding another dependency to this project, so let’s hack a quick &amp; dirty solution using regular expressions (at the risk of <a href="https://stackoverflow.com/a/1732454/38626" target="_blank" rel="noopener">un̨ho͞ly radiańcé destro҉ying all enli̍̈́̂̈́ghtenment</a>). The following function first drops the <code>&lt;head&gt;</code> section, then converts all <code>&lt;a&gt;</code> tags to the word HYPERLINK, then it gets rid of all HTML tags, leaving only the plain text. For readability, it also replaces multiple newlines with single newlines, and finally it unescapes html entities (such as <code>&amp;gt;</code> or <code>&amp;nbsp;</code>):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> html <span class="keyword">import</span> unescape</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">html_to_plain_text</span><span class="params">(html)</span>:</span></span><br><span class="line">    text = re.sub(<span class="string">'&lt;head.*?&gt;.*?&lt;/head&gt;'</span>, <span class="string">''</span>, html, flags=re.M | re.S | re.I)</span><br><span class="line">    text = re.sub(<span class="string">'&lt;a\s.*?&gt;'</span>, <span class="string">' HYPERLINK '</span>, text, flags=re.M | re.S | re.I)</span><br><span class="line">    text = re.sub(<span class="string">'&lt;.*?&gt;'</span>, <span class="string">''</span>, text, flags=re.M | re.S)</span><br><span class="line">    text = re.sub(<span class="string">r'(\s*\n)+'</span>, <span class="string">'\n'</span>, text, flags=re.M | re.S)</span><br><span class="line">    <span class="keyword">return</span> unescape(text)</span><br></pre></td></tr></table></figure>

<p>Let’s see if it works. This is HTML spam:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">html_spam_emails = [email <span class="keyword">for</span> email <span class="keyword">in</span> X_train[y_train==<span class="number">1</span>]</span><br><span class="line">                    <span class="keyword">if</span> get_email_structure(email) == <span class="string">"text/html"</span>]</span><br><span class="line">sample_html_spam = html_spam_emails[<span class="number">7</span>]</span><br><span class="line">print(sample_html_spam.get_content().strip()[:<span class="number">1000</span>], <span class="string">"..."</span>)</span><br></pre></td></tr></table></figure>

<p>And this is the resulting plain text:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(html_to_plain_text(sample_html_spam.get_content())[:<span class="number">1000</span>], <span class="string">"..."</span>)</span><br></pre></td></tr></table></figure>

<p>Great! Now let’s write a function that takes an email as input and returns its content as plain text, whatever its format is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">email_to_text</span><span class="params">(email)</span>:</span></span><br><span class="line">    html = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> part <span class="keyword">in</span> email.walk():</span><br><span class="line">        ctype = part.get_content_type()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> ctype <span class="keyword">in</span> (<span class="string">"text/plain"</span>, <span class="string">"text/html"</span>):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            content = part.get_content()</span><br><span class="line">        <span class="keyword">except</span>: <span class="comment"># in case of encoding issues</span></span><br><span class="line">            content = str(part.get_payload())</span><br><span class="line">        <span class="keyword">if</span> ctype == <span class="string">"text/plain"</span>:</span><br><span class="line">            <span class="keyword">return</span> content</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            html = content</span><br><span class="line">    <span class="keyword">if</span> html:</span><br><span class="line">        <span class="keyword">return</span> html_to_plain_text(html)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(email_to_text(sample_html_spam)[:<span class="number">100</span>], <span class="string">"..."</span>)</span><br></pre></td></tr></table></figure>

<p>Let’s throw in some stemming! For this to work, you need to install the Natural Language Toolkit (<a href="http://www.nltk.org/" target="_blank" rel="noopener">NLTK</a>). It’s as simple as running the following command (don’t forget to activate your virtualenv first; if you don’t have one, you will likely need administrator rights, or use the <code>--user</code> option):</p>
<p><code>$ pip3 install nltk</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> nltk</span><br><span class="line"></span><br><span class="line">    stemmer = nltk.PorterStemmer()</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> (<span class="string">"Computations"</span>, <span class="string">"Computation"</span>, <span class="string">"Computing"</span>, <span class="string">"Computed"</span>, <span class="string">"Compute"</span>, <span class="string">"Compulsive"</span>):</span><br><span class="line">        print(word, <span class="string">"=&gt;"</span>, stemmer.stem(word))</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    print(<span class="string">"Error: stemming requires the NLTK module."</span>)</span><br><span class="line">    stemmer = <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<p>We will also need a way to replace URLs with the word “URL”. For this, we could use hard core <a href="https://mathiasbynens.be/demo/url-regex" target="_blank" rel="noopener">regular expressions</a> but we will just use the <a href="https://github.com/lipoja/URLExtract" target="_blank" rel="noopener">urlextract</a> library. You can install it with the following command (don’t forget to activate your virtualenv first; if you don’t have one, you will likely need administrator rights, or use the <code>--user</code> option):</p>
<p><code>$ pip3 install urlextract</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> urlextract <span class="comment"># may require an Internet connection to download root domain names</span></span><br><span class="line">    </span><br><span class="line">    url_extractor = urlextract.URLExtract()</span><br><span class="line">    print(url_extractor.find_urls(<span class="string">"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s"</span>))</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    print(<span class="string">"Error: replacing URLs requires the urlextract module."</span>)</span><br><span class="line">    url_extractor = <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<p>We are ready to put all this together into a transformer that we will use to convert emails to word counters. Note that we split sentences into words using Python’s <code>split()</code> method, which uses whitespaces for word boundaries. This works for many written languages, but not all. For example, Chinese and Japanese scripts generally don’t use spaces between words, and Vietnamese often uses spaces even between syllables. It’s okay in this exercise, because the dataset is (mostly) in English.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmailToWordCounterTransformer</span><span class="params">(BaseEstimator, TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, strip_headers=True, lower_case=True, remove_punctuation=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                 replace_urls=True, replace_numbers=True, stemming=True)</span>:</span></span><br><span class="line">        self.strip_headers = strip_headers</span><br><span class="line">        self.lower_case = lower_case</span><br><span class="line">        self.remove_punctuation = remove_punctuation</span><br><span class="line">        self.replace_urls = replace_urls</span><br><span class="line">        self.replace_numbers = replace_numbers</span><br><span class="line">        self.stemming = stemming</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        X_transformed = []</span><br><span class="line">        <span class="keyword">for</span> email <span class="keyword">in</span> X:</span><br><span class="line">            text = email_to_text(email) <span class="keyword">or</span> <span class="string">""</span></span><br><span class="line">            <span class="keyword">if</span> self.lower_case:</span><br><span class="line">                text = text.lower()</span><br><span class="line">            <span class="keyword">if</span> self.replace_urls <span class="keyword">and</span> url_extractor <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                urls = list(set(url_extractor.find_urls(text)))</span><br><span class="line">                urls.sort(key=<span class="keyword">lambda</span> url: len(url), reverse=<span class="literal">True</span>)</span><br><span class="line">                <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">                    text = text.replace(url, <span class="string">" URL "</span>)</span><br><span class="line">            <span class="keyword">if</span> self.replace_numbers:</span><br><span class="line">                text = re.sub(<span class="string">r'\d+(?:\.\d*(?:[eE]\d+))?'</span>, <span class="string">'NUMBER'</span>, text)</span><br><span class="line">            <span class="keyword">if</span> self.remove_punctuation:</span><br><span class="line">                text = re.sub(<span class="string">r'\W+'</span>, <span class="string">' '</span>, text, flags=re.M)</span><br><span class="line">            word_counts = Counter(text.split())</span><br><span class="line">            <span class="keyword">if</span> self.stemming <span class="keyword">and</span> stemmer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                stemmed_word_counts = Counter()</span><br><span class="line">                <span class="keyword">for</span> word, count <span class="keyword">in</span> word_counts.items():</span><br><span class="line">                    stemmed_word = stemmer.stem(word)</span><br><span class="line">                    stemmed_word_counts[stemmed_word] += count</span><br><span class="line">                word_counts = stemmed_word_counts</span><br><span class="line">            X_transformed.append(word_counts)</span><br><span class="line">        <span class="keyword">return</span> np.array(X_transformed)</span><br></pre></td></tr></table></figure>

<p>Let’s try this transformer on a few emails:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_few = X_train[:<span class="number">3</span>]</span><br><span class="line">X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)</span><br><span class="line">X_few_wordcounts</span><br></pre></td></tr></table></figure>

<p>This looks about right!</p>
<p>Now we have the word counts, and we need to convert them to vectors. For this, we will build another transformer whose <code>fit()</code> method will build the vocabulary (an ordered list of the most common words) and whose <code>transform()</code> method will use the vocabulary to convert word counts to vectors. The output is a sparse matrix.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> csr_matrix</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordCounterToVectorTransformer</span><span class="params">(BaseEstimator, TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocabulary_size=<span class="number">1000</span>)</span>:</span></span><br><span class="line">        self.vocabulary_size = vocabulary_size</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        total_count = Counter()</span><br><span class="line">        <span class="keyword">for</span> word_count <span class="keyword">in</span> X:</span><br><span class="line">            <span class="keyword">for</span> word, count <span class="keyword">in</span> word_count.items():</span><br><span class="line">                total_count[word] += min(count, <span class="number">10</span>)</span><br><span class="line">        most_common = total_count.most_common()[:self.vocabulary_size]</span><br><span class="line">        self.most_common_ = most_common</span><br><span class="line">        self.vocabulary_ = &#123;word: index + <span class="number">1</span> <span class="keyword">for</span> index, (word, count) <span class="keyword">in</span> enumerate(most_common)&#125;</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        rows = []</span><br><span class="line">        cols = []</span><br><span class="line">        data = []</span><br><span class="line">        <span class="keyword">for</span> row, word_count <span class="keyword">in</span> enumerate(X):</span><br><span class="line">            <span class="keyword">for</span> word, count <span class="keyword">in</span> word_count.items():</span><br><span class="line">                rows.append(row)</span><br><span class="line">                cols.append(self.vocabulary_.get(word, <span class="number">0</span>))</span><br><span class="line">                data.append(count)</span><br><span class="line">        <span class="keyword">return</span> csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=<span class="number">10</span>)</span><br><span class="line">X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)</span><br><span class="line">X_few_vectors</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_few_vectors.toarray()</span><br></pre></td></tr></table></figure>

<p>What does this matrix mean? Well, the 64 in the third row, first column, means that the third email contains 64 words that are not part of the vocabulary. The 1 next to it means that the first word in the vocabulary is present once in this email. The 2 next to it means that the second word is present twice, and so on. You can look at the vocabulary to know which words we are talking about. The first word is “of”, the second word is “and”, etc.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vocab_transformer.vocabulary_</span><br></pre></td></tr></table></figure>

<p>We are now ready to train our first spam classifier! Let’s transform the whole dataset:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line">preprocess_pipeline = Pipeline([</span><br><span class="line">    (<span class="string">"email_to_wordcount"</span>, EmailToWordCounterTransformer()),</span><br><span class="line">    (<span class="string">"wordcount_to_vector"</span>, WordCounterToVectorTransformer()),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">X_train_transformed = preprocess_pipeline.fit_transform(X_train)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line">log_clf = LogisticRegression(random_state=<span class="number">42</span>)</span><br><span class="line">score = cross_val_score(log_clf, X_train_transformed, y_train, cv=<span class="number">3</span>, verbose=<span class="number">3</span>)</span><br><span class="line">score.mean()</span><br></pre></td></tr></table></figure>

<p>Over 98.7%, not bad for a first try! :) However, remember that we are using the “easy” dataset. You can try with the harder datasets, the results won’t be so amazing. You would have to try multiple models, select the best ones and fine-tune them using cross-validation, and so on.</p>
<p>But you get the picture, so let’s stop now, and just print out the precision/recall we get on the test set:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score</span><br><span class="line"></span><br><span class="line">X_test_transformed = preprocess_pipeline.transform(X_test)</span><br><span class="line"></span><br><span class="line">log_clf = LogisticRegression(random_state=<span class="number">42</span>)</span><br><span class="line">log_clf.fit(X_train_transformed, y_train)</span><br><span class="line"></span><br><span class="line">y_pred = log_clf.predict(X_test_transformed)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Precision: &#123;:.2f&#125;%"</span>.format(<span class="number">100</span> * precision_score(y_test, y_pred)))</span><br><span class="line">print(<span class="string">"Recall: &#123;:.2f&#125;%"</span>.format(<span class="number">100</span> * recall_score(y_test, y_pred)))</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/01/numpy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/01/numpy/" itemprop="url">numpy常见命令与问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-01T03:08:15-04:00">
                2019-10-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index">
                    <span itemprop="name">python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><h6 id="np-exp"><a href="#np-exp" class="headerlink" title="np.exp()"></a>np.exp()</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">该代码等同于：$a = e^b$ 即e（自然底数）的幂次方。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###### np.random.seed()</span><br><span class="line"></span><br><span class="line">采用一个随机数种子，使得每次的随机数一致。</span><br><span class="line"></span><br><span class="line">每次生成随机数前都需要运行一次，否则生成的随机数不一定与之前一致。</span><br><span class="line"></span><br><span class="line">随机种子的参数与随机数大小无关，只是制定了一个随机数生成的起始位置，确定位置后，后续随机数也随之固定。</span><br></pre></td></tr></table></figure>

<p>import numpy as np<br>np.random.seed(20)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###### np.around</span><br><span class="line"></span><br><span class="line">np.around(a, decimals = 0, out=None ) #四舍五入</span><br><span class="line"></span><br><span class="line">默认到整数，正数位小数点后第N位，负数为小数点前第N位。</span><br><span class="line"></span><br><span class="line">Out默认类型与a相同。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## nparray 操作</span><br><span class="line"></span><br><span class="line">#### 条件筛选</span><br><span class="line"></span><br><span class="line">1. 按某些固定值筛选</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">arr = np.array([1, 1, 1, 134, 45, 3, 46, 45, 65, 3, 23424, 234, 12, 12, 3, 546, 1, 2])</span><br><span class="line">print(np.where(arr==3))</span><br><span class="line">print(arr[np.where(arr == 3)])</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">(array([ 5,  9, 14], dtype=int32),)</span><br><span class="line">[3 3 3]</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>按多个固定值筛选</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">arr = np.array([1, 1, 1, 134, 45, 3, 46, 45, 65, 3, 23424, 234, 12, 12, 3, 546, 1, 2])</span><br><span class="line">print(np.where((arr == 3) | (arr == 1)))</span><br><span class="line">print(arr[np.where((arr == 3) | (arr == 1))])</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">(array([ 0,  1,  2,  5,  9, 14, 16], dtype=int32),)</span><br><span class="line">[1 1 1 3 3 3 1]</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>按范围筛选</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">arr = np.array([1, 1, 1, 134, 45, 3, 46, 45, 65, 3, 23424, 234, 12, 12, 3, 546, 1, 2])</span><br><span class="line">print(np.where(arr &gt; 3))</span><br><span class="line">print(arr[np.where(arr &gt; 3)])</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">(array([ 3,  4,  6,  7,  8, 10, 11, 12, 13, 15], dtype=int32),)</span><br><span class="line">[  134    45    46    45    65 23424   234    12    12   546]</span><br></pre></td></tr></table></figure>

<p>若不需要index</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">arr = np.array([1, 1, 1, 134, 45, 3, 46, 45, 65, 3, 23424, 234, 12, 12, 3, 546, 1, 2])</span><br><span class="line">print(arr[arr &gt; 3])</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">[  134    45    46    45    65 23424   234    12    12   546]</span><br></pre></td></tr></table></figure>

<h2 id="不常用命令"><a href="#不常用命令" class="headerlink" title="不常用命令"></a>不常用命令</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.set_printoptions(threshold=np.inf)</span><br></pre></td></tr></table></figure>

<p>展示整个数组，不使用省略号</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Xiaoyu Lu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Terrylxy" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xiaoyu Lu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>








        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
