<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="LXY&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="LXY&#39;s Blog">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LXY&#39;s Blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>LXY's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LXY's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/04/Multioutput-Classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/04/Multioutput-Classification/" itemprop="url">Multioutput Classification</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-04T16:07:21-04:00">
                2019-10-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/Classification/" itemprop="url" rel="index">
                    <span itemprop="name">Classification</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/04/Multilabel-Classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/04/Multilabel-Classification/" itemprop="url">Multilabel Classification</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-04T16:07:05-04:00">
                2019-10-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/Classification/" itemprop="url" rel="index">
                    <span itemprop="name">Classification</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/04/ROC-Curve/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/04/ROC-Curve/" itemprop="url">ROC Curve</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-04T15:59:35-04:00">
                2019-10-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/04/Cross-Validation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/04/Cross-Validation/" itemprop="url">Cross Validation</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-04T15:57:25-04:00">
                2019-10-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/Performance-Measures/" itemprop="url" rel="index">
                    <span itemprop="name">Performance Measures</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/04/SDGclassifier/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/04/SDGclassifier/" itemprop="url">SDGclassifier</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-04T15:55:03-04:00">
                2019-10-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/Classification/" itemprop="url" rel="index">
                    <span itemprop="name">Classification</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/04/MNIST/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/04/MNIST/" itemprop="url">MNIST</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-04T01:15:18-04:00">
                2019-10-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>MNIST 数据集来自美国国家标准与技术研究所, National Institute of Standards and Technology (NIST). 训练集 (training set) 由来自 250 个不同人手写的数字构成, 其中 50% 是高中学生, 50% 来自人口普查局 (the Census Bureau) 的工作人员. 测试集(test set) 也是同样比例的手写数字数据.</p>
<p>MNIST 数据集下载地址： <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/</a> </p>
<p>包含如下四个部分：</p>
<ul>
<li>Training set images: train-images-idx3-ubyte.gz (9.9 MB, 解压后 47 MB, 包含 60,000 个样本)</li>
<li>Training set labels: train-labels-idx1-ubyte.gz (29 KB, 解压后 60 KB, 包含 60,000 个标签)</li>
<li>Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 解压后 7.8 MB, 包含 10,000 个样本)</li>
<li>Test set labels: t10k-labels-idx1-ubyte.gz (5KB, 解压后 10 KB, 包含 10,000 个标签)</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/04/Lecture 4 Classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/04/Lecture 4 Classification/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-04T00:37:41-04:00">
                2019-10-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>title: Hands on Machine Learning-Chapter 3<br>date: 2019-10-01 19:52:14<br>tags: [python, Machine Learning, Sklearn]<br>categories: Hands on Machine Learning<br>toc: true</p>
<blockquote>
<p>《Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Technique for Building Intelligent Systems》—-Chapter 3</p>
</blockquote>
<p><em>This notebook contains all the sample code and solutions to the exercises in chapter 3.</em></p>
<p>In previous lesson we predicted values, now we will be predicting classes. </p>
<h1 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h1><p>First, let’s make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># To support both python 2 and python 3</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division, print_function, unicode_literals</span><br><span class="line"></span><br><span class="line"><span class="comment"># Common imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># to make this notebook's output stable across runs</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># To plot pretty figures</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">'axes.labelsize'</span>] = <span class="number">14</span></span><br><span class="line">plt.rcParams[<span class="string">'xtick.labelsize'</span>] = <span class="number">12</span></span><br><span class="line">plt.rcParams[<span class="string">'ytick.labelsize'</span>] = <span class="number">12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Where to save the figures</span></span><br><span class="line">PROJECT_ROOT_DIR = <span class="string">'..'</span></span><br><span class="line">CHAPTER_ID = <span class="string">"classification"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_fig</span><span class="params">(fig_id, tight_layout=True)</span>:</span></span><br><span class="line">    path = os.path.join(PROJECT_ROOT_DIR, <span class="string">"images"</span>, CHAPTER_ID, fig_id + <span class="string">".png"</span>)</span><br><span class="line">    print(<span class="string">"Saving figure"</span>, fig_id)</span><br><span class="line">    <span class="keyword">if</span> tight_layout:</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">    plt.savefig(path, format=<span class="string">'png'</span>, dpi=<span class="number">300</span>)</span><br></pre></td></tr></table></figure>

<h1 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h1><p>We will use data on handwriting of 70,000 digits written by school children and Census employees trying to correctly classify the digits.</p>
<p>As the code ‘fetch_mldata’ is not working anymore, my professor set a new function ‘fetch_mnist’ to change the url and the path of MNIST data.</p>
<blockquote>
<p><code>fetch_mldata</code> is <a href="https://scikit-learn.org/0.20/modules/generated/sklearn.datasets.fetch_mldata.html" target="_blank" rel="noopener">deprecated</a> since scikit-learn v0.20, and replaced with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html" target="_blank" rel="noopener"><code>fetch_openml</code></a>;<br>from <a href="https://stackoverflow.com/questions/57061437/why-am-i-getting-the-following-connectionreseterror-for-mnist-fetch-mldata" target="_blank" rel="noopener">stack overflow</a></p>
<p>Actually, the code to fetch MNIST is :</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_openml</span><br><span class="line">&gt; X, y = fetch_openml(<span class="string">'mnist_784'</span>, version=<span class="number">1</span>, return_X_y=<span class="literal">True</span>) </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p><strong>But, in this course, professor uses a different way to do the same thing. And in this note I will follow the professor’s version as it is a class note.</strong></p>
</blockquote>
<p>Define a function to copy MNIST from Github.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run this first </span></span><br><span class="line"><span class="keyword">from</span> shutil <span class="keyword">import</span> copyfileobj</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.base <span class="keyword">import</span> get_data_home</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_mnist</span><span class="params">(data_home=None)</span>:</span></span><br><span class="line">    mnist_alternative_url = <span class="string">"https://github.com/amplab/datascience-sp14/raw/master/lab7/mldata/mnist-original.mat"</span></span><br><span class="line">    data_home = get_data_home(data_home=data_home)</span><br><span class="line">    data_home = os.path.join(data_home, <span class="string">'mldata'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(data_home):</span><br><span class="line">        os.makedirs(data_home)</span><br><span class="line">    mnist_save_path = os.path.join(data_home, <span class="string">"mnist-original.mat"</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(mnist_save_path):</span><br><span class="line">        mnist_url = urllib.request.urlopen(mnist_alternative_url)</span><br><span class="line">        <span class="keyword">with</span> open(mnist_save_path, <span class="string">"wb"</span>) <span class="keyword">as</span> matlab_file:</span><br><span class="line">            copyfileobj(mnist_url, matlab_file)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">    path = <span class="string">'../../data/mnist.pkl.gz'</span></span><br><span class="line">    f = gzip.open(path, <span class="string">'rb'</span>)</span><br><span class="line">    training_data, validation_data, test_data = Pickle.load(f)</span><br><span class="line">    f.close()</span><br><span class="line"></span><br><span class="line">    X_train, y_train = training_data[<span class="number">0</span>], training_data[<span class="number">1</span>]</span><br><span class="line">    print(X_train.shape, y_train.shape)</span><br><span class="line">    <span class="comment"># (50000L, 784L) (50000L,)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># get the first image and it's label</span></span><br><span class="line">    img1_arr, img1_label = X_train[<span class="number">0</span>], y_train[<span class="number">0</span>]</span><br><span class="line">    print(img1_arr.shape, img1_label)</span><br><span class="line">    <span class="comment"># (784L,) , 5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># reshape first image(1 D vector) to 2D dimension image</span></span><br><span class="line">    img1_2d = np.reshape(img1_arr, (<span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">    <span class="comment"># show it</span></span><br><span class="line">    plt.subplot(<span class="number">111</span>)</span><br><span class="line">    plt.imshow(img1_2d, cmap=plt.get_cmap(<span class="string">'gray'</span>))</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fetch the data</span></span><br><span class="line">fetch_mnist()</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_mldata</span><br><span class="line">mnist = fetch_mldata(<span class="string">"MNIST original"</span>)</span><br><span class="line">mnist</span><br></pre></td></tr></table></figure>

<p>Result:</p>
<pre><code>{&apos;DESCR&apos;: &apos;mldata.org dataset: mnist-original&apos;,
 &apos;COL_NAMES&apos;: [&apos;label&apos;, &apos;data&apos;],
 &apos;target&apos;: array([0., 0., 0., ..., 9., 9., 9.]),
 &apos;data&apos;: array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}</code></pre><p>Scikit-learn data have a dictionary (DESCR), “data” for features and “target” for labels</p>
<blockquote>
<p>Skleran 数据集具有类似dictionary的结构：</p>
<ol>
<li>DESCR键：描述数据集</li>
<li>data键：含有一个数组，每一个实例为1行，每个特征为1列</li>
<li>target键 ：一个标签的数组</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X, y = mnist[<span class="string">"data"</span>], mnist[<span class="string">"target"</span>]</span><br><span class="line">X.shape</span><br></pre></td></tr></table></figure>

<p>OUT:(70000, 784)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.shape</span><br></pre></td></tr></table></figure>

<p>OUT:(70000,)</p>
<p>Each images has 28 by 28 pixels, with each pixed containing information on color intensity from 0 (white) to 255 (black). Let’s plot it.</p>
<blockquote>
<p> 数据X共有7万张图片，每张图片有784个特征。因为图片是28×28像素，每个特征代表了一个像素点的强度，从0（白色）到255（黑色），X[36000]的数字如下，通过“y[36000]”查看其标签为“5”。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#plot X 36000</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># Find the image number 36000</span></span><br><span class="line">some_digit = X[<span class="number">36000</span>]</span><br><span class="line"><span class="comment"># Reshape vector into a matrix</span></span><br><span class="line">some_digit_image = some_digit.reshape(<span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="comment"># Plot the image. matplotlib.cm.binary is the black-white coloring scheme. </span></span><br><span class="line"><span class="comment"># Interpolation is the smoothing of colors</span></span><br><span class="line">plt.imshow(some_digit_image, cmap = matplotlib.cm.binary,</span><br><span class="line">           interpolation=<span class="string">"nearest"</span>)</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line"></span><br><span class="line">save_fig(<span class="string">"some_digit_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Saving figure some_digit_plot</code></pre><p><img src="output_16_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># As you might have guessed the correct number is:</span></span><br><span class="line">y[<span class="number">36000</span>]</span><br></pre></td></tr></table></figure>

<p>OUT: 5.0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a function to draw the picture above.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_digit</span><span class="params">(data)</span>:</span></span><br><span class="line">    image = data.reshape(<span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    plt.imshow(image, cmap = matplotlib.cm.binary,</span><br><span class="line">               interpolation=<span class="string">"nearest"</span>)</span><br><span class="line">    plt.axis(<span class="string">"off"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXTRA Let's plot 10 by 10 graph</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_digits</span><span class="params">(instances, images_per_row=<span class="number">10</span>, **options)</span>:</span></span><br><span class="line">    <span class="comment"># image size</span></span><br><span class="line">    size = <span class="number">28</span></span><br><span class="line">    <span class="comment"># number if images per row is 10 or less </span></span><br><span class="line">    images_per_row = min(len(instances), images_per_row)</span><br><span class="line">    <span class="comment"># reshape data into 28 by 28 matrix</span></span><br><span class="line">    images = [instance.reshape(size,size) <span class="keyword">for</span> instance <span class="keyword">in</span> instances]</span><br><span class="line">    <span class="comment"># Number of rows is a floor (min = 1)</span></span><br><span class="line">    n_rows = (len(instances) - <span class="number">1</span>) // images_per_row + <span class="number">1</span></span><br><span class="line">    row_images = []</span><br><span class="line">    n_empty = n_rows * images_per_row - len(instances)</span><br><span class="line">    images.append(np.zeros((size, size * n_empty)))</span><br><span class="line">    <span class="comment"># loop filling the rows of images</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> range(n_rows):</span><br><span class="line">        <span class="comment"># form each row out of images per wor</span></span><br><span class="line">        rimages = images[row * images_per_row : (row + <span class="number">1</span>) * images_per_row]</span><br><span class="line">        <span class="comment"># append images to form row</span></span><br><span class="line">        row_images.append(np.concatenate(rimages, axis=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># append rows to form matrix</span></span><br><span class="line">    image = np.concatenate(row_images, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment">#plot the image</span></span><br><span class="line">    plt.imshow(image, cmap = matplotlib.cm.binary, **options)</span><br><span class="line">    plt.axis(<span class="string">"off"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">9</span>,<span class="number">9</span>))</span><br><span class="line">example_images = np.r_[X[:<span class="number">12000</span>:<span class="number">600</span>], X[<span class="number">13000</span>:<span class="number">30600</span>:<span class="number">600</span>], X[<span class="number">30600</span>:<span class="number">60000</span>:<span class="number">590</span>]]</span><br><span class="line">print(len(example_images))</span><br><span class="line">plot_digits(example_images, images_per_row=<span class="number">10</span>)</span><br><span class="line">save_fig(<span class="string">"more_digits_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>100
Saving figure more_digits_plot</code></pre><p><img src="output_20_1.png" alt="png"></p>
<h2 id="Set-data"><a href="#Set-data" class="headerlink" title="Set data"></a>Set data</h2><p>Training: the first 60000 images; Testing: the last 10000 images.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We randomly assign 60000 obs to training data and the rest to the testing data</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X_train, X_test, y_train, y_test = X[:<span class="number">60000</span>], X[<span class="number">60000</span>:], y[:<span class="number">60000</span>], y[<span class="number">60000</span>:]</span><br><span class="line">shuffle_index = np.random.permutation(<span class="number">60000</span>)</span><br><span class="line">X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]</span><br></pre></td></tr></table></figure>

<h1 id="Training-a-Binary-classifier"><a href="#Training-a-Binary-classifier" class="headerlink" title="Training a Binary classifier"></a>Training a Binary classifier</h1><p>Let’s start with a simple task to classify one digit (5). The classifier is true for all 5s, False for all other digits.</p>
<blockquote>
<p>现在先简化问题，只尝试识别一个数字——比如数字5。那么这个“数字5检测器”就是一个二元分类器的例子，它只能区分两个类别：5和非5。先为此分类任务创建目标向量(将数字标签转换为bool型标签true代表 5，false代表 非5)：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_train_5 = (y_train == <span class="number">5</span>)</span><br><span class="line">y_test_5 = (y_test == <span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<p>Let’s start with a Stochastic Gradient Descent (SGD) classifier, using Scikit-Learn’s SGDClassifier class. This classifier has the advantage of being capable of handling very large datasets efficiently.<br>This is in part because SGD deals with training instances independently, one at a time<br>(which also makes SGD well suited for online learning), as we will see later. Let’s create<br>an SGDClassifier and train it on the whole training set:</p>
<blockquote>
<p>接着挑选一个分类器并开始训练。一个好的初始选择是随机梯度下降（SGD）分类器，使用Scikit-Learn的SGDClassifier类即可。这个分类器的优势是，能够有效处理非常大型的数据集。这部分是因为SGD独立处理训练实例，一次一个。此时先创建一个SGDClassifier并在整个训练集上进行训练：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train_5</span><br></pre></td></tr></table></figure>

<p>OUT: array([False, False, False, …, False, False, False])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The alhorithm relies on randomness and for reproducibility requires random_state parameter.</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"></span><br><span class="line">sgd_clf = SGDClassifier(max_iter=<span class="number">5</span>, random_state=<span class="number">42</span>) <span class="comment">#max_iter is the max number of passes the training data (aka epochs)</span></span><br><span class="line">sgd_clf.fit(X_train, y_train_5)</span><br></pre></td></tr></table></figure>

<pre><code>SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
              l1_ratio=0.15, learning_rate=&apos;optimal&apos;, loss=&apos;hinge&apos;, max_iter=5,
              n_iter_no_change=5, n_jobs=None, penalty=&apos;l2&apos;, power_t=0.5,
              random_state=42, shuffle=True, tol=0.001, validation_fraction=0.1,
              verbose=0, warm_start=False)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test for our digit.</span></span><br><span class="line">sgd_clf.predict([some_digit])</span><br></pre></td></tr></table></figure>

<p>OUT: array([ True])</p>
<h1 id="Performance-Measure"><a href="#Performance-Measure" class="headerlink" title="Performance Measure"></a>Performance Measure</h1><p>Evaluating a classifier is often significantly trickier than evaluating a regressor, so we will spend a large<br>part of this chapter on this topic.</p>
<h2 id="Measuring-Accuracy-Using-Cross-Validation"><a href="#Measuring-Accuracy-Using-Cross-Validation" class="headerlink" title="Measuring Accuracy Using Cross-Validation"></a>Measuring Accuracy Using Cross-Validation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's start with accuracy</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">cross_val_score(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure>

<p>OUT: array([0.96225, 0.9645 , 0.94765])</p>
<p>Wow! Above 95% accuracy (ratio of correct predictions) on all cross-validation folds?<br>This looks amazing, doesn’t it? Well, before you get too excited, let’s look at a very<br>dumb classifier that just classifies every single image in the “not-5” class:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's look at a simple binary classifier: correctly classify 5 vs. not-5 digits.</span></span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Never5Classifier</span><span class="params">(BaseEstimator)</span>:</span></span><br><span class="line">    <span class="comment"># We don't really fit anything</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># We return a vector of zeros effectively predicting that all digits are not-5</span></span><br><span class="line">        <span class="keyword">return</span> np.zeros((len(X), <span class="number">1</span>), dtype=bool)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">never_5_clf = Never5Classifier()</span><br><span class="line">cross_val_score(never_5_clf, X_train, y_train_5, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>array([0.909  , 0.90715, 0.9128 ])</code></pre><p>Now 95% accuracy does not look that impressive. This is simply because only about 10% of the<br>images are 5s, so if you always guess that an image is not a 5, you will be right about<br>90% of the time.<br>This demonstrates why accuracy is generally not the preferred performance measure<br>for classifiers, especially when you are dealing with skewed datasets (i.e., when some<br>classes are much more frequent than others).</p>
<p><code>Cross_val_predict</code> predicts evaluation score for each fold. The model estimated in each prediction is based on a different fold.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_predict</span><br><span class="line">y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>)</span><br><span class="line">y_train_pred</span><br></pre></td></tr></table></figure>

<pre><code>array([False, False, False, ..., False, False, False])</code></pre><h2 id="Confusion-Matrix"><a href="#Confusion-Matrix" class="headerlink" title="Confusion Matrix"></a>Confusion Matrix</h2><table>
<thead>
<tr>
<th></th>
<th>Predict No</th>
<th align="left">Peridict Yes</th>
</tr>
</thead>
<tbody><tr>
<td>Actual No</td>
<td>True Negatives</td>
<td align="left">False Positives</td>
</tr>
<tr>
<td>Actual Yes</td>
<td>False Negatives</td>
<td align="left">True Positives</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line">confusion_matrix(y_train_5, y_train_pred)</span><br></pre></td></tr></table></figure>

<pre><code>array([[53417,  1162],
       [ 1350,  4071]])</code></pre><p>\begin{array}{rr} \hline<br>  &amp; \text{Predict No} &amp; \text{Predict Yes} \ \hline<br> \text{Actual No} &amp; \text{True Negatives} &amp; \text{False Positives} \<br> \text{Actual Yes} &amp; \text{False Negatives} &amp; \text{True Positives} \  \hline<br>\end{array}</p>
<p>We have 10 times as many false positive as false negatives. This is because our signal (5) is rare relative to the noise (not-5), so we are much more likely to get false positives. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train_perfect_predictions = y_train_5</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of perfect prediction. We made a table from two identical vectors. No errors!</span></span><br><span class="line">confusion_matrix(y_train_5, y_train_perfect_predictions)</span><br></pre></td></tr></table></figure>

<pre><code>array([[54579,     0],
       [    0,  5421]])</code></pre><p>An interesting one to look at is the accuracy of the positive predictions this is called the precision of the classifier. </p>
<p>TP is the number of true positives, and FP is the number of false positives.<br>A trivial way to have perfect precision is to make one single positive prediction and<br>ensure it is correct (precision = 1/1 = 100%). This would not be very useful since the<br>classifier would ignore all but one positive instance. So precision is typically used<br>along with another metric named recall, also called sensitivity or true positive rate<br>(TPR): this is the ratio of positive instances that are correctly detected by the classifier</p>
<p>FN is of course the number of false negatives.<br>Precision is the share of correctly identified positive values.  <br><br>Precision = $\frac{TP}{TP + FP}$ <br><br>Recall is the fraction of the true positive values identified.  <br><br>Recall = $\frac{TP}{TP + FN}$  <br><br>Accuracy is the total error rate. It works if the positive and negative value are roughly balanced.  <br><br>Accuracy = $\frac{TP + TN}{P + N}$  <br></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score</span><br><span class="line">precision_score(y_train_5, y_train_pred) </span><br><span class="line"><span class="comment"># Our of all digits we classified as fives, 77% we really fives.</span></span><br></pre></td></tr></table></figure>

<pre><code>0.7779476399770686</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Remember precision is TP / (TP + FP)</span></span><br><span class="line"><span class="number">4344</span> / (<span class="number">4344</span> + <span class="number">1307</span>)</span><br></pre></td></tr></table></figure>

<pre><code>0.7687135020350381</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Out of all fives in the data we classified as fives 80%</span></span><br><span class="line">recall_score(y_train_5, y_train_pred)</span><br></pre></td></tr></table></figure>

<pre><code>0.7509684560044272</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">4344</span> / (<span class="number">4344</span> + <span class="number">1077</span>)</span><br></pre></td></tr></table></figure>

<pre><code>0.801328168234643</code></pre><p>Now your 5-detector does not look as shiny as it did when you looked at its accuracy. When it claims an image represents a 5, it is correct only 77% of the time. Moreover,<br>it only detects 79% of the 5s.</p>
<p>It is often convenient to combine precision and recall into a single metric called the F1<br>score, in particular if you need a simple way to compare two classifiers. The F1 score is<br>the harmonic mean of precision and recall (Equation 3-3). Whereas the regular mean<br>treats all values equally, the harmonic mean gives much more weight to low values.<br>As a result, the classifier will only get a high F1 score if both recall and precision are<br>high. <br> <br><br>$ F1 = \frac{2}{\frac{1}{Precision} + \frac{1}{Recall}} =<br>2 * \frac{Precision * Recall}{Precision + Recall} = \frac{TP}{TP + \frac{FN + FP}{2}}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line">f1_score(y_train_5, y_train_pred)</span><br><span class="line"><span class="comment"># F1 is a good choice for data where the share of positives is greatly different from 0.5</span></span><br></pre></td></tr></table></figure>

<pre><code>0.7642200112633752</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Check the formula</span></span><br><span class="line"><span class="number">4344</span> / (<span class="number">4344</span> + (<span class="number">1077</span> + <span class="number">1307</span>)/<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>0.7846820809248555</code></pre><p>The F1 score favors classifiers that have similar precision and recall. <br><br>This is not always what you want: in some contexts you mostly care about precision, and in other contexts you really care about recall.<br></p>
<p>For example, if you trained a classifier to detect videos that are safe for kids, you would probably prefer a classifier that rejects many good videos (low recall) but keeps only safe ones (high precision), rather than a classifier<br>that has a much higher recall but lets a few really bad videos show up in your<br>product. <br><br>On the other hand, suppose you train a classifier to detect shoplifters on surveillance images: it is probably fine if your classifier has only 30% precision as long as it has 99% recall (sure, the security guards will get a few false alerts, but almost all shoplifters will get caught). <br></p>
<p>Unfortunately, you can’t have it both ways: increasing precision reduces recall, and<br>vice versa. This is called the precision/recall tradeoff. <br></p>
<p>Imagine your algorithm produces a score how much each digit resembles digit 5. Let’s the algorithm decide that it is optimal to label all digits with a score greater than 0.8 as digit 5. If we artificially increase the benchmark to 0.9, we will increase our precision (almost all digits we classify as fives would be actually fives), but we would reduce recall (we would catch fewer fives in the data).  Lowering the benchmark to 0.5 would produce an opposite effect. </p>
<p>Decision function of the SGD algorithm produces a similarity score used in the classification. By default if the score it greater than 0 the observation is classified as positive.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_scores = sgd_clf.decision_function([some_digit])</span><br><span class="line">y_scores</span><br><span class="line"><span class="comment"># In this case the score is greteer than 0, so the digit is classified as 5.</span></span><br></pre></td></tr></table></figure>

<pre><code>array([150526.40944343])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">threshold = <span class="number">0</span></span><br><span class="line">y_some_digit_pred = (y_scores &gt; threshold)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_some_digit_pred</span><br></pre></td></tr></table></figure>

<pre><code>array([ True])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If we increase a threshhold greater than 161855..</span></span><br><span class="line">threshold = <span class="number">200000</span></span><br><span class="line">y_some_digit_pred = (y_scores &gt; threshold)</span><br><span class="line">y_some_digit_pred</span><br></pre></td></tr></table></figure>

<pre><code>array([False])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get the secision scores</span></span><br><span class="line">y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>,</span><br><span class="line">                             method=<span class="string">"decision_function"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  ConvergenceWarning)
//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  ConvergenceWarning)
//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  ConvergenceWarning)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Vector of decision score for the whole training datasets</span></span><br><span class="line">y_scores.shape</span><br></pre></td></tr></table></figure>

<pre><code>(60000,)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hack to work around issue #9589 in Scikit-Learn 0.19.0</span></span><br><span class="line"><span class="keyword">if</span> y_scores.ndim == <span class="number">2</span>:</span><br><span class="line">    y_scores = y_scores[:, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<p>Next we will calculate recall and precision scores.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve</span><br><span class="line">precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)</span><br></pre></td></tr></table></figure>

<p>The algorithm picked a threshold that maximizes both prevision and recall. Lowering the threshold increases recall and decreases precision. Increasing the threshold leads to the opposite effect.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_precision_recall_vs_threshold</span><span class="params">(precisions, recalls, thresholds)</span>:</span></span><br><span class="line">    plt.plot(thresholds, precisions[:<span class="number">-1</span>], <span class="string">"b--"</span>, label=<span class="string">"Precision"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.plot(thresholds, recalls[:<span class="number">-1</span>], <span class="string">"g-"</span>, label=<span class="string">"Recall"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Threshold"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">"upper left"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.ylim([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">plot_precision_recall_vs_threshold(precisions, recalls, thresholds)</span><br><span class="line">plt.xlim([<span class="number">-700000</span>, <span class="number">700000</span>])</span><br><span class="line">save_fig(<span class="string">"precision_recall_vs_threshold_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Saving figure precision_recall_vs_threshold_plot</code></pre><p><img src="output_63_1.png" alt="png"></p>
<p>Precision is more bumpy. Generally there is an inverse relationship between precision and recall. However, there are exceptions. Consider the following:     2 2 3 4 5 2 4 5 | 4  5 5 5. Current precision is 3/4 = 0.75, recall 3/5 = 0.6. If we move the threshold to 2 2 3 4 5 2 4 | 5 4 5 5 5 we caught one more five, it increases both precision 4/5 = 0.8 and recall 4/5 = 0.8. This behavior is more likely in the right tail of threshold distribution when we are likely to catch 5 if we move the threshold to the right.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(y_train_pred == (y_scores &gt; <span class="number">0</span>)).all()</span><br></pre></td></tr></table></figure>

<pre><code>True</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If you aim for a 90% precision, which occurs around threshold = 105000</span></span><br><span class="line">y_train_pred_90 = (y_scores &gt; <span class="number">105000</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">precision_score(y_train_5, y_train_pred_90)</span><br></pre></td></tr></table></figure>

<pre><code>0.9007936507936508</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">recall_score(y_train_5, y_train_pred_90)</span><br></pre></td></tr></table></figure>

<pre><code>0.586238701346615</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_precision_vs_recall</span><span class="params">(precisions, recalls)</span>:</span></span><br><span class="line">    plt.plot(recalls, precisions, <span class="string">"b-"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Recall"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Precision"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plot_precision_vs_recall(precisions, recalls)</span><br><span class="line">save_fig(<span class="string">"precision_vs_recall_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Saving figure precision_vs_recall_plot</code></pre><p><img src="output_69_1.png" alt="png"></p>
<h2 id="ROC-curves"><a href="#ROC-curves" class="headerlink" title="ROC curves"></a>ROC curves</h2><p>The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It plots the true positive rate (another name for recall) against the false positive rate. <br><br>$ TPR = \frac{TP}{TP + FN}$ <br><br>$FPR = \frac{FP}{FP + TN}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_roc_curve</span><span class="params">(fpr, tpr, label=None)</span>:</span></span><br><span class="line">    plt.plot(fpr, tpr, linewidth=<span class="number">2</span>, label=label)</span><br><span class="line">    plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">'k--'</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    plt.xlabel(<span class="string">'False Positive Rate'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'True Positive Rate'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plot_roc_curve(fpr, tpr)</span><br><span class="line">save_fig(<span class="string">"roc_curve_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Saving figure roc_curve_plot</code></pre><p><img src="output_73_1.png" alt="png"></p>
<p>Tradeoff: the higher the recall (TPR), the more false positives (FPR) the classifier produces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward<br>the top-left corner). <br> <br></p>
<p>One way to compare classifiers is to measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC<br>AUC. Hi AUC works for many different TPR-FPR choice. <br> <br></p>
<p>Use PR curve when the positive class is rare and you care more about false positives than about false negatives. Use ROC, AUC otherwise. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">roc_auc_score(y_train_5, y_scores)</span><br></pre></td></tr></table></figure>

<pre><code>0.9562435587387078</code></pre><p>Let’s train a RandomForestClassifier and compare its ROC curve and ROC AUC<br>score to the SGDClassifier. <br></p>
<p>We calculate score for each instance in the training data. RandomForestClassifier class does not have a decision_function(),  instead it has predict_proba(). Each instance is assigned a probability of belonging to each class.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">forest_clf = RandomForestClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=<span class="number">3</span>,</span><br><span class="line">                                    method=<span class="string">"predict_proba"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>//anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  &quot;10 in version 0.20 to 100 in 0.22.&quot;, FutureWarning)
//anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  &quot;10 in version 0.20 to 100 in 0.22.&quot;, FutureWarning)
//anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  &quot;10 in version 0.20 to 100 in 0.22.&quot;, FutureWarning)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_scores_forest = y_probas_forest[:, <span class="number">1</span>] <span class="comment"># score = probability of positive class</span></span><br><span class="line"><span class="comment"># Get fit data from the random forest</span></span><br><span class="line">fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(fpr, tpr, <span class="string">"b:"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"SGD"</span>)</span><br><span class="line">plot_roc_curve(fpr_forest, tpr_forest, <span class="string">"Random Forest"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">save_fig(<span class="string">"roc_curve_comparison_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Saving figure roc_curve_comparison_plot</code></pre><p><img src="output_79_1.png" alt="png"></p>
<p>Random forest has very good fit statistics</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">roc_auc_score(y_train_5, y_scores_forest)</span><br></pre></td></tr></table></figure>

<pre><code>0.9931243366003829</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=<span class="number">3</span>)</span><br><span class="line">precision_score(y_train_5, y_train_pred_forest)</span><br></pre></td></tr></table></figure>

<pre><code>//anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  &quot;10 in version 0.20 to 100 in 0.22.&quot;, FutureWarning)
//anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  &quot;10 in version 0.20 to 100 in 0.22.&quot;, FutureWarning)
//anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  &quot;10 in version 0.20 to 100 in 0.22.&quot;, FutureWarning)





0.9852973447443494</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">recall_score(y_train_5, y_train_pred_forest)</span><br></pre></td></tr></table></figure>

<pre><code>0.8282604685482383</code></pre><p>98.5% precision and 82.2% recall</p>
<h1 id="Multiclass-classification"><a href="#Multiclass-classification" class="headerlink" title="Multiclass classification"></a>Multiclass classification</h1><p>Whereas binary classifiers distinguish between two classes, multiclass classifiers can distinguish between more than two classes.Some algorithms (such as Random Forest classifiers or naive Bayes classifiers) are capable of handling multiple classes directly. Others (such as Support Vector Machine classifiers or Linear classifiers) are strictly binary classifiers. <br> </p>
<p>You can use binary classifiers to estimate multiclass classification: <br><br>1 Create 10 binary classifier for each digit versus the rest of the digits, similar to our five or not-five classifiers. After we estimate 10 classifiers for each digit we set a label for the classifier that produced the highest score. This is called the one-versus-all (OvA) strategy (also called one-versus-the-rest). <br></p>
<p>2 Another strategy is o train binary classifier for each paid of digits: 0s vs 1s, 0 vs 4s, etc. This is called the one-versus-one (OvO) strategy. If there are N classes, you need to train N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45 binary classifiers! For each digit we pick the label with the highest average score against 9 other digits. The main advantage of OvO is that each classifier only needs to be trained on the part of the training set for the two classes that it must distinguish. <br></p>
<p>Algorithms that scales poorly (like SVM) are preferable for OVO, which reduce the data used in the estimation. However, most often  OvA is preferred and it is a usually the default option for binary classifiers other than SVM.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's try SGD algorithm.</span></span><br><span class="line">sgd_clf.fit(X_train, y_train)</span><br><span class="line">sgd_clf.predict([some_digit])</span><br></pre></td></tr></table></figure>

<pre><code>array([5.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's look at the 10 estimated scores for the digit. 5 has the higher score  161855.74572176.</span></span><br><span class="line">some_digit_scores = sgd_clf.decision_function([some_digit])</span><br><span class="line">some_digit_scores</span><br></pre></td></tr></table></figure>

<pre><code>array([[-152619.46799791, -441052.22074349, -249930.3138537 ,
        -237258.35168498, -447251.81933158,  120565.05820991,
        -834139.15404835, -188142.48490477, -555223.79499145,
        -536978.92518594]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Which vector has the highest score</span></span><br><span class="line">np.argmax(some_digit_scores)</span><br></pre></td></tr></table></figure>

<pre><code>5</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sgd_clf.classes_</span><br></pre></td></tr></table></figure>

<pre><code>array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sgd_clf.classes_[<span class="number">5</span>]</span><br></pre></td></tr></table></figure>

<pre><code>5.0</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Here is the example of the OVO classifier.</span></span><br><span class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsOneClassifier</span><br><span class="line">ovo_clf = OneVsOneClassifier(SGDClassifier(max_iter=<span class="number">5</span>, random_state=<span class="number">42</span>))</span><br><span class="line">ovo_clf.fit(X_train, y_train)</span><br><span class="line">ovo_clf.predict([some_digit])</span><br></pre></td></tr></table></figure>

<pre><code>array([5.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># As expected we estimated 45 binary classfiers</span></span><br><span class="line">len(ovo_clf.estimators_)</span><br></pre></td></tr></table></figure>

<pre><code>45</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">forest_clf.fit(X_train, y_train)</span><br><span class="line">forest_clf.predict([some_digit])</span><br></pre></td></tr></table></figure>

<pre><code>//anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
  &quot;10 in version 0.20 to 100 in 0.22.&quot;, FutureWarning)





array([5.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">forest_clf.predict_proba([some_digit])</span><br><span class="line"><span class="comment"># This classifier returns probabilities. The probability of digit 5 is 80%, 10% that it is 0 and 10% that it is 3.</span></span><br></pre></td></tr></table></figure>

<pre><code>array([[0.1, 0. , 0. , 0.1, 0. , 0.8, 0. , 0. , 0. , 0. ]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train</span><br></pre></td></tr></table></figure>

<pre><code>array([1., 6., 6., ..., 0., 2., 9.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Perfomance of OVA SGD.</span></span><br><span class="line">cross_val_score(sgd_clf, X_train, y_train, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br><span class="line"><span class="comment"># Accuracy of three folds</span></span><br></pre></td></tr></table></figure>

<pre><code>//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  ConvergenceWarning)
//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  ConvergenceWarning)
//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  ConvergenceWarning)





array([0.84993001, 0.81769088, 0.84707706])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Usually scaling the data improves the accracy</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))</span><br><span class="line">cross_val_score(sgd_clf, X_train_scaled, y_train, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  ConvergenceWarning)
//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  ConvergenceWarning)
//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  ConvergenceWarning)





array([0.91211758, 0.9099955 , 0.90643597])</code></pre><h1 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h1><p>Real life project has multiple steps such as exploring data preparation options, trying out multiple models, shortlisting the best ones and fine-tuning their hyperparameters using GridSearchCV, and automating as much as possible.</p>
<p>Here, we will assume that you have found a promising model and you want to find ways to improve it. One way to do this is to analyze the types of errors it makes.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's look at the confusion matrix</span></span><br><span class="line">y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=<span class="number">3</span>)</span><br><span class="line">conf_mx = confusion_matrix(y_train, y_train_pred)</span><br><span class="line">conf_mx</span><br></pre></td></tr></table></figure>

<pre><code>//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  ConvergenceWarning)
//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  ConvergenceWarning)
//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
  ConvergenceWarning)





array([[5749,    4,   22,   11,   11,   40,   36,   11,   36,    3],
       [   2, 6490,   43,   24,    6,   41,    8,   12,  107,    9],
       [  53,   42, 5330,   99,   87,   24,   89,   58,  159,   17],
       [  46,   41,  126, 5361,    1,  241,   34,   59,  129,   93],
       [  20,   30,   35,   10, 5369,    8,   48,   38,   76,  208],
       [  73,   45,   30,  194,   64, 4614,  106,   30,  170,   95],
       [  41,   30,   46,    2,   44,   91, 5611,    9,   43,    1],
       [  26,   18,   73,   30,   52,   11,    4, 5823,   14,  214],
       [  63,  159,   69,  168,   15,  172,   54,   26, 4997,  128],
       [  39,   39,   27,   90,  177,   40,    2,  230,   78, 5227]])</code></pre><p>Digit 5 has the lower correct prediction 4,582, digit 1 has the highest correct prediction 6,493. Let’s look at the graphical representation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_confusion_matrix</span><span class="params">(matrix)</span>:</span></span><br><span class="line">    <span class="string">"""If you prefer color and a colorbar"""</span></span><br><span class="line">    <span class="comment"># Plot 8 by 8 matrix</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    <span class="comment"># Color intensity comes from the matrix.</span></span><br><span class="line">    cax = ax.matshow(matrix)</span><br><span class="line">    fig.colorbar(cax)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Digit five is darker, digit 1 is ligher. </span></span><br><span class="line">plt.matshow(conf_mx, cmap=plt.cm.gray)</span><br><span class="line">plt.xlabel(<span class="string">"Predicted"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Actual"</span>)</span><br><span class="line">save_fig(<span class="string">"confusion_matrix_plot"</span>, tight_layout=<span class="literal">False</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Saving figure confusion_matrix_plot</code></pre><p><img src="output_103_1.png" alt="png"></p>
<p>Off-diagonal elements are the errors. We can look at most common misclassifications. From the matrix, we can see that the most common error is the confusion of digits 7 and 9 (236, 223).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">row_sums = conf_mx.sum(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">norm_conf_mx = conf_mx / row_sums</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We will no plot a diagal elements, otherwise their intensity would dominated the chart. </span></span><br><span class="line">np.fill_diagonal(norm_conf_mx, <span class="number">0</span>)</span><br><span class="line">plt.matshow(norm_conf_mx, cmap=plt.cm.gray)</span><br><span class="line">plt.xlabel(<span class="string">"Predicted"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Actual"</span>)</span><br><span class="line">save_fig(<span class="string">"confusion_matrix_errors_plot"</span>, tight_layout=<span class="literal">False</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Saving figure confusion_matrix_errors_plot</code></pre><p><img src="output_106_1.png" alt="png"></p>
<p>Largest error: symmetric confusion of 7/9, and 3/5. Other errors are asymmetric: the classification confuses 6 for 8, but not the 8 for 6. <br> </p>
<p>Solutions: <br><br>1 gather more training data for the confusing digits <br><br>2 setup closed loops that would run the algorithm until the desired error rate is achieved <br><br>3 Preprocess the data engineering new features <br> </p>
<br> 
Analyzing individual errors can also be a good way to gain insights on what your classifier is doing and why it is failing, but it is more difficult and time-consuming. Let’s plot examples of 3s and 5s.


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cl_a, cl_b = <span class="number">3</span>, <span class="number">5</span></span><br><span class="line">X_aa = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_a)]</span><br><span class="line">X_ab = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_b)]</span><br><span class="line">X_ba = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_a)]</span><br><span class="line">X_bb = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_b)]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">plt.subplot(<span class="number">221</span>); plot_digits(X_aa[:<span class="number">25</span>], images_per_row=<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">222</span>); plot_digits(X_ab[:<span class="number">25</span>], images_per_row=<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">223</span>); plot_digits(X_ba[:<span class="number">25</span>], images_per_row=<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">224</span>); plot_digits(X_bb[:<span class="number">25</span>], images_per_row=<span class="number">5</span>)</span><br><span class="line">save_fig(<span class="string">"error_analysis_digits_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Saving figure error_analysis_digits_plot</code></pre><p><img src="output_108_1.png" alt="png"></p>
<p>Top-left are the correcly classified 3s, top-right are 3s misclassified for 5s. Bottom-left are 5s misclassified as 3s,<br>bottom-right are the correctly classified 5s.  <br></p>
<p>Humans can easily recognize most of the errors. SGDClassifier assigns a weight to each pixel intensity, as 3s and 5s are differ<br>by only few pixel it’s easy to confuse them by shifting and rotation of the small line that makes a difference between 3 and 5.<br>Another solution: center and rotate all digits in the same way.</p>
<h1 id="Multilabel-classification"><a href="#Multilabel-classification" class="headerlink" title="Multilabel classification"></a>Multilabel classification</h1><p>Mutlilabel classification marks each object relative to several classes. For example, if we are looking for three people in<br>the picture Alice, Bob, and Charlie, then the classification would return  [1, 0, 1]  if it thinks that Alice and Charlie is in the picture, but not the Bob. Usually restriction on the multiple label help us to estimate this system. For example, if Bob and Charlie look alike we just need to find out two objects that resembles [Bob, Charlie], this is often easier that trying to distinguish similar objects. </p>
<p>We create two classes of digits large (&gt;6) and odd. We use the KNN (Kth nearest neighbor) , which is a weighted average of classes for the K nearest neighbors. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">y_train_large = (y_train &gt;= <span class="number">7</span>)</span><br><span class="line">y_train_odd = (y_train % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line">y_multilabel = np.c_[y_train_large, y_train_odd]</span><br><span class="line"></span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line">knn_clf.fit(X_train, y_multilabel)</span><br></pre></td></tr></table></figure>

<pre><code>KNeighborsClassifier(algorithm=&apos;auto&apos;, leaf_size=30, metric=&apos;minkowski&apos;,
                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,
                     weights=&apos;uniform&apos;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn_clf.predict([some_digit])</span><br></pre></td></tr></table></figure>

<pre><code>array([[False,  True]])</code></pre><p>And it gets it right! The digit 5 is indeed not large (False) and odd (True).</p>
<p><strong>Warning</strong>: the following cell may take a very long time (possibly hours depending on your hardware). KNN can be a very slow algorithm. F1 can weight classes by importance. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=<span class="number">3</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">f1_score(y_multilabel, y_train_knn_pred, average=<span class="string">"macro"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>---------------------------------------------------------------------------

KeyboardInterrupt                         Traceback (most recent call last)

&lt;ipython-input-88-17275a77e9d1&gt; in &lt;module&gt;
----&gt; 1 y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3, n_jobs=-1)
      2 f1_score(y_multilabel, y_train_knn_pred, average=&quot;macro&quot;)


//anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py in cross_val_predict(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)
    778     prediction_blocks = parallel(delayed(_fit_and_predict)(
    779         clone(estimator), X, y, train, test, verbose, fit_params, method)
--&gt; 780         for train, test in cv.split(X, y, groups))
    781 
    782     # Concatenate the predictions


//anaconda3/lib/python3.7/site-packages/joblib/parallel.py in __call__(self, iterable)
    932 
    933             with self._backend.retrieval_context():
--&gt; 934                 self.retrieve()
    935             # Make sure that we get a last message telling us we are done
    936             elapsed_time = time.time() - self._start_time


//anaconda3/lib/python3.7/site-packages/joblib/parallel.py in retrieve(self)
    831             try:
    832                 if getattr(self._backend, &apos;supports_timeout&apos;, False):
--&gt; 833                     self._output.extend(job.get(timeout=self.timeout))
    834                 else:
    835                     self._output.extend(job.get())


//anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py in wrap_future_result(future, timeout)
    519         AsyncResults.get from multiprocessing.&quot;&quot;&quot;
    520         try:
--&gt; 521             return future.result(timeout=timeout)
    522         except LokyTimeoutError:
    523             raise TimeoutError()


//anaconda3/lib/python3.7/concurrent/futures/_base.py in result(self, timeout)
    425                 return self.__get_result()
    426 
--&gt; 427             self._condition.wait(timeout)
    428 
    429             if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:


//anaconda3/lib/python3.7/threading.py in wait(self, timeout)
    294         try:    # restore state no matter what (e.g., KeyboardInterrupt)
    295             if timeout is None:
--&gt; 296                 waiter.acquire()
    297                 gotit = True
    298             else:


KeyboardInterrupt: </code></pre><h1 id="Extra-material"><a href="#Extra-material" class="headerlink" title="Extra material"></a>Extra material</h1><h2 id="Dummy-ie-random-classifier"><a href="#Dummy-ie-random-classifier" class="headerlink" title="Dummy (ie. random) classifier"></a>Dummy (ie. random) classifier</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.dummy <span class="keyword">import</span> DummyClassifier</span><br><span class="line">dmy_clf = DummyClassifier()</span><br><span class="line">y_probas_dmy = cross_val_predict(dmy_clf, X_train, y_train_5, cv=<span class="number">3</span>, method=<span class="string">"predict_proba"</span>)</span><br><span class="line">y_scores_dmy = y_probas_dmy[:, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fprr, tprr, thresholdsr = roc_curve(y_train_5, y_scores_dmy)</span><br><span class="line">plot_roc_curve(fprr, tprr)</span><br></pre></td></tr></table></figure>

<h2 id="KNN-classifier"><a href="#KNN-classifier" class="headerlink" title="KNN classifier"></a>KNN classifier</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">knn_clf = KNeighborsClassifier(n_jobs=<span class="number">-1</span>, weights=<span class="string">'distance'</span>, n_neighbors=<span class="number">4</span>)</span><br><span class="line">knn_clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_knn_pred = knn_clf.predict(X_test)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">accuracy_score(y_test, y_knn_pred)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.ndimage.interpolation <span class="keyword">import</span> shift</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shift_digit</span><span class="params">(digit_array, dx, dy, new=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> shift(digit_array.reshape(<span class="number">28</span>, <span class="number">28</span>), [dy, dx], cval=new).reshape(<span class="number">784</span>)</span><br><span class="line"></span><br><span class="line">plot_digit(shift_digit(some_digit, <span class="number">5</span>, <span class="number">1</span>, new=<span class="number">100</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_train_expanded = [X_train]</span><br><span class="line">y_train_expanded = [y_train]</span><br><span class="line"><span class="keyword">for</span> dx, dy <span class="keyword">in</span> ((<span class="number">1</span>, <span class="number">0</span>), (<span class="number">-1</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">-1</span>)):</span><br><span class="line">    shifted_images = np.apply_along_axis(shift_digit, axis=<span class="number">1</span>, arr=X_train, dx=dx, dy=dy)</span><br><span class="line">    X_train_expanded.append(shifted_images)</span><br><span class="line">    y_train_expanded.append(y_train)</span><br><span class="line"></span><br><span class="line">X_train_expanded = np.concatenate(X_train_expanded)</span><br><span class="line">y_train_expanded = np.concatenate(y_train_expanded)</span><br><span class="line">X_train_expanded.shape, y_train_expanded.shape</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn_clf.fit(X_train_expanded, y_train_expanded)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_knn_expanded_pred = knn_clf.predict(X_test)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy_score(y_test, y_knn_expanded_pred)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ambiguous_digit = X_test[<span class="number">2589</span>]</span><br><span class="line">knn_clf.predict_proba([ambiguous_digit])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_digit(ambiguous_digit)</span><br></pre></td></tr></table></figure>

<h1 id="Exercise-solutions"><a href="#Exercise-solutions" class="headerlink" title="Exercise solutions"></a>Exercise solutions</h1><h2 id="1-An-MNIST-Classifier-With-Over-97-Accuracy"><a href="#1-An-MNIST-Classifier-With-Over-97-Accuracy" class="headerlink" title="1. An MNIST Classifier With Over 97% Accuracy"></a>1. An MNIST Classifier With Over 97% Accuracy</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">param_grid = [&#123;<span class="string">'weights'</span>: [<span class="string">"uniform"</span>, <span class="string">"distance"</span>], <span class="string">'n_neighbors'</span>: [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]&#125;]</span><br><span class="line"></span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line">grid_search = GridSearchCV(knn_clf, param_grid, cv=<span class="number">5</span>, verbose=<span class="number">3</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">grid_search.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid_search.best_params_</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid_search.best_score_</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">y_pred = grid_search.predict(X_test)</span><br><span class="line">accuracy_score(y_test, y_pred)</span><br></pre></td></tr></table></figure>

<h2 id="2-Data-Augmentation"><a href="#2-Data-Augmentation" class="headerlink" title="2. Data Augmentation"></a>2. Data Augmentation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.ndimage.interpolation <span class="keyword">import</span> shift</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shift_image</span><span class="params">(image, dx, dy)</span>:</span></span><br><span class="line">    image = image.reshape((<span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">    shifted_image = shift(image, [dy, dx], cval=<span class="number">0</span>, mode=<span class="string">"constant"</span>)</span><br><span class="line">    <span class="keyword">return</span> shifted_image.reshape([<span class="number">-1</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">image = X_train[<span class="number">1000</span>]</span><br><span class="line">shifted_image_down = shift_image(image, <span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">shifted_image_left = shift_image(image, <span class="number">-5</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">3</span>))</span><br><span class="line">plt.subplot(<span class="number">131</span>)</span><br><span class="line">plt.title(<span class="string">"Original"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.imshow(image.reshape(<span class="number">28</span>, <span class="number">28</span>), interpolation=<span class="string">"nearest"</span>, cmap=<span class="string">"Greys"</span>)</span><br><span class="line">plt.subplot(<span class="number">132</span>)</span><br><span class="line">plt.title(<span class="string">"Shifted down"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.imshow(shifted_image_down.reshape(<span class="number">28</span>, <span class="number">28</span>), interpolation=<span class="string">"nearest"</span>, cmap=<span class="string">"Greys"</span>)</span><br><span class="line">plt.subplot(<span class="number">133</span>)</span><br><span class="line">plt.title(<span class="string">"Shifted left"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.imshow(shifted_image_left.reshape(<span class="number">28</span>, <span class="number">28</span>), interpolation=<span class="string">"nearest"</span>, cmap=<span class="string">"Greys"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_train_augmented = [image <span class="keyword">for</span> image <span class="keyword">in</span> X_train]</span><br><span class="line">y_train_augmented = [label <span class="keyword">for</span> label <span class="keyword">in</span> y_train]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> dx, dy <span class="keyword">in</span> ((<span class="number">1</span>, <span class="number">0</span>), (<span class="number">-1</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">-1</span>)):</span><br><span class="line">    <span class="keyword">for</span> image, label <span class="keyword">in</span> zip(X_train, y_train):</span><br><span class="line">        X_train_augmented.append(shift_image(image, dx, dy))</span><br><span class="line">        y_train_augmented.append(label)</span><br><span class="line"></span><br><span class="line">X_train_augmented = np.array(X_train_augmented)</span><br><span class="line">y_train_augmented = np.array(y_train_augmented)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">shuffle_idx = np.random.permutation(len(X_train_augmented))</span><br><span class="line">X_train_augmented = X_train_augmented[shuffle_idx]</span><br><span class="line">y_train_augmented = y_train_augmented[shuffle_idx]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn_clf = KNeighborsClassifier(**grid_search.best_params_)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn_clf.fit(X_train_augmented, y_train_augmented)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred = knn_clf.predict(X_test)</span><br><span class="line">accuracy_score(y_test, y_pred)</span><br></pre></td></tr></table></figure>

<p>By simply augmenting the data, we got a 0.5% accuracy boost. :)</p>
<h2 id="3-Tackle-the-Titanic-dataset"><a href="#3-Tackle-the-Titanic-dataset" class="headerlink" title="3. Tackle the Titanic dataset"></a>3. Tackle the Titanic dataset</h2><p>The goal is to predict whether or not a passenger survived based on attributes such as their age, sex, passenger class, where they embarked and so on.</p>
<p>First, login to <a href="https://www.kaggle.com/" target="_blank" rel="noopener">Kaggle</a> and go to the <a href="https://www.kaggle.com/c/titanic" target="_blank" rel="noopener">Titanic challenge</a> to download <code>train.csv</code> and <code>test.csv</code>. Save them to the <code>datasets/titanic</code> directory.</p>
<p>Next, let’s load the data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">TITANIC_PATH = os.path.join(<span class="string">"datasets"</span>, <span class="string">"titanic"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data = load_titanic_data(<span class="string">"train.csv"</span>)</span><br><span class="line">test_data = load_titanic_data(<span class="string">"test.csv"</span>)</span><br></pre></td></tr></table></figure>

<p>The data is already split into a training set and a test set. However, the test data does <em>not</em> contain the labels: your goal is to train the best model you can using the training data, then make your predictions on the test data and upload them to Kaggle to see your final score.</p>
<p>Let’s take a peek at the top few rows of the training set:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.head()</span><br></pre></td></tr></table></figure>

<p>The attributes have the following meaning:</p>
<ul>
<li><strong>Survived</strong>: that’s the target, 0 means the passenger did not survive, while 1 means he/she survived.</li>
<li><strong>Pclass</strong>: passenger class.</li>
<li><strong>Name</strong>, <strong>Sex</strong>, <strong>Age</strong>: self-explanatory</li>
<li><strong>SibSp</strong>: how many siblings &amp; spouses of the passenger aboard the Titanic.</li>
<li><strong>Parch</strong>: how many children &amp; parents of the passenger aboard the Titanic.</li>
<li><strong>Ticket</strong>: ticket id</li>
<li><strong>Fare</strong>: price paid (in pounds)</li>
<li><strong>Cabin</strong>: passenger’s cabin number</li>
<li><strong>Embarked</strong>: where the passenger embarked the Titanic</li>
</ul>
<p>Let’s get more info to see how much data is missing:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.info()</span><br></pre></td></tr></table></figure>

<p>Okay, the <strong>Age</strong>, <strong>Cabin</strong> and <strong>Embarked</strong> attributes are sometimes null (less than 891 non-null), especially the <strong>Cabin</strong> (77% are null). We will ignore the <strong>Cabin</strong> for now and focus on the rest. The <strong>Age</strong> attribute has about 19% null values, so we will need to decide what to do with them. Replacing null values with the median age seems reasonable.</p>
<p>The <strong>Name</strong> and <strong>Ticket</strong> attributes may have some value, but they will be a bit tricky to convert into useful numbers that a model can consume. So for now, we will ignore them.</p>
<p>Let’s take a look at the numerical attributes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.describe()</span><br></pre></td></tr></table></figure>

<ul>
<li>Yikes, only 38% <strong>Survived</strong>. :(  That’s close enough to 40%, so accuracy will be a reasonable metric to evaluate our model.</li>
<li>The mean <strong>Fare</strong> was £32.20, which does not seem so expensive (but it was probably a lot of money back then).</li>
<li>The mean <strong>Age</strong> was less than 30 years old.</li>
</ul>
<p>Let’s check that the target is indeed 0 or 1:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"Survived"</span>].value_counts()</span><br></pre></td></tr></table></figure>

<p>Now let’s take a quick look at all the categorical attributes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"Pclass"</span>].value_counts()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"Sex"</span>].value_counts()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"Embarked"</span>].value_counts()</span><br></pre></td></tr></table></figure>

<p>The Embarked attribute tells us where the passenger embarked: C=Cherbourg, Q=Queenstown, S=Southampton.</p>
<p>Now let’s build our preprocessing pipelines. We will reuse the <code>DataframeSelector</code> we built in the previous chapter to select specific attributes from the <code>DataFrame</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"></span><br><span class="line"><span class="comment"># A class to select numerical or categorical columns </span></span><br><span class="line"><span class="comment"># since Scikit-Learn doesn't handle DataFrames yet</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataFrameSelector</span><span class="params">(BaseEstimator, TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, attribute_names)</span>:</span></span><br><span class="line">        self.attribute_names = attribute_names</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> X[self.attribute_names]</span><br></pre></td></tr></table></figure>

<p>Let’s build the pipeline for the numerical attributes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line"></span><br><span class="line">imputer = Imputer(strategy=<span class="string">"median"</span>)</span><br><span class="line"></span><br><span class="line">num_pipeline = Pipeline([</span><br><span class="line">        (<span class="string">"select_numeric"</span>, DataFrameSelector([<span class="string">"Age"</span>, <span class="string">"SibSp"</span>, <span class="string">"Parch"</span>, <span class="string">"Fare"</span>])),</span><br><span class="line">        (<span class="string">"imputer"</span>, Imputer(strategy=<span class="string">"median"</span>)),</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_pipeline.fit_transform(train_data)</span><br></pre></td></tr></table></figure>

<p>We will also need an imputer for the string categorical columns (the regular <code>Imputer</code> does not work on those):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inspired from stackoverflow.com/questions/25239958</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MostFrequentImputer</span><span class="params">(BaseEstimator, TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        self.most_frequent_ = pd.Series([X[c].value_counts().index[<span class="number">0</span>] <span class="keyword">for</span> c <span class="keyword">in</span> X],</span><br><span class="line">                                        index=X.columns)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> X.fillna(self.most_frequent_)</span><br></pre></td></tr></table></figure>

<p>We can convert each categorical value to a one-hot vector using a <code>OneHotEncoder</code>. Right now this class can only handle integer categorical inputs, but in Scikit-Learn 0.20 it will also handle string categorical inputs (see <a href="https://github.com/scikit-learn/scikit-learn/issues/10521" target="_blank" rel="noopener">PR #10521</a>). So for now we import it from <code>future_encoders.py</code>, but when Scikit-Learn 0.20 is released, you can import it from <code>sklearn.preprocessing</code> instead:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> future_encoders <span class="keyword">import</span> OneHotEncoder</span><br></pre></td></tr></table></figure>

<p>Now we can build the pipeline for the categorical attributes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat_pipeline = Pipeline([</span><br><span class="line">        (<span class="string">"select_cat"</span>, DataFrameSelector([<span class="string">"Pclass"</span>, <span class="string">"Sex"</span>, <span class="string">"Embarked"</span>])),</span><br><span class="line">        (<span class="string">"imputer"</span>, MostFrequentImputer()),</span><br><span class="line">        (<span class="string">"cat_encoder"</span>, OneHotEncoder(sparse=<span class="literal">False</span>)),</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat_pipeline.fit_transform(train_data)</span><br></pre></td></tr></table></figure>

<p>Finally, let’s join the numerical and categorical pipelines:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> FeatureUnion</span><br><span class="line">preprocess_pipeline = FeatureUnion(transformer_list=[</span><br><span class="line">        (<span class="string">"num_pipeline"</span>, num_pipeline),</span><br><span class="line">        (<span class="string">"cat_pipeline"</span>, cat_pipeline),</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>

<p>Cool! Now we have a nice preprocessing pipeline that takes the raw data and outputs numerical input features that we can feed to any Machine Learning model we want.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train = preprocess_pipeline.fit_transform(train_data)</span><br><span class="line">X_train</span><br></pre></td></tr></table></figure>

<p>Let’s not forget to get the labels:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train = train_data[<span class="string">"Survived"</span>]</span><br></pre></td></tr></table></figure>

<p>We are now ready to train a classifier. Let’s start with an <code>SVC</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">svm_clf = SVC()</span><br><span class="line">svm_clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<p>Great, our model is trained, let’s use it to make predictions on the test set:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_test = preprocess_pipeline.transform(test_data)</span><br><span class="line">y_pred = svm_clf.predict(X_test)</span><br></pre></td></tr></table></figure>

<p>And now we could just build a CSV file with these predictions (respecting the format excepted by Kaggle), then upload it and hope for the best. But wait! We can do better than hope. Why don’t we use cross-validation to have an idea of how good our model is?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line">svm_scores = cross_val_score(svm_clf, X_train, y_train, cv=<span class="number">10</span>)</span><br><span class="line">svm_scores.mean()</span><br></pre></td></tr></table></figure>

<p>Okay, over 73% accuracy, clearly better than random chance, but it’s not a great score. Looking at the <a href="https://www.kaggle.com/c/titanic/leaderboard" target="_blank" rel="noopener">leaderboard</a> for the Titanic competition on Kaggle, you can see that you need to reach above 80% accuracy to be within the top 10% Kagglers. Some reached 100%, but since you can easily find the <a href="https://www.encyclopedia-titanica.org/titanic-victims/" target="_blank" rel="noopener">list of victims</a> of the Titanic, it seems likely that there was little Machine Learning involved in their performance! ;-) So let’s try to build a model that reaches 80% accuracy.</p>
<p>Let’s try a <code>RandomForestClassifier</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line">forest_clf = RandomForestClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=<span class="number">10</span>)</span><br><span class="line">forest_scores.mean()</span><br></pre></td></tr></table></figure>

<p>That’s much better!</p>
<p>Instead of just looking at the mean accuracy across the 10 cross-validation folds, let’s plot all 10 scores for each model, along with a box plot highlighting the lower and upper quartiles, and “whiskers” showing the extent of the scores (thanks to Nevin Yilmaz for suggesting this visualization). Note that the <code>boxplot()</code> function detects outliers (called “fliers”) and does not include them within the whiskers. Specifically, if the lower quartile is $Q_1$ and the upper quartile is $Q_3$, then the interquartile range $IQR = Q_3 - Q_1$ (this is the box’s height), and any score lower than $Q_1 - 1.5 \times IQR$ is a flier, and so is any score greater than $Q3 + 1.5 \times IQR$.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot([<span class="number">1</span>]*<span class="number">10</span>, svm_scores, <span class="string">"."</span>)</span><br><span class="line">plt.plot([<span class="number">2</span>]*<span class="number">10</span>, forest_scores, <span class="string">"."</span>)</span><br><span class="line">plt.boxplot([svm_scores, forest_scores], labels=(<span class="string">"SVM"</span>,<span class="string">"Random Forest"</span>))</span><br><span class="line">plt.ylabel(<span class="string">"Accuracy"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>To improve this result further, you could:</p>
<ul>
<li>Compare many more models and tune hyperparameters using cross validation and grid search,</li>
<li>Do more feature engineering, for example:<ul>
<li>replace <strong>SibSp</strong> and <strong>Parch</strong> with their sum,</li>
<li>try to identify parts of names that correlate well with the <strong>Survived</strong> attribute (e.g. if the name contains “Countess”, then survival seems more likely),</li>
</ul>
</li>
<li>try to convert numerical attributes to categorical attributes: for example, different age groups had very different survival rates (see below), so it may help to create an age bucket category and use it instead of the age. Similarly, it may be useful to have a special category for people traveling alone since only 30% of them survived (see below).</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"AgeBucket"</span>] = train_data[<span class="string">"Age"</span>] // <span class="number">15</span> * <span class="number">15</span></span><br><span class="line">train_data[[<span class="string">"AgeBucket"</span>, <span class="string">"Survived"</span>]].groupby([<span class="string">'AgeBucket'</span>]).mean()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"RelativesOnboard"</span>] = train_data[<span class="string">"SibSp"</span>] + train_data[<span class="string">"Parch"</span>]</span><br><span class="line">train_data[[<span class="string">"RelativesOnboard"</span>, <span class="string">"Survived"</span>]].groupby([<span class="string">'RelativesOnboard'</span>]).mean()</span><br></pre></td></tr></table></figure>

<h2 id="4-Spam-classifier"><a href="#4-Spam-classifier" class="headerlink" title="4. Spam classifier"></a>4. Spam classifier</h2><p>First, let’s fetch the data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tarfile</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> urllib</span><br><span class="line"></span><br><span class="line">DOWNLOAD_ROOT = <span class="string">"http://spamassassin.apache.org/old/publiccorpus/"</span></span><br><span class="line">HAM_URL = DOWNLOAD_ROOT + <span class="string">"20030228_easy_ham.tar.bz2"</span></span><br><span class="line">SPAM_URL = DOWNLOAD_ROOT + <span class="string">"20030228_spam.tar.bz2"</span></span><br><span class="line">SPAM_PATH = os.path.join(<span class="string">"datasets"</span>, <span class="string">"spam"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_spam_data</span><span class="params">(spam_url=SPAM_URL, spam_path=SPAM_PATH)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(spam_path):</span><br><span class="line">        os.makedirs(spam_path)</span><br><span class="line">    <span class="keyword">for</span> filename, url <span class="keyword">in</span> ((<span class="string">"ham.tar.bz2"</span>, HAM_URL), (<span class="string">"spam.tar.bz2"</span>, SPAM_URL)):</span><br><span class="line">        path = os.path.join(spam_path, filename)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(path):</span><br><span class="line">            urllib.request.urlretrieve(url, path)</span><br><span class="line">        tar_bz2_file = tarfile.open(path)</span><br><span class="line">        tar_bz2_file.extractall(path=SPAM_PATH)</span><br><span class="line">        tar_bz2_file.close()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fetch_spam_data()</span><br></pre></td></tr></table></figure>

<p>Next, let’s load all the emails:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HAM_DIR = os.path.join(SPAM_PATH, <span class="string">"easy_ham"</span>)</span><br><span class="line">SPAM_DIR = os.path.join(SPAM_PATH, <span class="string">"spam"</span>)</span><br><span class="line">ham_filenames = [name <span class="keyword">for</span> name <span class="keyword">in</span> sorted(os.listdir(HAM_DIR)) <span class="keyword">if</span> len(name) &gt; <span class="number">20</span>]</span><br><span class="line">spam_filenames = [name <span class="keyword">for</span> name <span class="keyword">in</span> sorted(os.listdir(SPAM_DIR)) <span class="keyword">if</span> len(name) &gt; <span class="number">20</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(ham_filenames)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(spam_filenames)</span><br></pre></td></tr></table></figure>

<p>We can use Python’s <code>email</code> module to parse these emails (this handles headers, encoding, and so on):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> email</span><br><span class="line"><span class="keyword">import</span> email.policy</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_email</span><span class="params">(is_spam, filename, spam_path=SPAM_PATH)</span>:</span></span><br><span class="line">    directory = <span class="string">"spam"</span> <span class="keyword">if</span> is_spam <span class="keyword">else</span> <span class="string">"easy_ham"</span></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(spam_path, directory, filename), <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> email.parser.BytesParser(policy=email.policy.default).parse(f)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ham_emails = [load_email(is_spam=<span class="literal">False</span>, filename=name) <span class="keyword">for</span> name <span class="keyword">in</span> ham_filenames]</span><br><span class="line">spam_emails = [load_email(is_spam=<span class="literal">True</span>, filename=name) <span class="keyword">for</span> name <span class="keyword">in</span> spam_filenames]</span><br></pre></td></tr></table></figure>

<p>Let’s look at one example of ham and one example of spam, to get a feel of what the data looks like:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(ham_emails[<span class="number">1</span>].get_content().strip())</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(spam_emails[<span class="number">6</span>].get_content().strip())</span><br></pre></td></tr></table></figure>

<p>Some emails are actually multipart, with images and attachments (which can have their own attachments). Let’s look at the various types of structures we have:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_email_structure</span><span class="params">(email)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(email, str):</span><br><span class="line">        <span class="keyword">return</span> email</span><br><span class="line">    payload = email.get_payload()</span><br><span class="line">    <span class="keyword">if</span> isinstance(payload, list):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"multipart(&#123;&#125;)"</span>.format(<span class="string">", "</span>.join([</span><br><span class="line">            get_email_structure(sub_email)</span><br><span class="line">            <span class="keyword">for</span> sub_email <span class="keyword">in</span> payload</span><br><span class="line">        ]))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> email.get_content_type()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">structures_counter</span><span class="params">(emails)</span>:</span></span><br><span class="line">    structures = Counter()</span><br><span class="line">    <span class="keyword">for</span> email <span class="keyword">in</span> emails:</span><br><span class="line">        structure = get_email_structure(email)</span><br><span class="line">        structures[structure] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> structures</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">structures_counter(ham_emails).most_common()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">structures_counter(spam_emails).most_common()</span><br></pre></td></tr></table></figure>

<p>It seems that the ham emails are more often plain text, while spam has quite a lot of HTML. Moreover, quite a few ham emails are signed using PGP, while no spam is. In short, it seems that the email structure is useful information to have.</p>
<p>Now let’s take a look at the email headers:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> header, value <span class="keyword">in</span> spam_emails[<span class="number">0</span>].items():</span><br><span class="line">    print(header,<span class="string">":"</span>,value)</span><br></pre></td></tr></table></figure>

<p>There’s probably a lot of useful information in there, such as the sender’s email address (<a href="mailto:12a1mailbot1@web.de" target="_blank" rel="noopener">12a1mailbot1@web.de</a> looks fishy), but we will just focus on the <code>Subject</code> header:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spam_emails[<span class="number">0</span>][<span class="string">"Subject"</span>]</span><br></pre></td></tr></table></figure>

<p>Okay, before we learn too much about the data, let’s not forget to split it into a training set and a test set:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X = np.array(ham_emails + spam_emails)</span><br><span class="line">y = np.array([<span class="number">0</span>] * len(ham_emails) + [<span class="number">1</span>] * len(spam_emails))</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<p>Okay, let’s start writing the preprocessing functions. First, we will need a function to convert HTML to plain text. Arguably the best way to do this would be to use the great <a href="https://www.crummy.com/software/BeautifulSoup/" target="_blank" rel="noopener">BeautifulSoup</a> library, but I would like to avoid adding another dependency to this project, so let’s hack a quick &amp; dirty solution using regular expressions (at the risk of <a href="https://stackoverflow.com/a/1732454/38626" target="_blank" rel="noopener">un̨ho͞ly radiańcé destro҉ying all enli̍̈́̂̈́ghtenment</a>). The following function first drops the <code>&lt;head&gt;</code> section, then converts all <code>&lt;a&gt;</code> tags to the word HYPERLINK, then it gets rid of all HTML tags, leaving only the plain text. For readability, it also replaces multiple newlines with single newlines, and finally it unescapes html entities (such as <code>&amp;gt;</code> or <code>&amp;nbsp;</code>):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> html <span class="keyword">import</span> unescape</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">html_to_plain_text</span><span class="params">(html)</span>:</span></span><br><span class="line">    text = re.sub(<span class="string">'&lt;head.*?&gt;.*?&lt;/head&gt;'</span>, <span class="string">''</span>, html, flags=re.M | re.S | re.I)</span><br><span class="line">    text = re.sub(<span class="string">'&lt;a\s.*?&gt;'</span>, <span class="string">' HYPERLINK '</span>, text, flags=re.M | re.S | re.I)</span><br><span class="line">    text = re.sub(<span class="string">'&lt;.*?&gt;'</span>, <span class="string">''</span>, text, flags=re.M | re.S)</span><br><span class="line">    text = re.sub(<span class="string">r'(\s*\n)+'</span>, <span class="string">'\n'</span>, text, flags=re.M | re.S)</span><br><span class="line">    <span class="keyword">return</span> unescape(text)</span><br></pre></td></tr></table></figure>

<p>Let’s see if it works. This is HTML spam:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">html_spam_emails = [email <span class="keyword">for</span> email <span class="keyword">in</span> X_train[y_train==<span class="number">1</span>]</span><br><span class="line">                    <span class="keyword">if</span> get_email_structure(email) == <span class="string">"text/html"</span>]</span><br><span class="line">sample_html_spam = html_spam_emails[<span class="number">7</span>]</span><br><span class="line">print(sample_html_spam.get_content().strip()[:<span class="number">1000</span>], <span class="string">"..."</span>)</span><br></pre></td></tr></table></figure>

<p>And this is the resulting plain text:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(html_to_plain_text(sample_html_spam.get_content())[:<span class="number">1000</span>], <span class="string">"..."</span>)</span><br></pre></td></tr></table></figure>

<p>Great! Now let’s write a function that takes an email as input and returns its content as plain text, whatever its format is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">email_to_text</span><span class="params">(email)</span>:</span></span><br><span class="line">    html = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> part <span class="keyword">in</span> email.walk():</span><br><span class="line">        ctype = part.get_content_type()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> ctype <span class="keyword">in</span> (<span class="string">"text/plain"</span>, <span class="string">"text/html"</span>):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            content = part.get_content()</span><br><span class="line">        <span class="keyword">except</span>: <span class="comment"># in case of encoding issues</span></span><br><span class="line">            content = str(part.get_payload())</span><br><span class="line">        <span class="keyword">if</span> ctype == <span class="string">"text/plain"</span>:</span><br><span class="line">            <span class="keyword">return</span> content</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            html = content</span><br><span class="line">    <span class="keyword">if</span> html:</span><br><span class="line">        <span class="keyword">return</span> html_to_plain_text(html)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(email_to_text(sample_html_spam)[:<span class="number">100</span>], <span class="string">"..."</span>)</span><br></pre></td></tr></table></figure>

<p>Let’s throw in some stemming! For this to work, you need to install the Natural Language Toolkit (<a href="http://www.nltk.org/" target="_blank" rel="noopener">NLTK</a>). It’s as simple as running the following command (don’t forget to activate your virtualenv first; if you don’t have one, you will likely need administrator rights, or use the <code>--user</code> option):</p>
<p><code>$ pip3 install nltk</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> nltk</span><br><span class="line"></span><br><span class="line">    stemmer = nltk.PorterStemmer()</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> (<span class="string">"Computations"</span>, <span class="string">"Computation"</span>, <span class="string">"Computing"</span>, <span class="string">"Computed"</span>, <span class="string">"Compute"</span>, <span class="string">"Compulsive"</span>):</span><br><span class="line">        print(word, <span class="string">"=&gt;"</span>, stemmer.stem(word))</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    print(<span class="string">"Error: stemming requires the NLTK module."</span>)</span><br><span class="line">    stemmer = <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<p>We will also need a way to replace URLs with the word “URL”. For this, we could use hard core <a href="https://mathiasbynens.be/demo/url-regex" target="_blank" rel="noopener">regular expressions</a> but we will just use the <a href="https://github.com/lipoja/URLExtract" target="_blank" rel="noopener">urlextract</a> library. You can install it with the following command (don’t forget to activate your virtualenv first; if you don’t have one, you will likely need administrator rights, or use the <code>--user</code> option):</p>
<p><code>$ pip3 install urlextract</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> urlextract <span class="comment"># may require an Internet connection to download root domain names</span></span><br><span class="line">    </span><br><span class="line">    url_extractor = urlextract.URLExtract()</span><br><span class="line">    print(url_extractor.find_urls(<span class="string">"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s"</span>))</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    print(<span class="string">"Error: replacing URLs requires the urlextract module."</span>)</span><br><span class="line">    url_extractor = <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<p>We are ready to put all this together into a transformer that we will use to convert emails to word counters. Note that we split sentences into words using Python’s <code>split()</code> method, which uses whitespaces for word boundaries. This works for many written languages, but not all. For example, Chinese and Japanese scripts generally don’t use spaces between words, and Vietnamese often uses spaces even between syllables. It’s okay in this exercise, because the dataset is (mostly) in English.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmailToWordCounterTransformer</span><span class="params">(BaseEstimator, TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, strip_headers=True, lower_case=True, remove_punctuation=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                 replace_urls=True, replace_numbers=True, stemming=True)</span>:</span></span><br><span class="line">        self.strip_headers = strip_headers</span><br><span class="line">        self.lower_case = lower_case</span><br><span class="line">        self.remove_punctuation = remove_punctuation</span><br><span class="line">        self.replace_urls = replace_urls</span><br><span class="line">        self.replace_numbers = replace_numbers</span><br><span class="line">        self.stemming = stemming</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        X_transformed = []</span><br><span class="line">        <span class="keyword">for</span> email <span class="keyword">in</span> X:</span><br><span class="line">            text = email_to_text(email) <span class="keyword">or</span> <span class="string">""</span></span><br><span class="line">            <span class="keyword">if</span> self.lower_case:</span><br><span class="line">                text = text.lower()</span><br><span class="line">            <span class="keyword">if</span> self.replace_urls <span class="keyword">and</span> url_extractor <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                urls = list(set(url_extractor.find_urls(text)))</span><br><span class="line">                urls.sort(key=<span class="keyword">lambda</span> url: len(url), reverse=<span class="literal">True</span>)</span><br><span class="line">                <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">                    text = text.replace(url, <span class="string">" URL "</span>)</span><br><span class="line">            <span class="keyword">if</span> self.replace_numbers:</span><br><span class="line">                text = re.sub(<span class="string">r'\d+(?:\.\d*(?:[eE]\d+))?'</span>, <span class="string">'NUMBER'</span>, text)</span><br><span class="line">            <span class="keyword">if</span> self.remove_punctuation:</span><br><span class="line">                text = re.sub(<span class="string">r'\W+'</span>, <span class="string">' '</span>, text, flags=re.M)</span><br><span class="line">            word_counts = Counter(text.split())</span><br><span class="line">            <span class="keyword">if</span> self.stemming <span class="keyword">and</span> stemmer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                stemmed_word_counts = Counter()</span><br><span class="line">                <span class="keyword">for</span> word, count <span class="keyword">in</span> word_counts.items():</span><br><span class="line">                    stemmed_word = stemmer.stem(word)</span><br><span class="line">                    stemmed_word_counts[stemmed_word] += count</span><br><span class="line">                word_counts = stemmed_word_counts</span><br><span class="line">            X_transformed.append(word_counts)</span><br><span class="line">        <span class="keyword">return</span> np.array(X_transformed)</span><br></pre></td></tr></table></figure>

<p>Let’s try this transformer on a few emails:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_few = X_train[:<span class="number">3</span>]</span><br><span class="line">X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)</span><br><span class="line">X_few_wordcounts</span><br></pre></td></tr></table></figure>

<p>This looks about right!</p>
<p>Now we have the word counts, and we need to convert them to vectors. For this, we will build another transformer whose <code>fit()</code> method will build the vocabulary (an ordered list of the most common words) and whose <code>transform()</code> method will use the vocabulary to convert word counts to vectors. The output is a sparse matrix.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> csr_matrix</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordCounterToVectorTransformer</span><span class="params">(BaseEstimator, TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocabulary_size=<span class="number">1000</span>)</span>:</span></span><br><span class="line">        self.vocabulary_size = vocabulary_size</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        total_count = Counter()</span><br><span class="line">        <span class="keyword">for</span> word_count <span class="keyword">in</span> X:</span><br><span class="line">            <span class="keyword">for</span> word, count <span class="keyword">in</span> word_count.items():</span><br><span class="line">                total_count[word] += min(count, <span class="number">10</span>)</span><br><span class="line">        most_common = total_count.most_common()[:self.vocabulary_size]</span><br><span class="line">        self.most_common_ = most_common</span><br><span class="line">        self.vocabulary_ = &#123;word: index + <span class="number">1</span> <span class="keyword">for</span> index, (word, count) <span class="keyword">in</span> enumerate(most_common)&#125;</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        rows = []</span><br><span class="line">        cols = []</span><br><span class="line">        data = []</span><br><span class="line">        <span class="keyword">for</span> row, word_count <span class="keyword">in</span> enumerate(X):</span><br><span class="line">            <span class="keyword">for</span> word, count <span class="keyword">in</span> word_count.items():</span><br><span class="line">                rows.append(row)</span><br><span class="line">                cols.append(self.vocabulary_.get(word, <span class="number">0</span>))</span><br><span class="line">                data.append(count)</span><br><span class="line">        <span class="keyword">return</span> csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=<span class="number">10</span>)</span><br><span class="line">X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)</span><br><span class="line">X_few_vectors</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_few_vectors.toarray()</span><br></pre></td></tr></table></figure>

<p>What does this matrix mean? Well, the 64 in the third row, first column, means that the third email contains 64 words that are not part of the vocabulary. The 1 next to it means that the first word in the vocabulary is present once in this email. The 2 next to it means that the second word is present twice, and so on. You can look at the vocabulary to know which words we are talking about. The first word is “of”, the second word is “and”, etc.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vocab_transformer.vocabulary_</span><br></pre></td></tr></table></figure>

<p>We are now ready to train our first spam classifier! Let’s transform the whole dataset:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line">preprocess_pipeline = Pipeline([</span><br><span class="line">    (<span class="string">"email_to_wordcount"</span>, EmailToWordCounterTransformer()),</span><br><span class="line">    (<span class="string">"wordcount_to_vector"</span>, WordCounterToVectorTransformer()),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">X_train_transformed = preprocess_pipeline.fit_transform(X_train)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line">log_clf = LogisticRegression(random_state=<span class="number">42</span>)</span><br><span class="line">score = cross_val_score(log_clf, X_train_transformed, y_train, cv=<span class="number">3</span>, verbose=<span class="number">3</span>)</span><br><span class="line">score.mean()</span><br></pre></td></tr></table></figure>

<p>Over 98.7%, not bad for a first try! :) However, remember that we are using the “easy” dataset. You can try with the harder datasets, the results won’t be so amazing. You would have to try multiple models, select the best ones and fine-tune them using cross-validation, and so on.</p>
<p>But you get the picture, so let’s stop now, and just print out the precision/recall we get on the test set:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score</span><br><span class="line"></span><br><span class="line">X_test_transformed = preprocess_pipeline.transform(X_test)</span><br><span class="line"></span><br><span class="line">log_clf = LogisticRegression(random_state=<span class="number">42</span>)</span><br><span class="line">log_clf.fit(X_train_transformed, y_train)</span><br><span class="line"></span><br><span class="line">y_pred = log_clf.predict(X_test_transformed)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Precision: &#123;:.2f&#125;%"</span>.format(<span class="number">100</span> * precision_score(y_test, y_pred)))</span><br><span class="line">print(<span class="string">"Recall: &#123;:.2f&#125;%"</span>.format(<span class="number">100</span> * recall_score(y_test, y_pred)))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/03/绝对路径与相对路径/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/03/绝对路径与相对路径/" itemprop="url">绝对路径与相对路径</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-03T23:55:54-04:00">
                2019-10-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/python/" itemprop="url" rel="index">
                    <span itemprop="name">python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>windows平台</p>
<p>python中路径的表达方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">open(<span class="string">'xxx.txt'</span>)</span><br><span class="line">open(<span class="string">'/data/xxx.txt'</span>)</span><br><span class="line">open(<span class="string">'D:\\data\\xxx.txt'</span>)</span><br></pre></td></tr></table></figure>

<p>前两者为相对路径，第三个为绝对路径。绝对路径指的是完整的路径，相对路径指相对于当前文件夹的路径。</p>
<p>第一个命令为打开当前文件夹内文件；</p>
<p>第二个命令为打开当前文件夹内，data文件夹里面的txt；</p>
<p>第三个命令为打开该路径文件。</p>
<blockquote>
<p>特别注意：‘/’用来表示绝对路径，’\‘用来表示相对路径，而’\\‘则是转义。网址和linux系统常用’/‘表示。</p>
</blockquote>
<p>获取当前文件夹的绝对路径：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">path = os.path.abspath(<span class="string">'.'</span>) <span class="comment">#当前文件夹绝对路径</span></span><br><span class="line">path = os.path.abspath(<span class="string">'..'</span>)<span class="comment">#当前文件夹的上级文件夹的绝对路径</span></span><br></pre></td></tr></table></figure>

<p>特别注意：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'..'</span> <span class="comment">#表示上级文件夹</span></span><br><span class="line"><span class="string">'.'</span>  <span class="comment">#表示当前文件夹</span></span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/03/Hexo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/03/Hexo/" itemprop="url">Hexo建立博客</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-03T19:15:35-04:00">
                2019-10-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Blog/" itemprop="url" rel="index">
                    <span itemprop="name">Blog</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>如何通过Hexo + Github 快速搭建个人Blog：</p>
<p>Hexo 可以生成静态站点，可以把Markdown文件展示成静态页面。</p>
<p>步骤：</p>
<ul>
<li>搭建本地环境</li>
<li>建立Github Pages</li>
<li>绑定域名</li>
<li>经营、优化博客</li>
</ul>
<h1 id="本地环境"><a href="#本地环境" class="headerlink" title="本地环境"></a>本地环境</h1><ol>
<li>安装 Node.js</li>
</ol>
<p>Hexo是使用 Node.js开发的，所以必须安装。</p>
<ol start="2">
<li>安装 Git</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/03/Next-theme/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/03/Next-theme/" itemprop="url">Next theme</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-03T18:28:58-04:00">
                2019-10-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Blog/" itemprop="url" rel="index">
                    <span itemprop="name">Blog</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何美化NEXT主题"><a href="#如何美化NEXT主题" class="headerlink" title="如何美化NEXT主题"></a>如何美化NEXT主题</h1><h2 id="1-添加动态背景"><a href="#1-添加动态背景" class="headerlink" title="1. 添加动态背景"></a>1. 添加动态背景</h2><p>NEXT主题5.1.1版本以上内置该功能(太受欢迎)。</p>
<p>配置文件 <code>themes/next/_config.yml</code>中找到：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Canvas-nest</span><br><span class="line">canvas_nest: false</span><br></pre></td></tr></table></figure>

<p>改为：</p>
<p><code>canvas_nest: true</code></p>
<p>想要更改该背景的参数：</p>
<p>文件所在位置<code>themes\next\source\lib\canvas-nest\canvas-nest.min.js</code></p>
<p>参数说明：</p>
<ul>
<li><code>color</code>: 线条色彩，默认<code>&quot;0,0,0&quot;</code> 三个值分别对应（R,G,B)</li>
<li><code>opacity</code>: 线条透明度，默认 <code>0.5</code>，取值范围 <code>0~1</code></li>
<li><code>count</code>: 线条总数，默认<code>99</code></li>
<li><code>zIndex</code> 该背景的z-index属性，CSS属性用于控制所在层的位置，默认<code>-1</code></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Xiaoyu Lu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Terrylxy" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xiaoyu Lu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>








        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
