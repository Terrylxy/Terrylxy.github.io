<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Machine Learning,python,Sklearn,">










<meta name="description" content="《Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Technique for Building Intelligent Systems》—-Chapter 3  This notebook contains all the sample code and solutions to t">
<meta name="keywords" content="Machine Learning,python,Sklearn">
<meta property="og:type" content="article">
<meta property="og:title" content="Hands on Machine Learning-Chapter 3">
<meta property="og:url" content="http://yoursite.com/2019/10/01/Lecture 4 Classification/index.html">
<meta property="og:site_name" content="LXY&#39;s Blog">
<meta property="og:description" content="《Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Technique for Building Intelligent Systems》—-Chapter 3  This notebook contains all the sample code and solutions to t">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lecture%204%20Classification/output_16_1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lecture%204%20Classification/output_20_1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lecture%204%20Classification/output_63_1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lecture%204%20Classification/output_69_1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lecture%204%20Classification/output_73_1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lecture%204%20Classification/output_79_1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lecture%204%20Classification/output_103_1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lecture%204%20Classification/output_106_1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lecture%204%20Classification/output_108_1.png">
<meta property="og:updated_time" content="2019-10-08T22:40:21.731Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hands on Machine Learning-Chapter 3">
<meta name="twitter:description" content="《Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Technique for Building Intelligent Systems》—-Chapter 3  This notebook contains all the sample code and solutions to t">
<meta name="twitter:image" content="http://yoursite.com/2019/10/01/Lecture%204%20Classification/output_16_1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/10/01/Lecture 4 Classification/">





  <title>Hands on Machine Learning-Chapter 3 | LXY's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LXY's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/01/Lecture 4 Classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Hands on Machine Learning-Chapter 3</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-01T19:52:14-04:00">
                2019-10-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/Hands-on-Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Hands-on Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>《Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Technique for Building Intelligent Systems》—-Chapter 3</p>
</blockquote>
<p><em>This notebook contains all the sample code and solutions to the exercises in chapter 3.</em></p>
<p>In previous lesson we predicted values, now we will be predicting classes. </p>
<h1 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h1><p>First, let’s make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># To support both python 2 and python 3</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division, print_function, unicode_literals</span><br><span class="line"></span><br><span class="line"><span class="comment"># Common imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># to make this notebook's output stable across runs</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># To plot pretty figures</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">'axes.labelsize'</span>] = <span class="number">14</span></span><br><span class="line">plt.rcParams[<span class="string">'xtick.labelsize'</span>] = <span class="number">12</span></span><br><span class="line">plt.rcParams[<span class="string">'ytick.labelsize'</span>] = <span class="number">12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Where to save the figures</span></span><br><span class="line">PROJECT_ROOT_DIR = <span class="string">'..'</span></span><br><span class="line">CHAPTER_ID = <span class="string">"classification"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_fig</span><span class="params">(fig_id, tight_layout=True)</span>:</span></span><br><span class="line">    path = os.path.join(PROJECT_ROOT_DIR, <span class="string">"images"</span>, CHAPTER_ID, fig_id + <span class="string">".png"</span>)</span><br><span class="line">    print(<span class="string">"Saving figure"</span>, fig_id)</span><br><span class="line">    <span class="keyword">if</span> tight_layout:</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">    plt.savefig(path, format=<span class="string">'png'</span>, dpi=<span class="number">300</span>)</span><br></pre></td></tr></table></figure>

<h1 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h1><p>We will use data on handwriting of 70,000 digits written by school children and Census employees trying to correctly classify the digits.</p>
<p>As the code ‘fetch_mldata’ is not working anymore, my professor set a new function ‘fetch_mnist’ to change the url and the path of MNIST data.</p>
<blockquote>
<p><code>fetch_mldata</code> is <a href="https://scikit-learn.org/0.20/modules/generated/sklearn.datasets.fetch_mldata.html" target="_blank" rel="noopener">deprecated</a> since scikit-learn v0.20, and replaced with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html" target="_blank" rel="noopener"><code>fetch_openml</code></a>;<br>from <a href="https://stackoverflow.com/questions/57061437/why-am-i-getting-the-following-connectionreseterror-for-mnist-fetch-mldata" target="_blank" rel="noopener">stack overflow</a></p>
<p>Actually, the code to fetch MNIST is :</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_openml</span><br><span class="line">&gt; X, y = fetch_openml(<span class="string">'mnist_784'</span>, version=<span class="number">1</span>, return_X_y=<span class="literal">True</span>) </span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p><strong>But, in this course, professor uses a different way to do the same thing. And in this note I will follow the professor’s version as it is a class note.</strong></p>
</blockquote>
<p>Define a function to copy MNIST from Github.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run this first </span></span><br><span class="line"><span class="keyword">from</span> shutil <span class="keyword">import</span> copyfileobj</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.base <span class="keyword">import</span> get_data_home</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_mnist</span><span class="params">(data_home=None)</span>:</span></span><br><span class="line">    mnist_alternative_url = <span class="string">"https://github.com/amplab/datascience-sp14/raw/master/lab7/mldata/mnist-original.mat"</span></span><br><span class="line">    data_home = get_data_home(data_home=data_home)</span><br><span class="line">    data_home = os.path.join(data_home, <span class="string">'mldata'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(data_home):</span><br><span class="line">        os.makedirs(data_home)</span><br><span class="line">    mnist_save_path = os.path.join(data_home, <span class="string">"mnist-original.mat"</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(mnist_save_path):</span><br><span class="line">        mnist_url = urllib.request.urlopen(mnist_alternative_url)</span><br><span class="line">        <span class="keyword">with</span> open(mnist_save_path, <span class="string">"wb"</span>) <span class="keyword">as</span> matlab_file:</span><br><span class="line">            copyfileobj(mnist_url, matlab_file)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">    path = <span class="string">'../../data/mnist.pkl.gz'</span></span><br><span class="line">    f = gzip.open(path, <span class="string">'rb'</span>)</span><br><span class="line">    training_data, validation_data, test_data = Pickle.load(f)</span><br><span class="line">    f.close()</span><br><span class="line"></span><br><span class="line">    X_train, y_train = training_data[<span class="number">0</span>], training_data[<span class="number">1</span>]</span><br><span class="line">    print(X_train.shape, y_train.shape)</span><br><span class="line">    <span class="comment"># (50000L, 784L) (50000L,)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># get the first image and it's label</span></span><br><span class="line">    img1_arr, img1_label = X_train[<span class="number">0</span>], y_train[<span class="number">0</span>]</span><br><span class="line">    print(img1_arr.shape, img1_label)</span><br><span class="line">    <span class="comment"># (784L,) , 5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># reshape first image(1 D vector) to 2D dimension image</span></span><br><span class="line">    img1_2d = np.reshape(img1_arr, (<span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">    <span class="comment"># show it</span></span><br><span class="line">    plt.subplot(<span class="number">111</span>)</span><br><span class="line">    plt.imshow(img1_2d, cmap=plt.get_cmap(<span class="string">'gray'</span>))</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fetch the data</span></span><br><span class="line">fetch_mnist()</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_mldata</span><br><span class="line">mnist = fetch_mldata(<span class="string">"MNIST original"</span>)</span><br><span class="line">mnist</span><br></pre></td></tr></table></figure>

<p>Result:</p>
<pre><code>{&apos;DESCR&apos;: &apos;mldata.org dataset: mnist-original&apos;,
 &apos;COL_NAMES&apos;: [&apos;label&apos;, &apos;data&apos;],
 &apos;target&apos;: array([0., 0., 0., ..., 9., 9., 9.]),
 &apos;data&apos;: array([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}</code></pre><p>Scikit-learn data have a dictionary (DESCR), “data” for features and “target” for labels</p>
<blockquote>
<p>Skleran 数据集具有类似dictionary的结构：</p>
<ol>
<li>DESCR键：描述数据集</li>
<li>data键：含有一个数组，每一个实例为1行，每个特征为1列</li>
<li>target键 ：一个标签的数组</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X, y = mnist[<span class="string">"data"</span>], mnist[<span class="string">"target"</span>]</span><br><span class="line">X.shape</span><br></pre></td></tr></table></figure>

<p>OUT:(70000, 784)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.shape</span><br></pre></td></tr></table></figure>

<p>OUT:(70000,)</p>
<p>Each images has 28 by 28 pixels, with each pixed containing information on color intensity from 0 (white) to 255 (black). Let’s plot it.</p>
<blockquote>
<p> 数据X共有7万张图片，每张图片有784个特征。因为图片是28×28像素，每个特征代表了一个像素点的强度，从0（白色）到255（黑色），X[36000]的数字如下，通过“y[36000]”查看其标签为“5”。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#plot X 36000</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># Find the image number 36000</span></span><br><span class="line">some_digit = X[<span class="number">36000</span>]</span><br><span class="line"><span class="comment"># Reshape vector into a matrix</span></span><br><span class="line">some_digit_image = some_digit.reshape(<span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"><span class="comment"># Plot the image. matplotlib.cm.binary is the black-white coloring scheme. </span></span><br><span class="line"><span class="comment"># Interpolation is the smoothing of colors</span></span><br><span class="line">plt.imshow(some_digit_image, cmap = matplotlib.cm.binary,</span><br><span class="line">           interpolation=<span class="string">"nearest"</span>)</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line"></span><br><span class="line">save_fig(<span class="string">"some_digit_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Saving figure some_digit_plot</code></pre><p><img src="output_16_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># As you might have guessed the correct number is:</span></span><br><span class="line">y[<span class="number">36000</span>]</span><br></pre></td></tr></table></figure>

<p>OUT: 5.0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a function to draw the picture above.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_digit</span><span class="params">(data)</span>:</span></span><br><span class="line">    image = data.reshape(<span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    plt.imshow(image, cmap = matplotlib.cm.binary,</span><br><span class="line">               interpolation=<span class="string">"nearest"</span>)</span><br><span class="line">    plt.axis(<span class="string">"off"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># EXTRA Let's plot 10 by 10 graph</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_digits</span><span class="params">(instances, images_per_row=<span class="number">10</span>, **options)</span>:</span></span><br><span class="line">    <span class="comment"># image size</span></span><br><span class="line">    size = <span class="number">28</span></span><br><span class="line">    <span class="comment"># number if images per row is 10 or less </span></span><br><span class="line">    images_per_row = min(len(instances), images_per_row)</span><br><span class="line">    <span class="comment"># reshape data into 28 by 28 matrix</span></span><br><span class="line">    images = [instance.reshape(size,size) <span class="keyword">for</span> instance <span class="keyword">in</span> instances]</span><br><span class="line">    <span class="comment"># Number of rows is a floor (min = 1)</span></span><br><span class="line">    n_rows = (len(instances) - <span class="number">1</span>) // images_per_row + <span class="number">1</span></span><br><span class="line">    row_images = []</span><br><span class="line">    n_empty = n_rows * images_per_row - len(instances)</span><br><span class="line">    images.append(np.zeros((size, size * n_empty)))</span><br><span class="line">    <span class="comment"># loop filling the rows of images</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> range(n_rows):</span><br><span class="line">        <span class="comment"># form each row out of images per wor</span></span><br><span class="line">        rimages = images[row * images_per_row : (row + <span class="number">1</span>) * images_per_row]</span><br><span class="line">        <span class="comment"># append images to form row</span></span><br><span class="line">        row_images.append(np.concatenate(rimages, axis=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># append rows to form matrix</span></span><br><span class="line">    image = np.concatenate(row_images, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment">#plot the image</span></span><br><span class="line">    plt.imshow(image, cmap = matplotlib.cm.binary, **options)</span><br><span class="line">    plt.axis(<span class="string">"off"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">9</span>,<span class="number">9</span>))</span><br><span class="line">example_images = np.r_[X[:<span class="number">12000</span>:<span class="number">600</span>], X[<span class="number">13000</span>:<span class="number">30600</span>:<span class="number">600</span>], X[<span class="number">30600</span>:<span class="number">60000</span>:<span class="number">590</span>]]</span><br><span class="line">print(len(example_images))</span><br><span class="line">plot_digits(example_images, images_per_row=<span class="number">10</span>)</span><br><span class="line">save_fig(<span class="string">"more_digits_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>100
Saving figure more_digits_plot</code></pre><p><img src="output_20_1.png" alt="png"></p>
<h2 id="Set-data"><a href="#Set-data" class="headerlink" title="Set data"></a>Set data</h2><p>Training: the first 60000 images; Testing: the last 10000 images.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We randomly assign 60000 obs to training data and the rest to the testing data</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X_train, X_test, y_train, y_test = X[:<span class="number">60000</span>], X[<span class="number">60000</span>:], y[:<span class="number">60000</span>], y[<span class="number">60000</span>:]</span><br><span class="line">shuffle_index = np.random.permutation(<span class="number">60000</span>)</span><br><span class="line">X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]</span><br></pre></td></tr></table></figure>

<h1 id="Training-a-Binary-classifier"><a href="#Training-a-Binary-classifier" class="headerlink" title="Training a Binary classifier"></a>Training a Binary classifier</h1><p>Let’s start with a simple task to classify one digit (5). The classifier is true for all 5s, False for all other digits.</p>
<blockquote>
<p>现在先简化问题，只尝试识别一个数字——比如数字5。那么这个“数字5检测器”就是一个二元分类器的例子，它只能区分两个类别：5和非5。先为此分类任务创建目标向量(将数字标签转换为bool型标签true代表 5，false代表 非5)：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_train_5 = (y_train == <span class="number">5</span>)</span><br><span class="line">y_test_5 = (y_test == <span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<p>Let’s start with a Stochastic Gradient Descent (SGD) classifier, using Scikit-Learn’s SGDClassifier class. This classifier has the advantage of being capable of handling very large datasets efficiently.<br>This is in part because SGD deals with training instances independently, one at a time<br>(which also makes SGD well suited for online learning), as we will see later. Let’s create<br>an SGDClassifier and train it on the whole training set:</p>
<blockquote>
<p>接着挑选一个分类器并开始训练。一个好的初始选择是随机梯度下降（SGD）分类器，使用Scikit-Learn的SGDClassifier类即可。这个分类器的优势是，能够有效处理非常大型的数据集。这部分是因为SGD独立处理训练实例，一次一个。此时先创建一个SGDClassifier并在整个训练集上进行训练：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train_5</span><br></pre></td></tr></table></figure>

<p>OUT: array([False, False, False, …, False, False, False])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The alhorithm relies on randomness and for reproducibility requires random_state parameter.</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"></span><br><span class="line">sgd_clf = SGDClassifier(max_iter=<span class="number">5</span>, random_state=<span class="number">42</span>) <span class="comment">#max_iter is the max number of passes the training data (aka epochs)</span></span><br><span class="line">sgd_clf.fit(X_train, y_train_5)</span><br></pre></td></tr></table></figure>

<pre><code>SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,
              l1_ratio=0.15, learning_rate=&apos;optimal&apos;, loss=&apos;hinge&apos;, max_iter=5,
              n_iter_no_change=5, n_jobs=None, penalty=&apos;l2&apos;, power_t=0.5,
              random_state=42, shuffle=True, tol=0.001, validation_fraction=0.1,
              verbose=0, warm_start=False)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test for our digit.</span></span><br><span class="line">sgd_clf.predict([some_digit])</span><br></pre></td></tr></table></figure>

<p>OUT: array([ True])</p>
<h1 id="Performance-Measure"><a href="#Performance-Measure" class="headerlink" title="Performance Measure"></a>Performance Measure</h1><p>Evaluating a classifier is often significantly trickier than evaluating a regressor, so we will spend a large<br>part of this chapter on this topic.</p>
<h2 id="Measuring-Accuracy-Using-Cross-Validation"><a href="#Measuring-Accuracy-Using-Cross-Validation" class="headerlink" title="Measuring Accuracy Using Cross-Validation"></a>Measuring Accuracy Using Cross-Validation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's start with accuracy</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">cross_val_score(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure>

<p>OUT: array([0.96225, 0.9645 , 0.94765])</p>
<p>Wow! Above 95% accuracy (ratio of correct predictions) on all cross-validation folds?<br>This looks amazing, doesn’t it? Well, before you get too excited, let’s look at a very dumb classifier that just classifies every single image in the “not-5” class:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's look at a simple binary classifier: correctly classify 5 vs. not-5 digits.</span></span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Never5Classifier</span><span class="params">(BaseEstimator)</span>:</span></span><br><span class="line">    <span class="comment"># We don't really fit anything</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># We return a vector of zeros effectively predicting that all digits are not-5</span></span><br><span class="line">        <span class="keyword">return</span> np.zeros((len(X), <span class="number">1</span>), dtype=bool)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">never_5_clf = Never5Classifier()</span><br><span class="line">cross_val_score(never_5_clf, X_train, y_train_5, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>array([0.909  , 0.90715, 0.9128 ])</code></pre><p>Now 95% accuracy does not look that impressive. This is simply because only about 10% of the images are 5s, so if you always guess that an image is not a 5, you will be right about 90% of the time.<br>This demonstrates why accuracy is generally not the preferred performance measure for classifiers, especially when you are dealing with skewed datasets (i.e., when some classes are much more frequent than others).</p>
<blockquote>
<p>所有折叠交叉验证的准确率（正确预测的比率）超过95%？这是因为只有大约10%的图像是数字5，所以如果你猜一张图不是5，90%的情况你都是正确的，这说明准确率通常无法成为分类器的首要性能指标，特别是当你处理偏斜数据集（skewed dataset）的时候（即某些类比其他类更为频繁）。</p>
</blockquote>
<p><code>Cross_val_predict</code> predicts evaluation score for each fold. The model estimated in each prediction is based on a different fold.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_predict</span><br><span class="line">y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>)</span><br><span class="line">y_train_pred</span><br></pre></td></tr></table></figure>

<p>OUT: array([False, False, False, …, False, False, False])</p>
<h2 id="Confusion-Matrix"><a href="#Confusion-Matrix" class="headerlink" title="Confusion Matrix"></a>Confusion Matrix</h2><table>
<thead>
<tr>
<th></th>
<th>Predict No</th>
<th align="left">Peridict Yes</th>
</tr>
</thead>
<tbody><tr>
<td>Actual No</td>
<td>True Negatives</td>
<td align="left">False Positives</td>
</tr>
<tr>
<td>Actual Yes</td>
<td>False Negatives</td>
<td align="left">True Positives</td>
</tr>
</tbody></table>
<p>现在，可以使用confusion_matrix（）函数来获取混淆矩阵了。只需要给出目标类别（y_train_5）和预测类别（y_train_pred）即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line">confusion_matrix(y_train_5, y_train_pred)</span><br></pre></td></tr></table></figure>

<p>OUT: array([[53417,  1162],<br>                      [ 1350,  4071]])</p>
<blockquote>
<p>混淆矩阵中的行表示实际类别，列表示预测类别。本例中第一行表示所有“非5”（负类）的图片中：53417张被正确地分为“非5”类别（真负类），1350张被错误地分类成了“5”（假正类）；第二行表示所有“5”（正类）的图片中：1162张被错误地分为“非5”类别（假负类），4071张被正确地分在了“5”这一类别（真正类）。</p>
</blockquote>
<p>We have 10 times as many false positive as false negatives. This is because our signal (5) is rare relative to the noise (not-5), so we are much more likely to get false positives. </p>
<p><strong>Example of perfect prediction. We made a table from two identical vectors. No errors!</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_train_perfect_predictions = y_train_5</span><br><span class="line">confusion_matrix(y_train_5, y_train_perfect_predictions)</span><br></pre></td></tr></table></figure>

<p>OUT: array([[54579,     0],<br>                      [    0,  5421]])</p>
<h2 id="Precision-amp-Recall"><a href="#Precision-amp-Recall" class="headerlink" title="Precision &amp; Recall"></a>Precision &amp; Recall</h2><p>An interesting one to look at is the accuracy of the positive predictions this is called the precision(精度) of the classifier. </p>
<p>TP is the number of <strong>true positives</strong>, and FP is the number of <strong>false positives</strong>.</p>
<p>A trivial way to have perfect precision is to make one single positive prediction and ensure it is correct (precision = 1/1 = 100%). This would not be very useful since the classifier would ignore all but one positive instance. </p>
<p>So precision is typically used along with another metric named recall(召回率）, also called sensitivity(灵敏度) or true positive rate (TPR)(真正类率) : this is the ratio of positive instances that are correctly detected by the classifier .</p>
<p>FN is of course the number of false negatives.<br>Precision is the share of correctly identified positive values.  <br>Precision = $\frac{TP}{TP + FP}$ <br><br>Recall is the fraction of the true positive values identified.  <br><br>Recall = $\frac{TP}{TP + FN}$  <br><br>Accuracy is the total error rate. It works if the positive and negative value are roughly balanced.  <br><br>Accuracy = $\frac{TP + TN}{P + N}$  <br></p>
<blockquote>
<p>An illustrated confusion matrix</p>
</blockquote>
<p>![1570221657909](D:\HexoBlog\source_posts\Lecture 4 Classification\confusion matrix.png)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score</span><br><span class="line">precision_score(y_train_5, y_train_pred) </span><br><span class="line"><span class="comment"># Our of all digits we classified as fives, 77% are really fives.</span></span><br></pre></td></tr></table></figure>

<p>OUT: 0.7779476399770686</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Remember precision is TP / (TP + FP)</span></span><br><span class="line"><span class="number">4344</span> / (<span class="number">4344</span> + <span class="number">1307</span>)</span><br></pre></td></tr></table></figure>

<p>OUT: 0.7687135020350381</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Out of all fives in the data we classified as fives 80%</span></span><br><span class="line">recall_score(y_train_5, y_train_pred)</span><br></pre></td></tr></table></figure>

<p>OUT: 0.7509684560044272</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">4344</span> / (<span class="number">4344</span> + <span class="number">1077</span>)</span><br></pre></td></tr></table></figure>

<p>OUT: 0.801328168234643</p>
<p>Now your 5-detector does not look as shiny as it did when you looked at its accuracy. When it claims an image represents a 5, it is correct only 77% of the time. Moreover, it only detects 79% of the 5s.</p>
<p>It is often convenient to combine precision and recall into a single metric called the F1 score, in particular if you need a simple way to compare two classifiers. The F1 score is the harmonic mean of precision and recall . Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only get a high F1 score if both recall and precision arehigh. <br> <br><br>$$<br> F1 = \frac{2}{\frac{1}{Precision} + \frac{1}{Recall}} =<br>2 * \frac{Precision * Recall}{Precision + Recall} = \frac{TP}{TP + \frac{FN + FP}{2}}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line">f1_score(y_train_5, y_train_pred)</span><br><span class="line"><span class="comment"># F1 is a good choice for data where the share of positives is greatly different from 0.5</span></span><br></pre></td></tr></table></figure>

<p>OUT: 0.7642200112633752</p>
<h2 id="Precision-Recall-Tradeoff"><a href="#Precision-Recall-Tradeoff" class="headerlink" title="Precision\Recall Tradeoff"></a>Precision\Recall Tradeoff</h2><p>The F1 score favors classifiers that have similar precision and recall. <br><br>This is not always what you want: in some contexts you mostly care about precision, and in other contexts you really care about recall.<br></p>
<p>For example, if you trained a classifier to detect videos that are safe for kids, you would probably prefer a classifier that rejects many good videos (low recall) but keeps only safe ones (high precision), rather than a classifier<br>that has a much higher recall but lets a few really bad videos show up in your<br>product. <br><br>On the other hand, suppose you train a classifier to detect shoplifters on surveillance images: it is probably fine if your classifier has only 30% precision as long as it has 99% recall (sure, the security guards will get a few false alerts, but almost all shoplifters will get caught). <br></p>
<p>*<em>Unfortunately, you can’t have it both ways: increasing precision reduces recall, and vice versa. This is called the precision/recall tradeoff. *</em><br></p>
<p>Imagine your algorithm produces a score how much each digit resembles digit 5. Let’s the algorithm decide that it is optimal to label all digits with a score greater than 0.8 as digit 5. If we artificially increase the benchmark to 0.9, we will increase our precision (almost all digits we classify as fives would be actually fives), but we would reduce recall (we would catch fewer fives in the data).  Lowering the benchmark to 0.5 would produce an opposite effect. </p>
<p>Decision function of the SGD algorithm produces a similarity score used in the classification. By default if the score it greater than 0 the observation is classified as positive.</p>
<p>Scikit-Learn不允许直接设置阈值，但是可以访问它用于预测的决策分数。不是调用分类器的predict()方法，而是调用decision_function（）方法，这个方法返回每个实例的分数，然后就可以根据这些分数，使用任意阈值进行预测了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y_scores = sgd_clf.decision_function([some_digit])</span><br><span class="line">y_scores</span><br><span class="line"><span class="comment"># In this case the score is greteer than 0, so the digit is classified as 5.</span></span><br><span class="line">OUT: array([<span class="number">150526.40944343</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">threshold = <span class="number">0</span></span><br><span class="line">y_some_digit_pred = (y_scores &gt; threshold)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_some_digit_pred</span><br><span class="line">OUT: array([ <span class="literal">True</span>])</span><br></pre></td></tr></table></figure>

<p>SGDClassifier分类器使用的阈值是0，所以前面的代码返回结果与predict（）方法一样（也就是True）。我们来试试提升阈值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If we increase a threshhold greater than 161855..</span></span><br><span class="line">threshold = <span class="number">200000</span></span><br><span class="line">y_some_digit_pred = (y_scores &gt; threshold)</span><br><span class="line">y_some_digit_pred</span><br><span class="line">OUT: array([<span class="literal">False</span>])</span><br></pre></td></tr></table></figure>

<p>这证明了提高阈值确实可以降低召回率。这张图确实是5，当阈值为0时，分类器可以检测到该图，但是当阈值提高到200000时，就错过了这张图。 </p>
<p>那么要如何决定使用什么阈值呢？首先，使用cross_val_predict（）函数获取训练集中所有实例的分数，但是这次需要它返回的是决策分数而不是预测结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Get the secision scores</span></span><br><span class="line">y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>,</span><br><span class="line">                             method=<span class="string">"decision_function"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Vector of decision score for the whole training datasets</span></span><br><span class="line">y_scores.shape</span><br><span class="line">OUT:(<span class="number">60000</span>,)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hack to work around issue #9589 in Scikit-Learn 0.19.0</span></span><br><span class="line"><span class="keyword">if</span> y_scores.ndim == <span class="number">2</span>:</span><br><span class="line">    y_scores = y_scores[:, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<p>.有了这些分数，可以使用precision_recall_curve（）函数来计算所有可能的阈值的精度和召回率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve</span><br><span class="line">precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)</span><br></pre></td></tr></table></figure>

<p>The algorithm picked a threshold that maximizes both prevision and recall. Lowering the threshold increases recall and decreases precision. Increasing the threshold leads to the opposite effect.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_precision_recall_vs_threshold</span><span class="params">(precisions, recalls, thresholds)</span>:</span></span><br><span class="line">    plt.plot(thresholds, precisions[:<span class="number">-1</span>], <span class="string">"b--"</span>, label=<span class="string">"Precision"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.plot(thresholds, recalls[:<span class="number">-1</span>], <span class="string">"g-"</span>, label=<span class="string">"Recall"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Threshold"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">"upper left"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.ylim([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">plot_precision_recall_vs_threshold(precisions, recalls, thresholds)</span><br><span class="line">plt.xlim([<span class="number">-700000</span>, <span class="number">700000</span>])</span><br><span class="line">save_fig(<span class="string">"precision_recall_vs_threshold_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_63_1.png" alt="png"></p>
<p>Precision is more bumpy. Generally there is an inverse relationship between precision and recall. However, there are exceptions. Consider the following:     2 2 3 4 5 2 4 5 | 4  5 5 5. Current precision is 3/4 = 0.75, recall 3/5 = 0.6. If we move the threshold to 2 2 3 4 5 2 4 | 5 4 5 5 5 we caught one more five, it increases both precision 4/5 = 0.8 and recall 4/5 = 0.8. This behavior is more likely in the right tail of threshold distribution when we are likely to catch 5 if we move the threshold to the right.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(y_train_pred == (y_scores &gt; <span class="number">0</span>)).all()</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If you aim for a 90% precision, which occurs around threshold = 105000</span></span><br><span class="line">y_train_pred_90 = (y_scores &gt; <span class="number">105000</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">precision_score(y_train_5, y_train_pred_90)</span><br><span class="line">OUT:<span class="number">0.9007936507936508</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">recall_score(y_train_5, y_train_pred_90)</span><br><span class="line">OUT:<span class="number">0.586238701346615</span></span><br></pre></td></tr></table></figure>

<p>现在，就可以通过轻松选择阈值来实现最佳的精度/召回率权衡了。还有一种找到好的精度/召回率权衡的方法是直接绘制精度和召回率的函数图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_precision_vs_recall</span><span class="params">(precisions, recalls)</span>:</span></span><br><span class="line">    plt.plot(recalls, precisions, <span class="string">"b-"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"Recall"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Precision"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plot_precision_vs_recall(precisions, recalls)</span><br><span class="line">save_fig(<span class="string">"precision_vs_recall_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_69_1.png" alt="png"></p>
<h2 id="ROC-Curves"><a href="#ROC-Curves" class="headerlink" title="ROC Curves"></a>ROC Curves</h2><p>The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It plots the true positive rate (another name for recall) against the false positive rate. <br><br>$TPR = \frac{TP}{TP + FN}$ <br><br>$FPR = \frac{FP}{FP + TN}$</p>
<p>还有一种经常与二元分类器一起使用的工具，叫作受试者工作特征曲线（简称ROC）。它与精度/召回率曲线非常相似，但绘制的不是精度和召回率，而是真正类率（召回率的另一名称）和假正类率（FPR）。FPR是被错误分为正类的负类实例比率。它等于1减去真负类率（TNR），后者是被正确分类为负类的负类实例比率，也称为特异度。因此，ROC曲线绘制的是灵敏度和（1-特异度）的关系。</p>
<p>Use roc_curve() to calculate TPR and FPR for various shreshold values:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_roc_curve</span><span class="params">(fpr, tpr, label=None)</span>:</span></span><br><span class="line">    plt.plot(fpr, tpr, linewidth=<span class="number">2</span>, label=label)</span><br><span class="line">    plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">'k--'</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    plt.xlabel(<span class="string">'False Positive Rate'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'True Positive Rate'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plot_roc_curve(fpr, tpr)</span><br><span class="line">save_fig(<span class="string">"roc_curve_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_73_1.png" alt="png"></p>
<p>Tradeoff: the higher the recall (TPR), the more false positives (FPR) the classifier produces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner). <br> <br></p>
<p>One way to compare classifiers is to measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC AUC. Hi AUC works for many different TPR-FPR choice. </p>
<blockquote>
<p>虚线表示纯随机分类器的ROC曲线；一个优秀的分类器应该离这条线越远越好（向左上角）。有一种比较分类器的方法是测量曲线下面积（AUC）。完美的分类器的ROC AUC等于1，而纯随机分类器的ROC AUC等于0.5。</p>
</blockquote>
<p>Use PR curve when the positive class is rare and you care more about false positives than about false negatives. Use ROC, AUC otherwise. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line">roc_auc_score(y_train_5, y_scores)</span><br><span class="line">OUT:<span class="number">0.9562435587387078</span></span><br></pre></td></tr></table></figure>

<p>Let’s train a RandomForestClassifier and compare its ROC curve and ROC AUC<br>score to the SGDClassifier. <br></p>
<p>We calculate score for each instance in the training data. RandomForestClassifier class does not have a decision_function(),  instead it has predict_proba(). Each instance is assigned a probability of belonging to each class.</p>
<blockquote>
<p>训练一个RandomForestClassifier分类器，并比较它和SGDClassifier分类器的ROC曲线和ROC AUC分数。首先，获取训练集中每个实例的分数。但是由于它的工作方式不同（参见第7章），RandomForestClassifier类没有decision_function（）方法，相反，它有的是dict_proba（）方法。Scikit-Learn的分类器通常都会有这两种方法的其中一种。dict_proba（）方法会返回一个数组，其中每行为一个实例，每列代表一个类别，意思是某个给定实例属于某个给定类别的概率（例如，这张图片有70%的可能是数字5）：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">forest_clf = RandomForestClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=<span class="number">3</span>,</span><br><span class="line">                                    method=<span class="string">"predict_proba"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_scores_forest = y_probas_forest[:, <span class="number">1</span>] <span class="comment"># score = probability of positive class</span></span><br><span class="line"><span class="comment"># Get fit data from the random forest</span></span><br><span class="line">fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">plt.plot(fpr, tpr, <span class="string">"b:"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"SGD"</span>)</span><br><span class="line">plot_roc_curve(fpr_forest, tpr_forest, <span class="string">"Random Forest"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower right"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">save_fig(<span class="string">"roc_curve_comparison_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Saving figure roc_curve_comparison_plot</code></pre><p><img src="output_79_1.png" alt="png"></p>
<p>Random forest has very good fit statistics</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">roc_auc_score(y_train_5, y_scores_forest)</span><br><span class="line">OUT:<span class="number">0.9931243366003829</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=<span class="number">3</span>)</span><br><span class="line">precision_score(y_train_5, y_train_pred_forest)</span><br><span class="line">OUT:<span class="number">0.9852973447443494</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">recall_score(y_train_5, y_train_pred_forest)</span><br><span class="line">OUT:<span class="number">0.8282604685482383</span></span><br></pre></td></tr></table></figure>

<p>由于ROC曲线与精度/召回率（或PR）曲线非常相似，因此你可能会问如何决定使用哪种曲线。有一个经验法则是，当正类非常少见或者你更关注假正类而不是假负类时，你应该选择PR曲线，反之则是ROC曲线。例如，看前面的ROC曲线图（以及ROC  AUC分数），你可能会觉得分类器真不错。但这主要是因为跟负类（非5）相比，正类（数字5）的数量真得很少。相比之下，PR曲线清楚地说明分类器还有改进的空间（曲线还可以更接近右上角）。</p>
<h1 id="Multiclass-classification"><a href="#Multiclass-classification" class="headerlink" title="Multiclass classification"></a>Multiclass classification</h1><p>Whereas binary classifiers distinguish between two classes, multiclass classifiers can distinguish between more than two classes.Some algorithms (such as Random Forest classifiers or naive Bayes classifiers) are capable of handling multiple classes directly. Others (such as Support Vector Machine classifiers or Linear classifiers) are strictly binary classifiers. <br> </p>
<p>You can use binary classifiers to estimate multiclass classification: <br></p>
<ol>
<li><p>Create 10 binary classifier for each digit versus the rest of the digits, similar to our five or not-five classifiers. After we estimate 10 classifiers for each digit we set a label for the classifier that produced the highest score. This is called the one-versus-all (OvA) strategy (also called one-versus-the-rest). <br></p>
</li>
<li><p>Another strategy is o train binary classifier for each paid of digits: 0s vs 1s, 0 vs 4s, etc. This is called the one-versus-one (OvO) strategy. If there are N classes, you need to train N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45 binary classifiers! For each digit we pick the label with the highest average score against 9 other digits. The main advantage of OvO is that each classifier only needs to be trained on the part of the training set for the two classes that it must distinguish. <br></p>
</li>
</ol>
<p>Algorithms that scales poorly (like SVM) are preferable for OVO, which reduce the data used in the estimation. However, most often  OvA is preferred and it is a usually the default option for binary classifiers other than SVM.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's try SGD algorithm.</span></span><br><span class="line">sgd_clf.fit(X_train, y_train) <span class="comment">#y_train not y_train_5</span></span><br><span class="line">sgd_clf.predict([some_digit])</span><br><span class="line">OUT: array([<span class="number">5.</span>])</span><br></pre></td></tr></table></figure>

<p>这段代码使用原始目标类别0到9（y_train）在训练集上对SGDClassifier进行训练，而不是以“5”和“剩余”作为目标类别（y_train_5）。然后做出预测（在本例中预测正确）。而在内部，Scikit-Learn实际上训练了10个二元分类器，获得它们对图片的决策分数，然后选择了分数最高的类别。</p>
<p>想要知道是不是这样，可以调用decision_function（）方法。它会返回10个分数，每个类别1个，而不再是每个实例返回1个分数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's look at the 10 estimated scores for the digit. 5 has the higher score  161855.74572176.</span></span><br><span class="line">some_digit_scores = sgd_clf.decision_function([some_digit])</span><br><span class="line">some_digit_scores</span><br></pre></td></tr></table></figure>

<pre><code>OUT:array([[-152619.46799791, -441052.22074349, -249930.3138537 ,
        -237258.35168498, -447251.81933158,  120565.05820991,
        -834139.15404835, -188142.48490477, -555223.79499145,
        -536978.92518594]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Which vector has the highest score</span></span><br><span class="line">np.argmax(some_digit_scores)</span><br><span class="line">OUT: <span class="number">5</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sgd_clf.classes_</span><br><span class="line">OUT: array([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">7.</span>, <span class="number">8.</span>, <span class="number">9.</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sgd_clf.classes_[<span class="number">5</span>]</span><br><span class="line">OUT: <span class="number">5.0</span></span><br></pre></td></tr></table></figure>

<p>当训练分类器时，目标类别的列表会存储在classes_这个属性中，按值的大小排序。在本例里，classes_数组中每个类别的索引正好对应其类别本身（例如，索引上第5个类别正好是数字5这个类别），但是一般来说，不会这么恰巧。</p>
<p> 如果想要强制Scikit-Learn使用一对一或者一对多策略，可以使用OneVsOne Classifier或OneVsRestClassifier类。只需要创建一个实例，然后将二元分类器传给其构造函数。例如，下面这段代码使用OvO策略，基于SGDClassifier创建了一个多类别分类器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Here is the example of the OVO classifier.</span></span><br><span class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsOneClassifier</span><br><span class="line">ovo_clf = OneVsOneClassifier(SGDClassifier(max_iter=<span class="number">5</span>, random_state=<span class="number">42</span>))</span><br><span class="line">ovo_clf.fit(X_train, y_train)</span><br><span class="line">ovo_clf.predict([some_digit])</span><br><span class="line">OUT: array([<span class="number">5.</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># As expected we estimated 45 binary classfiers</span></span><br><span class="line">len(ovo_clf.estimators_)</span><br></pre></td></tr></table></figure>

<pre><code>45</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">forest_clf.fit(X_train, y_train)</span><br><span class="line">forest_clf.predict([some_digit])</span><br></pre></td></tr></table></figure>

<pre><code>array([5.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">forest_clf.predict_proba([some_digit])</span><br><span class="line"><span class="comment"># This classifier returns probabilities. The probability of digit 5 is 80%, 10% that it is 0 and 10% that it is 3.</span></span><br></pre></td></tr></table></figure>

<pre><code>array([[0.1, 0. , 0. , 0.1, 0. , 0.8, 0. , 0. , 0. , 0. ]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train</span><br></pre></td></tr></table></figure>

<pre><code>array([1., 6., 6., ..., 0., 2., 9.])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Perfomance of OVA SGD.</span></span><br><span class="line">cross_val_score(sgd_clf, X_train, y_train, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br><span class="line"><span class="comment"># Accuracy of three folds</span></span><br></pre></td></tr></table></figure>

<pre><code>array([0.84993001, 0.81769088, 0.84707706])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Usually scaling the data improves the accracy</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))</span><br><span class="line">cross_val_score(sgd_clf, X_train_scaled, y_train, cv=<span class="number">3</span>, scoring=<span class="string">"accuracy"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>array([0.91211758, 0.9099955 , 0.90643597])</code></pre><h1 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h1><p>Real life project has multiple steps such as exploring data preparation options, trying out multiple models, shortlisting the best ones and fine-tuning their hyperparameters using GridSearchCV, and automating as much as possible.</p>
<p>Here, we will assume that you have found a promising model and you want to find ways to improve it. One way to do this is to analyze the types of errors it makes.</p>
<p>首先，看看混淆矩阵。就像之前做的，使用cross_val_predict（）函数进行预测，然后调用confusion_matrix（）函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Let's look at the confusion matrix</span></span><br><span class="line">y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=<span class="number">3</span>)</span><br><span class="line">conf_mx = confusion_matrix(y_train, y_train_pred)</span><br><span class="line">conf_mx</span><br></pre></td></tr></table></figure>

<pre><code>array([[5749,    4,   22,   11,   11,   40,   36,   11,   36,    3],
       [   2, 6490,   43,   24,    6,   41,    8,   12,  107,    9],
       [  53,   42, 5330,   99,   87,   24,   89,   58,  159,   17],
       [  46,   41,  126, 5361,    1,  241,   34,   59,  129,   93],
       [  20,   30,   35,   10, 5369,    8,   48,   38,   76,  208],
       [  73,   45,   30,  194,   64, 4614,  106,   30,  170,   95],
       [  41,   30,   46,    2,   44,   91, 5611,    9,   43,    1],
       [  26,   18,   73,   30,   52,   11,    4, 5823,   14,  214],
       [  63,  159,   69,  168,   15,  172,   54,   26, 4997,  128],
       [  39,   39,   27,   90,  177,   40,    2,  230,   78, 5227]])</code></pre><p>Digit 5 has the lower correct prediction 4,582, digit 1 has the highest correct prediction 6,493. Let’s look at the graphical representation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_confusion_matrix</span><span class="params">(matrix)</span>:</span></span><br><span class="line">    <span class="string">"""If you prefer color and a colorbar"""</span></span><br><span class="line">    <span class="comment"># Plot 8 by 8 matrix</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">    <span class="comment"># Color intensity comes from the matrix.</span></span><br><span class="line">    cax = ax.matshow(matrix)</span><br><span class="line">    fig.colorbar(cax)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Digit five is darker, digit 1 is ligher. </span></span><br><span class="line">plt.matshow(conf_mx, cmap=plt.cm.gray)</span><br><span class="line">plt.xlabel(<span class="string">"Predicted"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Actual"</span>)</span><br><span class="line">save_fig(<span class="string">"confusion_matrix_plot"</span>, tight_layout=<span class="literal">False</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_103_1.png" alt="png"></p>
<p>混淆矩阵看起来很不错，因为大多数图片都在主对角线上，这说明它们被正确分类。数字5看起来比其他数字稍稍暗一些，这可能意味着数据集中数字5的图片较少，也可能是分类器在数字5上的执行效果不如在其他数字上好。实际上，你可能会验证这两者都属实。 让我们把焦点放在错误上。首先，你需要将混淆矩阵中的每个值除以相应类别中的图片数量，这样你比较的就是错误率而不是错误的绝对值（后者对图片数量较多的类别不公平）：</p>
<p>Off-diagonal elements are the errors. We can look at most common misclassifications. From the matrix, we can see that the most common error is the confusion of digits 7 and 9 (236, 223).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">row_sums = conf_mx.sum(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">norm_conf_mx = conf_mx / row_sums</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We will no plot a diagal elements, otherwise their intensity would dominated the chart. </span></span><br><span class="line">np.fill_diagonal(norm_conf_mx, <span class="number">0</span>) <span class="comment">#用0填充对角线，只保留错误，重新绘制结果： </span></span><br><span class="line">plt.matshow(norm_conf_mx, cmap=plt.cm.gray)</span><br><span class="line">plt.xlabel(<span class="string">"Predicted"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Actual"</span>)</span><br><span class="line">save_fig(<span class="string">"confusion_matrix_errors_plot"</span>, tight_layout=<span class="literal">False</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_106_1.png" alt="png"></p>
<p>Largest error: symmetric confusion of 7/9, and 3/5. Other errors are asymmetric: the classification confuses 6 for 8, but not the 8 for 6. <br> </p>
<p>Solutions: <br><br>1 gather more training data for the confusing digits <br><br>2 setup closed loops that would run the algorithm until the desired error rate is achieved <br><br>3 Preprocess the data engineering new features <br><br>Analyzing individual errors can also be a good way to gain insights on what your classifier is doing and why it is failing, but it is more difficult and time-consuming. Let’s plot examples of 3s and 5s.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cl_a, cl_b = <span class="number">3</span>, <span class="number">5</span></span><br><span class="line">X_aa = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_a)]</span><br><span class="line">X_ab = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_b)]</span><br><span class="line">X_ba = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_a)]</span><br><span class="line">X_bb = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_b)]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">plt.subplot(<span class="number">221</span>); plot_digits(X_aa[:<span class="number">25</span>], images_per_row=<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">222</span>); plot_digits(X_ab[:<span class="number">25</span>], images_per_row=<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">223</span>); plot_digits(X_ba[:<span class="number">25</span>], images_per_row=<span class="number">5</span>)</span><br><span class="line">plt.subplot(<span class="number">224</span>); plot_digits(X_bb[:<span class="number">25</span>], images_per_row=<span class="number">5</span>)</span><br><span class="line">save_fig(<span class="string">"error_analysis_digits_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Saving figure error_analysis_digits_plot</code></pre><p><img src="output_108_1.png" alt="png"></p>
<p>Top-left are the correcly classified 3s, top-right are 3s misclassified for 5s. Bottom-left are 5s misclassified as 3s, bottom-right are the correctly classified 5s.  <br></p>
<p>Humans can easily recognize most of the errors. SGDClassifier assigns a weight to each pixel intensity, as 3s and 5s are differ by only few pixel it’s easy to confuse them by shifting and rotation of the small line that makes a difference between 3 and 5. </p>
<p>Another solution: center and rotate all digits in the same way.</p>
<h1 id="Multilabel-classification"><a href="#Multilabel-classification" class="headerlink" title="Multilabel classification"></a>Multilabel classification</h1><p>Mutlilabel classification marks each object relative to several classes. For example, if we are looking for three people in the picture Alice, Bob, and Charlie, then the classification would return  [1, 0, 1]  if it thinks that Alice and Charlie is in the picture, but not the Bob. Usually restriction on the multiple label help us to estimate this system. For example, if Bob and Charlie look alike we just need to find out two objects that resembles [Bob, Charlie], this is often easier that trying to distinguish similar objects. </p>
<p>We create two classes of digits large (&gt;6) and odd. We use the KNN (Kth nearest neighbor) , which is a weighted average of classes for the K nearest neighbors. </p>
<p>为了阐释清楚，这里不讨论面部识别，让我们来看一个更为简单的例子：</p>
<p>在某些情况下，你希望分类器为每个实例产出多个类别。下面的代码会创建一个y_multilabel数组，其中包含两个数字图片的目标标签：第一个表示数字是否是大数（7、8、9），第二个表示是否为奇数。下一行创建一个KNeighborsClassifier实例（它支持多标签分类，不是所有的分类器都支持），然后使用多个目标数组对它进行训练。现在用它做一个预测，注意它输出的两个标签：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">y_train_large = (y_train &gt;= <span class="number">7</span>)</span><br><span class="line">y_train_odd = (y_train % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line">y_multilabel = np.c_[y_train_large, y_train_odd]</span><br><span class="line"></span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line">knn_clf.fit(X_train, y_multilabel)</span><br></pre></td></tr></table></figure>

<pre><code>KNeighborsClassifier(algorithm=&apos;auto&apos;, leaf_size=30, metric=&apos;minkowski&apos;,
                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,
                     weights=&apos;uniform&apos;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn_clf.predict([some_digit])</span><br></pre></td></tr></table></figure>

<pre><code>OUT:array([[False,  True]])</code></pre><p>And it gets it right! The digit 5 is indeed not large (False) and odd (True).</p>
<p><strong>Warning</strong>: the following cell may take a very long time (possibly hours depending on your hardware). KNN can be a very slow algorithm. F1 can weight classes by importance. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=<span class="number">3</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">f1_score(y_multilabel, y_train_knn_pred, average=<span class="string">"macro"</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Multioutput-Classification"><a href="#Multioutput-Classification" class="headerlink" title="Multioutput Classification"></a>Multioutput Classification</h1><p>我们即将讨论的最后一种分类任务叫作多输出-多类别分类（或简单地称为多输出分类）。简单来说，它是多标签分类的泛化，其标签也可以是多种类别的（比如它可以有两个以上可能的值）。<br>     为了说明这一点，构建一个系统用来去除图片中的噪声。给它输入一张有噪声的图片，它将输出一张干净的数字图片，跟其他MNIST图片一样，以像素强度的一个数组作为呈现方式。请注意，这个分类器的输出是多个标签（一个像素点一个标签），每个标签可以有多个值（像素强度范围为0到225）。所以这是个多输出分类器系统的例子。</p>
<p>先从创建训练集和测试集开始，使用NumPy的randint（）函数为MNIST图片的像素强度增加噪声：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">noise = np.random.randint(0, 100, (len(X_train), 784))</span><br><span class="line">X_train_mod = X_train + noise</span><br><span class="line">noise = np.random.randint(0, 100, (len(X_test), 784))</span><br><span class="line">X_test_mod = X_test + noise</span><br><span class="line">y_train_mod = X_train</span><br><span class="line">y_test_mod = X_test</span><br><span class="line"> </span><br><span class="line">#观察一下其中一个数据</span><br><span class="line">some_index = 5500</span><br><span class="line">plt.subplot(121); plot_digit(X_test_mod[some_index])</span><br><span class="line">plt.subplot(122); plot_digit(y_test_mod[some_index])</span><br><span class="line">save_fig(&quot;noisy_digit_example_plot&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>![img](D:\HexoBlog\source_posts\Lecture 4 Classification\multioutput.png)</p>
<p>左边是有噪声的输入图片，右边是干净的目标图片。现在通过训练分类器，清洗这张图片</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">knn_clf.fit(X_train_mod, y_train_mod)</span><br><span class="line">clean_digit = knn_clf.predict([X_test_mod[some_index]])</span><br><span class="line">plot_digit(clean_digit)</span><br><span class="line">save_fig(&quot;cleaned_digit_example_plot&quot;)</span><br></pre></td></tr></table></figure>

<p>![img](D:\HexoBlog\source_posts\Lecture 4 Classification\multioutput_output.png)</p>
<h1 id="Extra-material"><a href="#Extra-material" class="headerlink" title="Extra material"></a>Extra material</h1><h2 id="Dummy-ie-random-classifier"><a href="#Dummy-ie-random-classifier" class="headerlink" title="Dummy (ie. random) classifier"></a>Dummy (ie. random) classifier</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.dummy <span class="keyword">import</span> DummyClassifier</span><br><span class="line">dmy_clf = DummyClassifier()</span><br><span class="line">y_probas_dmy = cross_val_predict(dmy_clf, X_train, y_train_5, cv=<span class="number">3</span>, method=<span class="string">"predict_proba"</span>)</span><br><span class="line">y_scores_dmy = y_probas_dmy[:, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fprr, tprr, thresholdsr = roc_curve(y_train_5, y_scores_dmy)</span><br><span class="line">plot_roc_curve(fprr, tprr)</span><br></pre></td></tr></table></figure>

<h2 id="KNN-classifier"><a href="#KNN-classifier" class="headerlink" title="KNN classifier"></a>KNN classifier</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">knn_clf = KNeighborsClassifier(n_jobs=<span class="number">-1</span>, weights=<span class="string">'distance'</span>, n_neighbors=<span class="number">4</span>)</span><br><span class="line">knn_clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_knn_pred = knn_clf.predict(X_test)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">accuracy_score(y_test, y_knn_pred)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.ndimage.interpolation <span class="keyword">import</span> shift</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shift_digit</span><span class="params">(digit_array, dx, dy, new=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> shift(digit_array.reshape(<span class="number">28</span>, <span class="number">28</span>), [dy, dx], cval=new).reshape(<span class="number">784</span>)</span><br><span class="line"></span><br><span class="line">plot_digit(shift_digit(some_digit, <span class="number">5</span>, <span class="number">1</span>, new=<span class="number">100</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_train_expanded = [X_train]</span><br><span class="line">y_train_expanded = [y_train]</span><br><span class="line"><span class="keyword">for</span> dx, dy <span class="keyword">in</span> ((<span class="number">1</span>, <span class="number">0</span>), (<span class="number">-1</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">-1</span>)):</span><br><span class="line">    shifted_images = np.apply_along_axis(shift_digit, axis=<span class="number">1</span>, arr=X_train, dx=dx, dy=dy)</span><br><span class="line">    X_train_expanded.append(shifted_images)</span><br><span class="line">    y_train_expanded.append(y_train)</span><br><span class="line"></span><br><span class="line">X_train_expanded = np.concatenate(X_train_expanded)</span><br><span class="line">y_train_expanded = np.concatenate(y_train_expanded)</span><br><span class="line">X_train_expanded.shape, y_train_expanded.shape</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn_clf.fit(X_train_expanded, y_train_expanded)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_knn_expanded_pred = knn_clf.predict(X_test)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy_score(y_test, y_knn_expanded_pred)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ambiguous_digit = X_test[<span class="number">2589</span>]</span><br><span class="line">knn_clf.predict_proba([ambiguous_digit])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_digit(ambiguous_digit)</span><br></pre></td></tr></table></figure>

<h1 id="Exercise-solutions"><a href="#Exercise-solutions" class="headerlink" title="Exercise solutions"></a>Exercise solutions</h1><h2 id="1-An-MNIST-Classifier-With-Over-97-Accuracy"><a href="#1-An-MNIST-Classifier-With-Over-97-Accuracy" class="headerlink" title="1. An MNIST Classifier With Over 97% Accuracy"></a>1. An MNIST Classifier With Over 97% Accuracy</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">param_grid = [&#123;<span class="string">'weights'</span>: [<span class="string">"uniform"</span>, <span class="string">"distance"</span>], <span class="string">'n_neighbors'</span>: [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]&#125;]</span><br><span class="line"></span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line">grid_search = GridSearchCV(knn_clf, param_grid, cv=<span class="number">5</span>, verbose=<span class="number">3</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">grid_search.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid_search.best_params_</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grid_search.best_score_</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">y_pred = grid_search.predict(X_test)</span><br><span class="line">accuracy_score(y_test, y_pred)</span><br></pre></td></tr></table></figure>

<h2 id="2-Data-Augmentation"><a href="#2-Data-Augmentation" class="headerlink" title="2. Data Augmentation"></a>2. Data Augmentation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.ndimage.interpolation <span class="keyword">import</span> shift</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shift_image</span><span class="params">(image, dx, dy)</span>:</span></span><br><span class="line">    image = image.reshape((<span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">    shifted_image = shift(image, [dy, dx], cval=<span class="number">0</span>, mode=<span class="string">"constant"</span>)</span><br><span class="line">    <span class="keyword">return</span> shifted_image.reshape([<span class="number">-1</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">image = X_train[<span class="number">1000</span>]</span><br><span class="line">shifted_image_down = shift_image(image, <span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">shifted_image_left = shift_image(image, <span class="number">-5</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">3</span>))</span><br><span class="line">plt.subplot(<span class="number">131</span>)</span><br><span class="line">plt.title(<span class="string">"Original"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.imshow(image.reshape(<span class="number">28</span>, <span class="number">28</span>), interpolation=<span class="string">"nearest"</span>, cmap=<span class="string">"Greys"</span>)</span><br><span class="line">plt.subplot(<span class="number">132</span>)</span><br><span class="line">plt.title(<span class="string">"Shifted down"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.imshow(shifted_image_down.reshape(<span class="number">28</span>, <span class="number">28</span>), interpolation=<span class="string">"nearest"</span>, cmap=<span class="string">"Greys"</span>)</span><br><span class="line">plt.subplot(<span class="number">133</span>)</span><br><span class="line">plt.title(<span class="string">"Shifted left"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.imshow(shifted_image_left.reshape(<span class="number">28</span>, <span class="number">28</span>), interpolation=<span class="string">"nearest"</span>, cmap=<span class="string">"Greys"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_train_augmented = [image <span class="keyword">for</span> image <span class="keyword">in</span> X_train]</span><br><span class="line">y_train_augmented = [label <span class="keyword">for</span> label <span class="keyword">in</span> y_train]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> dx, dy <span class="keyword">in</span> ((<span class="number">1</span>, <span class="number">0</span>), (<span class="number">-1</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">-1</span>)):</span><br><span class="line">    <span class="keyword">for</span> image, label <span class="keyword">in</span> zip(X_train, y_train):</span><br><span class="line">        X_train_augmented.append(shift_image(image, dx, dy))</span><br><span class="line">        y_train_augmented.append(label)</span><br><span class="line"></span><br><span class="line">X_train_augmented = np.array(X_train_augmented)</span><br><span class="line">y_train_augmented = np.array(y_train_augmented)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">shuffle_idx = np.random.permutation(len(X_train_augmented))</span><br><span class="line">X_train_augmented = X_train_augmented[shuffle_idx]</span><br><span class="line">y_train_augmented = y_train_augmented[shuffle_idx]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn_clf = KNeighborsClassifier(**grid_search.best_params_)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn_clf.fit(X_train_augmented, y_train_augmented)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_pred = knn_clf.predict(X_test)</span><br><span class="line">accuracy_score(y_test, y_pred)</span><br></pre></td></tr></table></figure>

<p>By simply augmenting the data, we got a 0.5% accuracy boost. :)</p>
<h2 id="3-Tackle-the-Titanic-dataset"><a href="#3-Tackle-the-Titanic-dataset" class="headerlink" title="3. Tackle the Titanic dataset"></a>3. Tackle the Titanic dataset</h2><p>The goal is to predict whether or not a passenger survived based on attributes such as their age, sex, passenger class, where they embarked and so on.</p>
<p>First, login to <a href="https://www.kaggle.com/" target="_blank" rel="noopener">Kaggle</a> and go to the <a href="https://www.kaggle.com/c/titanic" target="_blank" rel="noopener">Titanic challenge</a> to download <code>train.csv</code> and <code>test.csv</code>. Save them to the <code>datasets/titanic</code> directory.</p>
<p>Next, let’s load the data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">TITANIC_PATH = os.path.join(<span class="string">"datasets"</span>, <span class="string">"titanic"</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data = load_titanic_data(<span class="string">"train.csv"</span>)</span><br><span class="line">test_data = load_titanic_data(<span class="string">"test.csv"</span>)</span><br></pre></td></tr></table></figure>

<p>The data is already split into a training set and a test set. However, the test data does <em>not</em> contain the labels: your goal is to train the best model you can using the training data, then make your predictions on the test data and upload them to Kaggle to see your final score.</p>
<p>Let’s take a peek at the top few rows of the training set:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.head()</span><br></pre></td></tr></table></figure>

<p>The attributes have the following meaning:</p>
<ul>
<li><strong>Survived</strong>: that’s the target, 0 means the passenger did not survive, while 1 means he/she survived.</li>
<li><strong>Pclass</strong>: passenger class.</li>
<li><strong>Name</strong>, <strong>Sex</strong>, <strong>Age</strong>: self-explanatory</li>
<li><strong>SibSp</strong>: how many siblings &amp; spouses of the passenger aboard the Titanic.</li>
<li><strong>Parch</strong>: how many children &amp; parents of the passenger aboard the Titanic.</li>
<li><strong>Ticket</strong>: ticket id</li>
<li><strong>Fare</strong>: price paid (in pounds)</li>
<li><strong>Cabin</strong>: passenger’s cabin number</li>
<li><strong>Embarked</strong>: where the passenger embarked the Titanic</li>
</ul>
<p>Let’s get more info to see how much data is missing:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.info()</span><br></pre></td></tr></table></figure>

<p>Okay, the <strong>Age</strong>, <strong>Cabin</strong> and <strong>Embarked</strong> attributes are sometimes null (less than 891 non-null), especially the <strong>Cabin</strong> (77% are null). We will ignore the <strong>Cabin</strong> for now and focus on the rest. The <strong>Age</strong> attribute has about 19% null values, so we will need to decide what to do with them. Replacing null values with the median age seems reasonable.</p>
<p>The <strong>Name</strong> and <strong>Ticket</strong> attributes may have some value, but they will be a bit tricky to convert into useful numbers that a model can consume. So for now, we will ignore them.</p>
<p>Let’s take a look at the numerical attributes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.describe()</span><br></pre></td></tr></table></figure>

<ul>
<li>Yikes, only 38% <strong>Survived</strong>. :(  That’s close enough to 40%, so accuracy will be a reasonable metric to evaluate our model.</li>
<li>The mean <strong>Fare</strong> was £32.20, which does not seem so expensive (but it was probably a lot of money back then).</li>
<li>The mean <strong>Age</strong> was less than 30 years old.</li>
</ul>
<p>Let’s check that the target is indeed 0 or 1:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"Survived"</span>].value_counts()</span><br></pre></td></tr></table></figure>

<p>Now let’s take a quick look at all the categorical attributes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"Pclass"</span>].value_counts()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"Sex"</span>].value_counts()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"Embarked"</span>].value_counts()</span><br></pre></td></tr></table></figure>

<p>The Embarked attribute tells us where the passenger embarked: C=Cherbourg, Q=Queenstown, S=Southampton.</p>
<p>Now let’s build our preprocessing pipelines. We will reuse the <code>DataframeSelector</code> we built in the previous chapter to select specific attributes from the <code>DataFrame</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"></span><br><span class="line"><span class="comment"># A class to select numerical or categorical columns </span></span><br><span class="line"><span class="comment"># since Scikit-Learn doesn't handle DataFrames yet</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataFrameSelector</span><span class="params">(BaseEstimator, TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, attribute_names)</span>:</span></span><br><span class="line">        self.attribute_names = attribute_names</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> X[self.attribute_names]</span><br></pre></td></tr></table></figure>

<p>Let’s build the pipeline for the numerical attributes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line"></span><br><span class="line">imputer = Imputer(strategy=<span class="string">"median"</span>)</span><br><span class="line"></span><br><span class="line">num_pipeline = Pipeline([</span><br><span class="line">        (<span class="string">"select_numeric"</span>, DataFrameSelector([<span class="string">"Age"</span>, <span class="string">"SibSp"</span>, <span class="string">"Parch"</span>, <span class="string">"Fare"</span>])),</span><br><span class="line">        (<span class="string">"imputer"</span>, Imputer(strategy=<span class="string">"median"</span>)),</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_pipeline.fit_transform(train_data)</span><br></pre></td></tr></table></figure>

<p>We will also need an imputer for the string categorical columns (the regular <code>Imputer</code> does not work on those):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inspired from stackoverflow.com/questions/25239958</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MostFrequentImputer</span><span class="params">(BaseEstimator, TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        self.most_frequent_ = pd.Series([X[c].value_counts().index[<span class="number">0</span>] <span class="keyword">for</span> c <span class="keyword">in</span> X],</span><br><span class="line">                                        index=X.columns)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> X.fillna(self.most_frequent_)</span><br></pre></td></tr></table></figure>

<p>We can convert each categorical value to a one-hot vector using a <code>OneHotEncoder</code>. Right now this class can only handle integer categorical inputs, but in Scikit-Learn 0.20 it will also handle string categorical inputs (see <a href="https://github.com/scikit-learn/scikit-learn/issues/10521" target="_blank" rel="noopener">PR #10521</a>). So for now we import it from <code>future_encoders.py</code>, but when Scikit-Learn 0.20 is released, you can import it from <code>sklearn.preprocessing</code> instead:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> future_encoders <span class="keyword">import</span> OneHotEncoder</span><br></pre></td></tr></table></figure>

<p>Now we can build the pipeline for the categorical attributes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat_pipeline = Pipeline([</span><br><span class="line">        (<span class="string">"select_cat"</span>, DataFrameSelector([<span class="string">"Pclass"</span>, <span class="string">"Sex"</span>, <span class="string">"Embarked"</span>])),</span><br><span class="line">        (<span class="string">"imputer"</span>, MostFrequentImputer()),</span><br><span class="line">        (<span class="string">"cat_encoder"</span>, OneHotEncoder(sparse=<span class="literal">False</span>)),</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat_pipeline.fit_transform(train_data)</span><br></pre></td></tr></table></figure>

<p>Finally, let’s join the numerical and categorical pipelines:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> FeatureUnion</span><br><span class="line">preprocess_pipeline = FeatureUnion(transformer_list=[</span><br><span class="line">        (<span class="string">"num_pipeline"</span>, num_pipeline),</span><br><span class="line">        (<span class="string">"cat_pipeline"</span>, cat_pipeline),</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>

<p>Cool! Now we have a nice preprocessing pipeline that takes the raw data and outputs numerical input features that we can feed to any Machine Learning model we want.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train = preprocess_pipeline.fit_transform(train_data)</span><br><span class="line">X_train</span><br></pre></td></tr></table></figure>

<p>Let’s not forget to get the labels:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train = train_data[<span class="string">"Survived"</span>]</span><br></pre></td></tr></table></figure>

<p>We are now ready to train a classifier. Let’s start with an <code>SVC</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">svm_clf = SVC()</span><br><span class="line">svm_clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<p>Great, our model is trained, let’s use it to make predictions on the test set:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_test = preprocess_pipeline.transform(test_data)</span><br><span class="line">y_pred = svm_clf.predict(X_test)</span><br></pre></td></tr></table></figure>

<p>And now we could just build a CSV file with these predictions (respecting the format excepted by Kaggle), then upload it and hope for the best. But wait! We can do better than hope. Why don’t we use cross-validation to have an idea of how good our model is?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line">svm_scores = cross_val_score(svm_clf, X_train, y_train, cv=<span class="number">10</span>)</span><br><span class="line">svm_scores.mean()</span><br></pre></td></tr></table></figure>

<p>Okay, over 73% accuracy, clearly better than random chance, but it’s not a great score. Looking at the <a href="https://www.kaggle.com/c/titanic/leaderboard" target="_blank" rel="noopener">leaderboard</a> for the Titanic competition on Kaggle, you can see that you need to reach above 80% accuracy to be within the top 10% Kagglers. Some reached 100%, but since you can easily find the <a href="https://www.encyclopedia-titanica.org/titanic-victims/" target="_blank" rel="noopener">list of victims</a> of the Titanic, it seems likely that there was little Machine Learning involved in their performance! ;-) So let’s try to build a model that reaches 80% accuracy.</p>
<p>Let’s try a <code>RandomForestClassifier</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line">forest_clf = RandomForestClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=<span class="number">10</span>)</span><br><span class="line">forest_scores.mean()</span><br></pre></td></tr></table></figure>

<p>That’s much better!</p>
<p>Instead of just looking at the mean accuracy across the 10 cross-validation folds, let’s plot all 10 scores for each model, along with a box plot highlighting the lower and upper quartiles, and “whiskers” showing the extent of the scores (thanks to Nevin Yilmaz for suggesting this visualization). Note that the <code>boxplot()</code> function detects outliers (called “fliers”) and does not include them within the whiskers. Specifically, if the lower quartile is $Q_1$ and the upper quartile is $Q_3$, then the interquartile range $IQR = Q_3 - Q_1$ (this is the box’s height), and any score lower than $Q_1 - 1.5 \times IQR$ is a flier, and so is any score greater than $Q3 + 1.5 \times IQR$.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot([<span class="number">1</span>]*<span class="number">10</span>, svm_scores, <span class="string">"."</span>)</span><br><span class="line">plt.plot([<span class="number">2</span>]*<span class="number">10</span>, forest_scores, <span class="string">"."</span>)</span><br><span class="line">plt.boxplot([svm_scores, forest_scores], labels=(<span class="string">"SVM"</span>,<span class="string">"Random Forest"</span>))</span><br><span class="line">plt.ylabel(<span class="string">"Accuracy"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>To improve this result further, you could:</p>
<ul>
<li>Compare many more models and tune hyperparameters using cross validation and grid search,</li>
<li>Do more feature engineering, for example:<ul>
<li>replace <strong>SibSp</strong> and <strong>Parch</strong> with their sum,</li>
<li>try to identify parts of names that correlate well with the <strong>Survived</strong> attribute (e.g. if the name contains “Countess”, then survival seems more likely),</li>
</ul>
</li>
<li>try to convert numerical attributes to categorical attributes: for example, different age groups had very different survival rates (see below), so it may help to create an age bucket category and use it instead of the age. Similarly, it may be useful to have a special category for people traveling alone since only 30% of them survived (see below).</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"AgeBucket"</span>] = train_data[<span class="string">"Age"</span>] // <span class="number">15</span> * <span class="number">15</span></span><br><span class="line">train_data[[<span class="string">"AgeBucket"</span>, <span class="string">"Survived"</span>]].groupby([<span class="string">'AgeBucket'</span>]).mean()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">"RelativesOnboard"</span>] = train_data[<span class="string">"SibSp"</span>] + train_data[<span class="string">"Parch"</span>]</span><br><span class="line">train_data[[<span class="string">"RelativesOnboard"</span>, <span class="string">"Survived"</span>]].groupby([<span class="string">'RelativesOnboard'</span>]).mean()</span><br></pre></td></tr></table></figure>

<h2 id="4-Spam-classifier"><a href="#4-Spam-classifier" class="headerlink" title="4. Spam classifier"></a>4. Spam classifier</h2><p>First, let’s fetch the data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tarfile</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> urllib</span><br><span class="line"></span><br><span class="line">DOWNLOAD_ROOT = <span class="string">"http://spamassassin.apache.org/old/publiccorpus/"</span></span><br><span class="line">HAM_URL = DOWNLOAD_ROOT + <span class="string">"20030228_easy_ham.tar.bz2"</span></span><br><span class="line">SPAM_URL = DOWNLOAD_ROOT + <span class="string">"20030228_spam.tar.bz2"</span></span><br><span class="line">SPAM_PATH = os.path.join(<span class="string">"datasets"</span>, <span class="string">"spam"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_spam_data</span><span class="params">(spam_url=SPAM_URL, spam_path=SPAM_PATH)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(spam_path):</span><br><span class="line">        os.makedirs(spam_path)</span><br><span class="line">    <span class="keyword">for</span> filename, url <span class="keyword">in</span> ((<span class="string">"ham.tar.bz2"</span>, HAM_URL), (<span class="string">"spam.tar.bz2"</span>, SPAM_URL)):</span><br><span class="line">        path = os.path.join(spam_path, filename)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(path):</span><br><span class="line">            urllib.request.urlretrieve(url, path)</span><br><span class="line">        tar_bz2_file = tarfile.open(path)</span><br><span class="line">        tar_bz2_file.extractall(path=SPAM_PATH)</span><br><span class="line">        tar_bz2_file.close()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fetch_spam_data()</span><br></pre></td></tr></table></figure>

<p>Next, let’s load all the emails:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HAM_DIR = os.path.join(SPAM_PATH, <span class="string">"easy_ham"</span>)</span><br><span class="line">SPAM_DIR = os.path.join(SPAM_PATH, <span class="string">"spam"</span>)</span><br><span class="line">ham_filenames = [name <span class="keyword">for</span> name <span class="keyword">in</span> sorted(os.listdir(HAM_DIR)) <span class="keyword">if</span> len(name) &gt; <span class="number">20</span>]</span><br><span class="line">spam_filenames = [name <span class="keyword">for</span> name <span class="keyword">in</span> sorted(os.listdir(SPAM_DIR)) <span class="keyword">if</span> len(name) &gt; <span class="number">20</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(ham_filenames)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(spam_filenames)</span><br></pre></td></tr></table></figure>

<p>We can use Python’s <code>email</code> module to parse these emails (this handles headers, encoding, and so on):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> email</span><br><span class="line"><span class="keyword">import</span> email.policy</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_email</span><span class="params">(is_spam, filename, spam_path=SPAM_PATH)</span>:</span></span><br><span class="line">    directory = <span class="string">"spam"</span> <span class="keyword">if</span> is_spam <span class="keyword">else</span> <span class="string">"easy_ham"</span></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(spam_path, directory, filename), <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> email.parser.BytesParser(policy=email.policy.default).parse(f)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ham_emails = [load_email(is_spam=<span class="literal">False</span>, filename=name) <span class="keyword">for</span> name <span class="keyword">in</span> ham_filenames]</span><br><span class="line">spam_emails = [load_email(is_spam=<span class="literal">True</span>, filename=name) <span class="keyword">for</span> name <span class="keyword">in</span> spam_filenames]</span><br></pre></td></tr></table></figure>

<p>Let’s look at one example of ham and one example of spam, to get a feel of what the data looks like:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(ham_emails[<span class="number">1</span>].get_content().strip())</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(spam_emails[<span class="number">6</span>].get_content().strip())</span><br></pre></td></tr></table></figure>

<p>Some emails are actually multipart, with images and attachments (which can have their own attachments). Let’s look at the various types of structures we have:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_email_structure</span><span class="params">(email)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(email, str):</span><br><span class="line">        <span class="keyword">return</span> email</span><br><span class="line">    payload = email.get_payload()</span><br><span class="line">    <span class="keyword">if</span> isinstance(payload, list):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"multipart(&#123;&#125;)"</span>.format(<span class="string">", "</span>.join([</span><br><span class="line">            get_email_structure(sub_email)</span><br><span class="line">            <span class="keyword">for</span> sub_email <span class="keyword">in</span> payload</span><br><span class="line">        ]))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> email.get_content_type()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">structures_counter</span><span class="params">(emails)</span>:</span></span><br><span class="line">    structures = Counter()</span><br><span class="line">    <span class="keyword">for</span> email <span class="keyword">in</span> emails:</span><br><span class="line">        structure = get_email_structure(email)</span><br><span class="line">        structures[structure] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> structures</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">structures_counter(ham_emails).most_common()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">structures_counter(spam_emails).most_common()</span><br></pre></td></tr></table></figure>

<p>It seems that the ham emails are more often plain text, while spam has quite a lot of HTML. Moreover, quite a few ham emails are signed using PGP, while no spam is. In short, it seems that the email structure is useful information to have.</p>
<p>Now let’s take a look at the email headers:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> header, value <span class="keyword">in</span> spam_emails[<span class="number">0</span>].items():</span><br><span class="line">    print(header,<span class="string">":"</span>,value)</span><br></pre></td></tr></table></figure>

<p>There’s probably a lot of useful information in there, such as the sender’s email address (<a href="mailto:12a1mailbot1@web.de" target="_blank" rel="noopener">12a1mailbot1@web.de</a> looks fishy), but we will just focus on the <code>Subject</code> header:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spam_emails[<span class="number">0</span>][<span class="string">"Subject"</span>]</span><br></pre></td></tr></table></figure>

<p>Okay, before we learn too much about the data, let’s not forget to split it into a training set and a test set:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X = np.array(ham_emails + spam_emails)</span><br><span class="line">y = np.array([<span class="number">0</span>] * len(ham_emails) + [<span class="number">1</span>] * len(spam_emails))</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<p>Okay, let’s start writing the preprocessing functions. First, we will need a function to convert HTML to plain text. Arguably the best way to do this would be to use the great <a href="https://www.crummy.com/software/BeautifulSoup/" target="_blank" rel="noopener">BeautifulSoup</a> library, but I would like to avoid adding another dependency to this project, so let’s hack a quick &amp; dirty solution using regular expressions (at the risk of <a href="https://stackoverflow.com/a/1732454/38626" target="_blank" rel="noopener">un̨ho͞ly radiańcé destro҉ying all enli̍̈́̂̈́ghtenment</a>). The following function first drops the <code>&lt;head&gt;</code> section, then converts all <code>&lt;a&gt;</code> tags to the word HYPERLINK, then it gets rid of all HTML tags, leaving only the plain text. For readability, it also replaces multiple newlines with single newlines, and finally it unescapes html entities (such as <code>&amp;gt;</code> or <code>&amp;nbsp;</code>):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> html <span class="keyword">import</span> unescape</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">html_to_plain_text</span><span class="params">(html)</span>:</span></span><br><span class="line">    text = re.sub(<span class="string">'&lt;head.*?&gt;.*?&lt;/head&gt;'</span>, <span class="string">''</span>, html, flags=re.M | re.S | re.I)</span><br><span class="line">    text = re.sub(<span class="string">'&lt;a\s.*?&gt;'</span>, <span class="string">' HYPERLINK '</span>, text, flags=re.M | re.S | re.I)</span><br><span class="line">    text = re.sub(<span class="string">'&lt;.*?&gt;'</span>, <span class="string">''</span>, text, flags=re.M | re.S)</span><br><span class="line">    text = re.sub(<span class="string">r'(\s*\n)+'</span>, <span class="string">'\n'</span>, text, flags=re.M | re.S)</span><br><span class="line">    <span class="keyword">return</span> unescape(text)</span><br></pre></td></tr></table></figure>

<p>Let’s see if it works. This is HTML spam:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">html_spam_emails = [email <span class="keyword">for</span> email <span class="keyword">in</span> X_train[y_train==<span class="number">1</span>]</span><br><span class="line">                    <span class="keyword">if</span> get_email_structure(email) == <span class="string">"text/html"</span>]</span><br><span class="line">sample_html_spam = html_spam_emails[<span class="number">7</span>]</span><br><span class="line">print(sample_html_spam.get_content().strip()[:<span class="number">1000</span>], <span class="string">"..."</span>)</span><br></pre></td></tr></table></figure>

<p>And this is the resulting plain text:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(html_to_plain_text(sample_html_spam.get_content())[:<span class="number">1000</span>], <span class="string">"..."</span>)</span><br></pre></td></tr></table></figure>

<p>Great! Now let’s write a function that takes an email as input and returns its content as plain text, whatever its format is:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">email_to_text</span><span class="params">(email)</span>:</span></span><br><span class="line">    html = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> part <span class="keyword">in</span> email.walk():</span><br><span class="line">        ctype = part.get_content_type()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> ctype <span class="keyword">in</span> (<span class="string">"text/plain"</span>, <span class="string">"text/html"</span>):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            content = part.get_content()</span><br><span class="line">        <span class="keyword">except</span>: <span class="comment"># in case of encoding issues</span></span><br><span class="line">            content = str(part.get_payload())</span><br><span class="line">        <span class="keyword">if</span> ctype == <span class="string">"text/plain"</span>:</span><br><span class="line">            <span class="keyword">return</span> content</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            html = content</span><br><span class="line">    <span class="keyword">if</span> html:</span><br><span class="line">        <span class="keyword">return</span> html_to_plain_text(html)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(email_to_text(sample_html_spam)[:<span class="number">100</span>], <span class="string">"..."</span>)</span><br></pre></td></tr></table></figure>

<p>Let’s throw in some stemming! For this to work, you need to install the Natural Language Toolkit (<a href="http://www.nltk.org/" target="_blank" rel="noopener">NLTK</a>). It’s as simple as running the following command (don’t forget to activate your virtualenv first; if you don’t have one, you will likely need administrator rights, or use the <code>--user</code> option):</p>
<p><code>$ pip3 install nltk</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> nltk</span><br><span class="line"></span><br><span class="line">    stemmer = nltk.PorterStemmer()</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> (<span class="string">"Computations"</span>, <span class="string">"Computation"</span>, <span class="string">"Computing"</span>, <span class="string">"Computed"</span>, <span class="string">"Compute"</span>, <span class="string">"Compulsive"</span>):</span><br><span class="line">        print(word, <span class="string">"=&gt;"</span>, stemmer.stem(word))</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    print(<span class="string">"Error: stemming requires the NLTK module."</span>)</span><br><span class="line">    stemmer = <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<p>We will also need a way to replace URLs with the word “URL”. For this, we could use hard core <a href="https://mathiasbynens.be/demo/url-regex" target="_blank" rel="noopener">regular expressions</a> but we will just use the <a href="https://github.com/lipoja/URLExtract" target="_blank" rel="noopener">urlextract</a> library. You can install it with the following command (don’t forget to activate your virtualenv first; if you don’t have one, you will likely need administrator rights, or use the <code>--user</code> option):</p>
<p><code>$ pip3 install urlextract</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> urlextract <span class="comment"># may require an Internet connection to download root domain names</span></span><br><span class="line">    </span><br><span class="line">    url_extractor = urlextract.URLExtract()</span><br><span class="line">    print(url_extractor.find_urls(<span class="string">"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s"</span>))</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    print(<span class="string">"Error: replacing URLs requires the urlextract module."</span>)</span><br><span class="line">    url_extractor = <span class="literal">None</span></span><br></pre></td></tr></table></figure>

<p>We are ready to put all this together into a transformer that we will use to convert emails to word counters. Note that we split sentences into words using Python’s <code>split()</code> method, which uses whitespaces for word boundaries. This works for many written languages, but not all. For example, Chinese and Japanese scripts generally don’t use spaces between words, and Vietnamese often uses spaces even between syllables. It’s okay in this exercise, because the dataset is (mostly) in English.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmailToWordCounterTransformer</span><span class="params">(BaseEstimator, TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, strip_headers=True, lower_case=True, remove_punctuation=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                 replace_urls=True, replace_numbers=True, stemming=True)</span>:</span></span><br><span class="line">        self.strip_headers = strip_headers</span><br><span class="line">        self.lower_case = lower_case</span><br><span class="line">        self.remove_punctuation = remove_punctuation</span><br><span class="line">        self.replace_urls = replace_urls</span><br><span class="line">        self.replace_numbers = replace_numbers</span><br><span class="line">        self.stemming = stemming</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        X_transformed = []</span><br><span class="line">        <span class="keyword">for</span> email <span class="keyword">in</span> X:</span><br><span class="line">            text = email_to_text(email) <span class="keyword">or</span> <span class="string">""</span></span><br><span class="line">            <span class="keyword">if</span> self.lower_case:</span><br><span class="line">                text = text.lower()</span><br><span class="line">            <span class="keyword">if</span> self.replace_urls <span class="keyword">and</span> url_extractor <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                urls = list(set(url_extractor.find_urls(text)))</span><br><span class="line">                urls.sort(key=<span class="keyword">lambda</span> url: len(url), reverse=<span class="literal">True</span>)</span><br><span class="line">                <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">                    text = text.replace(url, <span class="string">" URL "</span>)</span><br><span class="line">            <span class="keyword">if</span> self.replace_numbers:</span><br><span class="line">                text = re.sub(<span class="string">r'\d+(?:\.\d*(?:[eE]\d+))?'</span>, <span class="string">'NUMBER'</span>, text)</span><br><span class="line">            <span class="keyword">if</span> self.remove_punctuation:</span><br><span class="line">                text = re.sub(<span class="string">r'\W+'</span>, <span class="string">' '</span>, text, flags=re.M)</span><br><span class="line">            word_counts = Counter(text.split())</span><br><span class="line">            <span class="keyword">if</span> self.stemming <span class="keyword">and</span> stemmer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                stemmed_word_counts = Counter()</span><br><span class="line">                <span class="keyword">for</span> word, count <span class="keyword">in</span> word_counts.items():</span><br><span class="line">                    stemmed_word = stemmer.stem(word)</span><br><span class="line">                    stemmed_word_counts[stemmed_word] += count</span><br><span class="line">                word_counts = stemmed_word_counts</span><br><span class="line">            X_transformed.append(word_counts)</span><br><span class="line">        <span class="keyword">return</span> np.array(X_transformed)</span><br></pre></td></tr></table></figure>

<p>Let’s try this transformer on a few emails:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_few = X_train[:<span class="number">3</span>]</span><br><span class="line">X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)</span><br><span class="line">X_few_wordcounts</span><br></pre></td></tr></table></figure>

<p>This looks about right!</p>
<p>Now we have the word counts, and we need to convert them to vectors. For this, we will build another transformer whose <code>fit()</code> method will build the vocabulary (an ordered list of the most common words) and whose <code>transform()</code> method will use the vocabulary to convert word counts to vectors. The output is a sparse matrix.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> csr_matrix</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordCounterToVectorTransformer</span><span class="params">(BaseEstimator, TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocabulary_size=<span class="number">1000</span>)</span>:</span></span><br><span class="line">        self.vocabulary_size = vocabulary_size</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        total_count = Counter()</span><br><span class="line">        <span class="keyword">for</span> word_count <span class="keyword">in</span> X:</span><br><span class="line">            <span class="keyword">for</span> word, count <span class="keyword">in</span> word_count.items():</span><br><span class="line">                total_count[word] += min(count, <span class="number">10</span>)</span><br><span class="line">        most_common = total_count.most_common()[:self.vocabulary_size]</span><br><span class="line">        self.most_common_ = most_common</span><br><span class="line">        self.vocabulary_ = &#123;word: index + <span class="number">1</span> <span class="keyword">for</span> index, (word, count) <span class="keyword">in</span> enumerate(most_common)&#125;</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        rows = []</span><br><span class="line">        cols = []</span><br><span class="line">        data = []</span><br><span class="line">        <span class="keyword">for</span> row, word_count <span class="keyword">in</span> enumerate(X):</span><br><span class="line">            <span class="keyword">for</span> word, count <span class="keyword">in</span> word_count.items():</span><br><span class="line">                rows.append(row)</span><br><span class="line">                cols.append(self.vocabulary_.get(word, <span class="number">0</span>))</span><br><span class="line">                data.append(count)</span><br><span class="line">        <span class="keyword">return</span> csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=<span class="number">10</span>)</span><br><span class="line">X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)</span><br><span class="line">X_few_vectors</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_few_vectors.toarray()</span><br></pre></td></tr></table></figure>

<p>What does this matrix mean? Well, the 64 in the third row, first column, means that the third email contains 64 words that are not part of the vocabulary. The 1 next to it means that the first word in the vocabulary is present once in this email. The 2 next to it means that the second word is present twice, and so on. You can look at the vocabulary to know which words we are talking about. The first word is “of”, the second word is “and”, etc.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vocab_transformer.vocabulary_</span><br></pre></td></tr></table></figure>

<p>We are now ready to train our first spam classifier! Let’s transform the whole dataset:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line">preprocess_pipeline = Pipeline([</span><br><span class="line">    (<span class="string">"email_to_wordcount"</span>, EmailToWordCounterTransformer()),</span><br><span class="line">    (<span class="string">"wordcount_to_vector"</span>, WordCounterToVectorTransformer()),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">X_train_transformed = preprocess_pipeline.fit_transform(X_train)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line">log_clf = LogisticRegression(random_state=<span class="number">42</span>)</span><br><span class="line">score = cross_val_score(log_clf, X_train_transformed, y_train, cv=<span class="number">3</span>, verbose=<span class="number">3</span>)</span><br><span class="line">score.mean()</span><br></pre></td></tr></table></figure>

<p>Over 98.7%, not bad for a first try! :) However, remember that we are using the “easy” dataset. You can try with the harder datasets, the results won’t be so amazing. You would have to try multiple models, select the best ones and fine-tune them using cross-validation, and so on.</p>
<p>But you get the picture, so let’s stop now, and just print out the precision/recall we get on the test set:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score</span><br><span class="line"></span><br><span class="line">X_test_transformed = preprocess_pipeline.transform(X_test)</span><br><span class="line"></span><br><span class="line">log_clf = LogisticRegression(random_state=<span class="number">42</span>)</span><br><span class="line">log_clf.fit(X_train_transformed, y_train)</span><br><span class="line"></span><br><span class="line">y_pred = log_clf.predict(X_test_transformed)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Precision: &#123;:.2f&#125;%"</span>.format(<span class="number">100</span> * precision_score(y_test, y_pred)))</span><br><span class="line">print(<span class="string">"Recall: &#123;:.2f&#125;%"</span>.format(<span class="number">100</span> * recall_score(y_test, y_pred)))</span><br></pre></td></tr></table></figure>


      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
            <a href="/tags/python/" rel="tag"># python</a>
          
            <a href="/tags/Sklearn/" rel="tag"># Sklearn</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/10/01/numpy/" rel="next" title="numpy常见命令与问题">
                <i class="fa fa-chevron-left"></i> numpy常见命令与问题
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/10/01/Lesson 5 Training/" rel="prev" title="Hands on Machine Learning-Chapter 4">
                Hands on Machine Learning-Chapter 4 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Xiaoyu Lu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Terrylxy" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Setup"><span class="nav-number">1.</span> <span class="nav-text">Setup</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MNIST"><span class="nav-number">2.</span> <span class="nav-text">MNIST</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Set-data"><span class="nav-number">2.1.</span> <span class="nav-text">Set data</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Training-a-Binary-classifier"><span class="nav-number">3.</span> <span class="nav-text">Training a Binary classifier</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Performance-Measure"><span class="nav-number">4.</span> <span class="nav-text">Performance Measure</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Measuring-Accuracy-Using-Cross-Validation"><span class="nav-number">4.1.</span> <span class="nav-text">Measuring Accuracy Using Cross-Validation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Confusion-Matrix"><span class="nav-number">4.2.</span> <span class="nav-text">Confusion Matrix</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Precision-amp-Recall"><span class="nav-number">4.3.</span> <span class="nav-text">Precision &amp; Recall</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Precision-Recall-Tradeoff"><span class="nav-number">4.4.</span> <span class="nav-text">Precision\Recall Tradeoff</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ROC-Curves"><span class="nav-number">4.5.</span> <span class="nav-text">ROC Curves</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Multiclass-classification"><span class="nav-number">5.</span> <span class="nav-text">Multiclass classification</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Error-Analysis"><span class="nav-number">6.</span> <span class="nav-text">Error Analysis</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Multilabel-classification"><span class="nav-number">7.</span> <span class="nav-text">Multilabel classification</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Multioutput-Classification"><span class="nav-number">8.</span> <span class="nav-text">Multioutput Classification</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Extra-material"><span class="nav-number">9.</span> <span class="nav-text">Extra material</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Dummy-ie-random-classifier"><span class="nav-number">9.1.</span> <span class="nav-text">Dummy (ie. random) classifier</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KNN-classifier"><span class="nav-number">9.2.</span> <span class="nav-text">KNN classifier</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Exercise-solutions"><span class="nav-number">10.</span> <span class="nav-text">Exercise solutions</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-An-MNIST-Classifier-With-Over-97-Accuracy"><span class="nav-number">10.1.</span> <span class="nav-text">1. An MNIST Classifier With Over 97% Accuracy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Data-Augmentation"><span class="nav-number">10.2.</span> <span class="nav-text">2. Data Augmentation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Tackle-the-Titanic-dataset"><span class="nav-number">10.3.</span> <span class="nav-text">3. Tackle the Titanic dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Spam-classifier"><span class="nav-number">10.4.</span> <span class="nav-text">4. Spam classifier</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xiaoyu Lu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>








        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  


  

  

</body>
</html>
