<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Machine Learning,python,Sklearn,">










<meta name="description" content="Training Linear ModelsBefore we used ML models without understanding how they work. Now we will try to look under the hood. We will look at two versions of Linear regression: regular closed-form equat">
<meta name="keywords" content="Machine Learning,python,Sklearn">
<meta property="og:type" content="article">
<meta property="og:title" content="Hands on Machine Learning-Chapter 4">
<meta property="og:url" content="http://yoursite.com/2019/10/01/Lesson 5 Training/index.html">
<meta property="og:site_name" content="LXY&#39;s Blog">
<meta property="og:description" content="Training Linear ModelsBefore we used ML models without understanding how they work. Now we will try to look under the hood. We will look at two versions of Linear regression: regular closed-form equat">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_14_0.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_22_0.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_46_0.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_53_0.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_58_1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_59_1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_73_0.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_74_0.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_81_0.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_86_0.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_88_0.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_91_0.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_93_0.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_95_0.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_97_0.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_98_0.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_99_0.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_104_1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_107_1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_111_1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_114_0.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_117_0.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_126_1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_128_1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_133_1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_136_1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_178_0.png">
<meta property="og:updated_time" content="2019-10-11T02:44:43.936Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hands on Machine Learning-Chapter 4">
<meta name="twitter:description" content="Training Linear ModelsBefore we used ML models without understanding how they work. Now we will try to look under the hood. We will look at two versions of Linear regression: regular closed-form equat">
<meta name="twitter:image" content="http://yoursite.com/2019/10/01/Lesson%205%20Training/output_14_0.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/10/01/Lesson 5 Training/">





  <title>Hands on Machine Learning-Chapter 4 | LXY's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LXY's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/01/Lesson 5 Training/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiaoyu Lu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LXY's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Hands on Machine Learning-Chapter 4</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-10-01T19:52:14-04:00">
                2019-10-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
                  , 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/Hands-on-Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Hands-on Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Training-Linear-Models"><a href="#Training-Linear-Models" class="headerlink" title="Training Linear Models"></a>Training Linear Models</h1><p>Before we used ML models without understanding how they work. Now we will try to look under the hood. We will look at two versions of Linear regression: regular closed-form equation $\beta = \frac{X’Y}{X’X}$ and iterative optimization Gradient Descent (GD) approach which tweaks model parameters to minimize cost function. Even though eventually GD will converge to the parameters $\beta$ estimated by the first method, GD is the important concepts that will help us to study neural networks. </p>
<p>Next we will look at the polynomial regression, which is more flexible than linear regression. It has more parameters and more prone to overfitting. Finally, we will look at regression models used in classification: Logistic Regression and Softmax Regression. </p>
<p>In Chapter 1, we looked at simple regression: <br> <br><br>$life_satisfaction = \theta_0 + \theta_1 \times GDP_percapita$ <br> <br><br>This is a linear function of the input feature of one input feature. The model parameters are $\theta_0$ and $\theta_1$. In general regression has the following form: <br> <br><br>$$\hat{y} = \theta_0 + \theta_1 x_1 +  \theta_2 x_2 + … +  \theta_n x_n$$</p>
<ul>
<li>$\hat{y}$ is the predicted value</li>
<li>$n$ is the number of features</li>
<li>$x_i$ is the $i^{th}$ feature</li>
<li>$\theta_j$ is the $j^{th}$ model parameters. Number of parameters is $n+1$ because of the constant</li>
</ul>
<a id="more"></a>


<p>We can write linear regression in the vector form:<br>$$\hat{y} = h_{\theta} (x) = \theta^T \cdot x$$</p>
<ul>
<li>$\theta^T$ is the transpose of vector of parameters or feature weights $\theta$.</li>
<li>$\theta^T \cdot x$ is the dot product of vector of parameters $\theta^T$ and vector of features $x$.</li>
<li>$h_{\theta} (x)$ is the hypothesis function or prediction using parameters $\theta$</li>
</ul>
<p>First we train linear regression on training data minimizing Mean Square Error (MSE). Usually we use Root Mean Square Error (RMSE) as a metric to compare OLS models, the same set of parameters minimize both MSE and RMSE.<br>$$MSE(X, h_\theta) = \frac{1}{m}\sum^m_{i=1} \big(\theta^T \cdot x^{(i)} - y^{(i)}\big)^2$$</p>
<p>Later we will refer to $h_\theta$ as just $h$, and $MSE(\theta)$ instead of $MSE(X,h_\theta)$</p>
<p>Normal regression find $\theta$ that minimizes MSE:<br>$$ \hat{\theta} = (\pmb{X}^T \cdot \pmb{X})^{-1} \cdot \pmb{X}^T \cdot \pmb{y}$$</p>
<ul>
<li>$\hat{\theta}$ is the $\theta$ that minimizes MSE</li>
<li>$\pmb{\theta}$ is the vector of target values $y^{(1)}$ to $y^{(m)}$.</li>
</ul>
<p><strong>Note</strong>: the first releases of the book implied that the <code>LinearRegression</code> class was based on the Normal Equation. This was an error, my apologies: as explained above, it is based on the pseudoinverse, which ultimately relies on the SVD matrix decomposition of $\mathbf{X}$ (see chapter 8 for details about the SVD decomposition). Its time complexity is $O(n^2)$ and it works even when $m &lt; n$ or when some features are linear combinations of other features (in these cases, $\mathbf{X}^T \mathbf{X}$ is not invertible so the Normal Equation fails), see <a href="https://github.com/ageron/handson-ml/issues/184" target="_blank" rel="noopener">issue #184</a> for more details. However, this does not change the rest of the description of the <code>LinearRegression</code> class, in particular, it is based on an analytical solution, it does not scale well with the number of features, it scales linearly with the number of instances, all the data must fit in memory, it does not require feature scaling and the order of the instances in the training set does not matter.</p>
<p>There is a mistake in the book: the LinearRegression class does not actually use the Normal Equation, it computes the pseudoinverse of X (specifically the Moore-Penrose pseudoinverse), and it multiplies it by y. It gives the same result as the normal equation, but it has two important advantages:</p>
<p>It is $O(n^2)$ instead of $O(n^2.4)$ to $O(n^3)$. So it’s much faster than the Normal Equation when there are many features.<br>It behaves better when some of the eigenvalues of X are small (or equal to zero). In plain English, this is when some features are highly correlated (or perfectly correlated, that is when one feature is a linear combination of the others). It also behaves well when m &lt; n. More details below.</p>
<p>When some features are linear combinations of the others, or when m &lt; n, the matrix $X^T$, $X$ is not invertible (it is said to be “singular” or “degenerate”). So the Normal Equation cannot be used. However, the pseudoinverse is always defined: it is based on the SVD decomposition of the matrix $X$, which finds the eigenvalues and eigenvectors of the matrix (as explained in chapter 8 on dimensionality reduction, when talking about PCA), so it can simply ignore the eigenvalues that are smaller than a tiny threshold. This means that the LinearRegression class will find the optimal solution even when some features are redundant or when m &lt; n.<br>This also explains two parameters that the LinearRegression predictor learns during training:</p>
<p>rank_ is the rank of the matrix X, i.e., the number of eigenvalues that are not tiny or zero.<br>singular_ is the list of eigenvalues (just like PCA().fit(X).singular_values_). If a feature is a linear combination of other features, then one of these features will have a tiny or zero eigenvalue.<br>Fortunately, this error does not change much of what I wrote in the book about the LinearRegression class: it is based on an analytical solution, it performs poorly when there is a large number of features, it is linear with regards to the number of instances, it does not support out-of-core (i.e., all data must fit in memory to use it), it does not require feature scaling, the order of the instances in the training set does not matter, and so on. You could say it’s an implementation detail, but it’s quite an important one, and I apologize for the error.</p>
<h1 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h1><p>First, let’s make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># To support both python 2 and python 3</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division, print_function, unicode_literals</span><br><span class="line"></span><br><span class="line"><span class="comment"># Common imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># to make this notebook's output stable across runs</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># To plot pretty figures</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">'axes.labelsize'</span>] = <span class="number">14</span></span><br><span class="line">plt.rcParams[<span class="string">'xtick.labelsize'</span>] = <span class="number">12</span></span><br><span class="line">plt.rcParams[<span class="string">'ytick.labelsize'</span>] = <span class="number">12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Where to save the figures</span></span><br><span class="line">PROJECT_ROOT_DIR = <span class="string">".."</span></span><br><span class="line">CHAPTER_ID = <span class="string">"training_linear_models"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_fig</span><span class="params">(fig_id, tight_layout=True)</span>:</span></span><br><span class="line">    path = os.path.join(PROJECT_ROOT_DIR, <span class="string">"images"</span>, CHAPTER_ID, fig_id + <span class="string">".png"</span>)</span><br><span class="line">    print(<span class="string">"Saving figure"</span>, fig_id)</span><br><span class="line">    <span class="keyword">if</span> tight_layout:</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">    plt.savefig(path, format=<span class="string">'png'</span>, dpi=<span class="number">300</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ignore useless warnings (see SciPy issue #5998)</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(action=<span class="string">"ignore"</span>, module=<span class="string">"scipy"</span>, message=<span class="string">"^internal gelsd"</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Linear-regression-using-the-Normal-Equation"><a href="#Linear-regression-using-the-Normal-Equation" class="headerlink" title="Linear regression using the Normal Equation"></a>Linear regression using the Normal Equation</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#Generate random numbers between 0 and 1. </span></span><br><span class="line">X = <span class="number">2</span> * np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * X + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>We generate data with a noisy linear trend, let’s plot it.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(X, y, <span class="string">"b."</span>)</span><br><span class="line">plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">15</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_14_0.png" alt="png"></p>
<p>Let’s compute $\hat{\theta}$ using $(X’X)^{-1}\cdot(X’y)$. Command inv() inverts a matrix. First we add constant term to the X, which will have two features ($X_1$ and constant). $X = X[1,X_1]$. In Python matrix transposition is <font color="blue">X_b.T<font color="black"> $=X’$, matrix multuplication $X’X$ = <font color="blue">X_b.T.dot.X_b<font color="black"></font></font></font></font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_b = np.c_[np.ones((<span class="number">100</span>, <span class="number">1</span>)), X]  <span class="comment"># add x0 = 1 to each instance</span></span><br><span class="line">theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)</span><br><span class="line">theta_best</span><br></pre></td></tr></table></figure>

<pre><code>array([[4.21509616],
       [2.77011339]])</code></pre><p>Actual function to generate noise was $y = 4 + 3x + u$, where $u$ is the Gaussian noise. If we were to have a lot of data we would have estimated $\theta_0 = 4$ and $\theta_1 = 3$, instead we estimate $\theta_0 = 4.21$ and $\theta_1 = 2.77$. We can see how adding more observations moves us closer to the desired parameters. We are getting closer to $[4,3]$ as $n \rightarrow \infty$ <br></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># Define function to estimate OLS as with different number of observations</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">OLS</span><span class="params">(n)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">42</span>)</span><br><span class="line">    X = <span class="number">2</span> * np.random.rand(n, <span class="number">1</span>)</span><br><span class="line">    y = <span class="number">4</span> + <span class="number">3</span> * X + np.random.randn(n, <span class="number">1</span>)</span><br><span class="line">    X_b = np.c_[np.ones((n, <span class="number">1</span>)), X]  <span class="comment"># add x0 = 1 to each instance</span></span><br><span class="line">    theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)</span><br><span class="line">    <span class="keyword">return</span> theta_best </span><br><span class="line">theta_best = OLS(<span class="number">100</span>)</span><br><span class="line">print(<span class="string">"n= 100"</span>, <span class="string">"theta0 ="</span> ,theta_best[<span class="number">0</span>] , <span class="string">"theta1 ="</span>, theta_best[<span class="number">1</span>] )</span><br><span class="line">theta_best = OLS(<span class="number">1000</span>)</span><br><span class="line">print(<span class="string">"n= 1000"</span>, <span class="string">"theta0 ="</span> ,theta_best[<span class="number">0</span>] , <span class="string">"theta1 ="</span>, theta_best[<span class="number">1</span>] )</span><br><span class="line">theta_best = OLS(<span class="number">10000</span>)</span><br><span class="line">print(<span class="string">"n= 10000"</span>, <span class="string">"theta0 ="</span> ,theta_best[<span class="number">0</span>] , <span class="string">"theta1 ="</span>, theta_best[<span class="number">1</span>] )</span><br></pre></td></tr></table></figure>

<pre><code>n= 100 theta0 = [4.21509616] theta1 = [2.77011339]
n= 1000 theta0 = [4.17478026] theta1 = [2.92260742]
n= 10000 theta0 = [4.03177675] theta1 = [2.98034911]</code></pre><p>We can predict y for the values of x = (0,2), extreme value of x to plot the regression function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)</span><br><span class="line"><span class="comment"># Extereme value of X</span></span><br><span class="line">X_new = np.array([[<span class="number">0</span>], [<span class="number">2</span>]])</span><br><span class="line">X_new_b = np.c_[np.ones((<span class="number">2</span>, <span class="number">1</span>)), X_new]  <span class="comment"># add x0 = 1 to each instance</span></span><br><span class="line"><span class="comment"># Extreme value for y, knowing both bounds of X and Y will allow us to draw a line between them.</span></span><br><span class="line">y_predict = X_new_b.dot(theta_best)</span><br><span class="line">y_predict</span><br></pre></td></tr></table></figure>

<pre><code>array([[4.21509616],
       [9.75532293]])</code></pre><p>Plot the regression line using extreme values.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(X_new, y_predict, <span class="string">"r-"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Predictions"</span>)</span><br><span class="line">plt.plot(X, y, <span class="string">"b."</span>)</span><br><span class="line">plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">15</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_22_0.png" alt="png"></p>
<p>We can simply use prepackaged Linear Regression model from sklearn.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X, y)</span><br><span class="line">lin_reg.intercept_, lin_reg.coef_</span><br></pre></td></tr></table></figure>

<pre><code>(array([4.21509616]), array([[2.77011339]]))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lin_reg.predict(X_new)</span><br></pre></td></tr></table></figure>

<pre><code>array([[4.21509616],
       [9.75532293]])</code></pre><p>The LinearRegression class is based on the scipy.linalg.lstsq() function (the name stands for “least squares”), which you could call directly:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=<span class="number">1e-6</span>)</span><br><span class="line">theta_best_svd</span><br></pre></td></tr></table></figure>

<pre><code>array([[4.21509616],
       [2.77011339]])</code></pre><p>This function computes $X^{+}y$, where $X^+$ is the pseudoinverse of $X$ (specifically the Moore-Penrose inverse). You can use np.linalg.pinv() to compute the pseudoinverse directly:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.linalg.pinv(X_b).dot(y)</span><br></pre></td></tr></table></figure>

<pre><code>array([[4.21509616],
       [2.77011339]])</code></pre><p>If we have complete linear dependence, the classication regression will fail, but SVD will not.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">Q = <span class="number">2</span> * np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">W = <span class="number">4</span> + <span class="number">3</span> * Q + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">Q_b = np.c_[np.ones((<span class="number">100</span>, <span class="number">1</span>)), Q, Q]  </span><br><span class="line">theta_best = np.linalg.inv(Q_b.T.dot(Q_b)).dot(Q_b.T).dot(W)</span><br><span class="line">print(theta_best)</span><br></pre></td></tr></table></figure>

<pre><code>[[-1008.14178262]
 [ 2039.99368946]
 [-1544.3488403 ]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.linalg.pinv(Q_b).dot(W)</span><br></pre></td></tr></table></figure>

<pre><code>array([[4.21509616],
       [1.38505669],
       [1.38505669]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lin_reg.fit(Q_b, W)</span><br><span class="line">lin_reg.intercept_, lin_reg.coef_</span><br></pre></td></tr></table></figure>

<pre><code>(array([4.21509616]), array([[0.        , 1.38505669, 1.38505669]]))</code></pre><h1 id="Linear-regression-using-batch-gradient-descent"><a href="#Linear-regression-using-batch-gradient-descent" class="headerlink" title="Linear regression using batch gradient descent"></a>Linear regression using batch gradient descent</h1><p>The Normal Equation computes the inverse of $X^T · X$, which is an $n × n$ matrix (where n is the number of features). The computational complexity of inverting such a matrix is typically about O$(n^{2.4})$ to $O(n^3)$ (depending on the implementation). In other words, if you double the number of features, you multiply the computation time by roughly $2^{2.4} = 5.3$ to $2^3 = 8$.</p>
<ul>
<li>However, the question is linear to the number of instances (observations) $O(m)$, so we can handle large datasets if they fit in the memory. </li>
<li>Once the coefficients are estimated, predictions are fast to make, the complexity is linear with respect to both features and instances, twice as many features will take twice the time. <br></li>
</ul>
<p>Next we will look how to estimate linear with very large datasets</p>
<p>Gradient descent minimizes a cost function.  Starting with several random numbers, you try steps in different directions testing if a step minimizes a cost function. We stop the algorithm converges – reduction in cost function stopped. (picture).</p>
<ul>
<li>An important parameter in Gradient Descent is the size of the steps, determined by the learning rate hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time.</li>
<li>If the learning rate is too high, you might jump across the valley and end up on the other side, possibly even higher up than you were before. (picture)</li>
<li>Real-life cost function is complex and multi-dimensional. Starting point, direction of change, and the step-size can all lead to success or failure of optimization (picture). </li>
</ul>
<p>MSE function of linear regression is convininent for optimization. It is:</p>
<ul>
<li>Convex – if you pick any two points on the curve, the line segment joining them never crosses the curve). Hence, there are not local minima, only global minimum.</li>
<li>Lipschitz Continuous – the slope does not change very rapidly, i.e. the derivative is bounded a by a real number: $\mid  f(x_{1}) - f(x_{2}) \mid  \leq K\mid x_{1} - x_{2}\mid$, where $K \in \mathbf{R}$</li>
</ul>
<p>These two facts have a great consequence: Gradient Descent is guaranteed to approach arbitrarily close the global minimum (if you wait long enough and if the learning rate is not too high).</p>
<p>Regression with two parameters looks like a bowl, where need to find a minimum. If the features are scaled the bowl is round, and we will reach the bottom relatively quickly. If the features are not scaled different scales confuse the algorithm into thinking that features are important than others. <br></p>
<p>Training a model means searching for a combination of model parameters that minimizes a cost function. It is a search in the model’s parameter space: the more parameters a model has, the more dimensions this space has, and the harder the search is. Fortunately, since the cost function is convex in the case of Linear Regression, the needle is simply at the bottom of the bowl, so there is only one minimum point.</p>
<h1 id="Batch-Gradient-Descent"><a href="#Batch-Gradient-Descent" class="headerlink" title="Batch Gradient Descent"></a>Batch Gradient Descent</h1><p>To implement Gradient Descent, we compute the gradient of the cost function with regards to each model parameter $θ<em>j$. In other words, you need to calculate how much the cost function will change if you change θj just a little bit.This is called a partial derivative. (Ex. mountain slopes facing different directions). Next we compute the partial derivative of the cost function with regards to parameter $\theta_j$, noted $\frac{\partial MSE(\theta)}{\partial \theta_j}$.<br>$$<br>\frac{\partial MSE(\theta)}{\partial \theta_j} = \frac{2}{m}\sum\limits</em>{i=1}^m \big(\theta^{T} \cdot x^{(i)} - y^{(i)}\big)x^{(i)}<br>$$<br>We estimate a vector of gradients in one go. The gradient vector, noted $\nabla\theta MSE(\theta)$, contains all the partial derivatives of the cost function (one for each model parameter):</p>
<pre><code>\nabla\theta MSE(\theta)               \begin{cases}
              \frac{\partial MSE(\theta)}{\partial \theta_0}\\
              \frac{\partial MSE(\theta)}{\partial \theta_1}\\
             \vdots \\
              \frac{\partial MSE(\theta)}{\partial \theta_n}\\
            \end{cases}</code></pre><p>Notice that this formula involves calculations over the full training set X, at each Gradient Descent step! This is why the algorithm is called Batch Gradient Descent: it uses the whole batch of training data at every step. As a result it is terribly slow on very large training sets. However, Gradient Descent scales well with the number of features; training a Linear Regression model when there are hundreds of thousands of features is much faster using Gradient Descent than using the Normal Equation.</p>
<p>Once you have the gradient vector, which points uphill, just go in the opposite direction. This means subtracting $\nabla\theta MSE(\theta)$ from $\theta$. This is where the learning rate $\eta$ comes into play. multiply the gradient vector by $\eta$ to determine the size of the downhill step:<br>$$<br>\theta^{next :step} = \theta - \eta \nabla\theta MSE(\theta)<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set step at 0.1</span></span><br><span class="line">eta = <span class="number">0.1</span></span><br><span class="line"><span class="comment"># number of steps</span></span><br><span class="line">n_iterations = <span class="number">1000</span></span><br><span class="line"><span class="comment"># number of observations</span></span><br><span class="line">m = <span class="number">100</span></span><br><span class="line"><span class="comment"># randomly set the starting point</span></span><br><span class="line">theta = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Walk 1000 steps. </span></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">    gradients = <span class="number">2</span>/m * X_b.T.dot(X_b.dot(theta) - y)</span><br><span class="line">    theta = theta - eta * gradients</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theta</span><br></pre></td></tr></table></figure>

<pre><code>array([[4.21509616],
       [2.77011339]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is what we got from Matrix OLS.</span></span><br><span class="line">theta_best = OLS(<span class="number">100</span>)</span><br><span class="line">print(<span class="string">"n= 100"</span>, <span class="string">"theta0 ="</span> ,theta_best[<span class="number">0</span>] , <span class="string">"theta1 ="</span>, theta_best[<span class="number">1</span>] )</span><br></pre></td></tr></table></figure>

<pre><code>n= 100 theta0 = [4.21509616] theta1 = [2.77011339]</code></pre><p>Let’s look at the first 10 steps of gradient descent algorithms with different step sizes.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set a path vector</span></span><br><span class="line">theta_path_bgd = []</span><br><span class="line"><span class="comment"># program that plots gradient descent graph</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_gradient_descent</span><span class="params">(theta, eta, theta_path=None)</span>:</span></span><br><span class="line">    <span class="comment"># Number of observations m</span></span><br><span class="line">    m = len(X_b)</span><br><span class="line">    <span class="comment"># Plot target and features in scatter plot</span></span><br><span class="line">    plt.plot(X, y, <span class="string">"b."</span>)</span><br><span class="line">    <span class="comment"># set number of iterations</span></span><br><span class="line">    n_iterations = <span class="number">1000</span></span><br><span class="line">    <span class="comment"># loop over the iterations</span></span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">        theta_path_bgd.append(theta)</span><br><span class="line">    <span class="comment"># plot first 10 iterations</span></span><br><span class="line">        <span class="keyword">if</span> iteration &lt; <span class="number">10</span>:</span><br><span class="line">            <span class="comment"># preduct y using existing theta (starting values)</span></span><br><span class="line">            y_predict = X_new_b.dot(theta)</span><br><span class="line">            <span class="comment"># Plot first iteration with red, others in blue color</span></span><br><span class="line">            style = <span class="string">"b-"</span> <span class="keyword">if</span> iteration &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">"r--"</span></span><br><span class="line">            <span class="comment"># Plot regression line predicting y</span></span><br><span class="line">            plt.plot(X_new, y_predict, style)</span><br><span class="line">        <span class="comment"># Calculate gradient using formula above.</span></span><br><span class="line">        gradients = <span class="number">2</span>/m * X_b.T.dot(X_b.dot(theta) - y)</span><br><span class="line">        <span class="comment"># Update theta using the gradient</span></span><br><span class="line">        theta = theta - eta * gradients </span><br><span class="line">        <span class="comment">#Save estimate thetas in the vector theta_path</span></span><br><span class="line">    plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">15</span>])</span><br><span class="line">    plt.title(<span class="string">r"$\eta = &#123;&#125;$"</span>.format(eta), fontsize=<span class="number">16</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">theta = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)  <span class="comment"># random initialization</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">4</span>))</span><br><span class="line">plt.subplot(<span class="number">131</span>); plot_gradient_descent(theta, eta=<span class="number">0.02</span>)</span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.subplot(<span class="number">132</span>); plot_gradient_descent(theta, eta=<span class="number">0.1</span>, theta_path=theta_path_bgd)</span><br><span class="line">plt.subplot(<span class="number">133</span>); plot_gradient_descent(theta, eta=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_46_0.png" alt="png"></p>
<ol>
<li>On the left, the learning rate is too low: the algorithm will eventually reach the solution, but it will take a long time. </li>
<li>In the middle, the learning rate looks pretty good: in just a few iterations, it has already converged to the solution</li>
<li>On the right, the learning rate is too high: the algorithm diverges, jumping all over the place and actually getting further and further away from the solution at every step</li>
</ol>
<p>If you have enough time it is always better to use small steps and a lot of iterations. If the data is too big for that, then you need to experiment using grid search. <br></p>
<p>How to set the number of iterations? <br><br>If you set it too small, your may not find the minimum, if it is too large you may waste time without improving the fit. A good ways it set minimum gradient that would continue the iterations. If the rate of improvement is less than the minimum tolerance $\epsilon$, it is a good idea to stop the iterations.  </p>
<p>When the cost function is convex and its slope does not change abruptly (as is the case for the MSE cost function), the Batch Gradient Descent with a fixed learning rate has a convergence rate of $O = \frac{1}{Iterations}$. In other words, if you divide the tolerance $\epsilon$ by 10 (to have a more precise solution), then the algorithm will have to run about 10 times more iterations.</p>
<h1 id="Stochastic-Gradient-Descent-随机梯度下降"><a href="#Stochastic-Gradient-Descent-随机梯度下降" class="headerlink" title="Stochastic Gradient Descent 随机梯度下降"></a>Stochastic Gradient Descent 随机梯度下降</h1><ul>
<li>Batch Gradient Descent uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large.</li>
<li>Stochastic Gradient Descent (SGD) just picks a random instance in the training set at every step and computes the gradients based only on that single instance. This makes the algorithm much faster since it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration.</li>
<li>SGD, due to its stochastic (i.e., random) nature, is much less regular than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down . So once the algorithm stops, the final parameter values are good, but not optimal. (picture)</li>
</ul>
<p>Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then shuffle it again, and so on. However, this generally converges more slowly.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set path array</span></span><br><span class="line">theta_path_sgd = []</span><br><span class="line">m = len(X_b)</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># number of iterations</span></span><br><span class="line">n_epochs = <span class="number">50</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">50</span>  <span class="comment"># learning schedule hyperparameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learning_schedule</span><span class="params">(t)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> t0 / (t + t1)</span><br><span class="line"><span class="comment"># start with randomly generated starting parameters theta</span></span><br><span class="line">theta = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)  <span class="comment"># random initialization</span></span><br><span class="line"><span class="comment"># loop over iterations</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">    <span class="comment"># add theta to the vector</span></span><br><span class="line">    theta_path_sgd.append(theta)                 <span class="comment"># not shown</span></span><br><span class="line">    <span class="comment"># loop over observations. Here we loop over all observations, we can just had a few draw, like range(20)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment"># show regression lines for the first 20 iterations</span></span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">0</span> <span class="keyword">and</span> i &lt; <span class="number">20</span>:                    <span class="comment"># not shown in the book</span></span><br><span class="line">            <span class="comment"># Predict y using existing theta</span></span><br><span class="line">            y_predict = X_new_b.dot(theta)           <span class="comment"># not shown</span></span><br><span class="line">            <span class="comment"># first line is red</span></span><br><span class="line">            style = <span class="string">"b-"</span> <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="string">"r--"</span>         <span class="comment"># not shown</span></span><br><span class="line">            <span class="comment">#other lines are blue. Plot predictions</span></span><br><span class="line">            plt.plot(X_new, y_predict, style)        <span class="comment"># not shown</span></span><br><span class="line">        <span class="comment"># Randomly select number between 1 and m.</span></span><br><span class="line">        random_index = np.random.randint(m)</span><br><span class="line">        <span class="comment"># extract randomly selected observation for x</span></span><br><span class="line">        xi = X_b[random_index:random_index+<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># extract randomly selected observation for y</span></span><br><span class="line">        yi = y[random_index:random_index+<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># Calculate gradient for randomly selected observations</span></span><br><span class="line">        gradients = <span class="number">2</span> * xi.T.dot(xi.dot(theta) - yi)</span><br><span class="line">        <span class="comment"># Tolerance level, is 50 / (epoch * m + i + 5)</span></span><br><span class="line">        eta = learning_schedule(epoch * m + i)</span><br><span class="line">        <span class="comment"># update theta</span></span><br><span class="line">        theta = theta - eta * gradients</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.plot(X, y, <span class="string">"b."</span>)                                 <span class="comment"># not shown</span></span><br><span class="line">plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)                     <span class="comment"># not shown</span></span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)           <span class="comment"># not shown</span></span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">15</span>])                              <span class="comment"># not shown</span></span><br><span class="line">plt.show()                                           <span class="comment"># not shown</span></span><br></pre></td></tr></table></figure>

<p><img src="output_53_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">thetapath = np.array(theta_path_sgd)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">thetapath[<span class="number">49</span>,:]</span><br></pre></td></tr></table></figure>

<pre><code>array([[4.21200431],
       [2.74968529]])</code></pre><p>Remember that correct values are $[4.21509616, 2.77011339]$, we are very close</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># First 20 steps of iteration 1</span></span><br><span class="line">plt.plot(thetapath[<span class="number">0</span>:<span class="number">20</span>,<span class="number">0</span>],thetapath[<span class="number">0</span>:<span class="number">20</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[&lt;matplotlib.lines.Line2D at 0x248543aa358&gt;]</code></pre><p><img src="output_58_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#40-50 steps</span></span><br><span class="line">plt.plot(thetapath[<span class="number">40</span>:<span class="number">49</span>,<span class="number">0</span>],thetapath[<span class="number">40</span>:<span class="number">49</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[&lt;matplotlib.lines.Line2D at 0x24854424d30&gt;]</code></pre><p><img src="output_59_1.png" alt="png"></p>
<p>We are close, but we are not really converging to the correct value. Next we estimate a SGD regression included in sklearn. Number of iterations is 50. The defaults to optimizing the squared error cost function. We run 50 epochs, starting with a learning rate of 0.1 (eta0=0.1), using the default learning schedule (different from the preceding one), and it does not use any regularization (penalty=None; more details on this shortly):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line">sgd_reg = SGDRegressor(max_iter=<span class="number">50</span>, penalty=<span class="literal">None</span>,  eta0 = <span class="number">0.1</span>,  random_state=<span class="number">42</span>)</span><br><span class="line">sgd_reg.fit(X, y.ravel())</span><br><span class="line">sgd_reg.intercept_, sgd_reg.coef_</span><br></pre></td></tr></table></figure>

<pre><code>(array([4.24365286]), array([2.8250878]))</code></pre><p>Not great, let’s increase the number of iterations</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line">sgd_reg = SGDRegressor(max_iter=<span class="number">500</span>, penalty=<span class="literal">None</span>, eta0=<span class="number">0.1</span>,random_state=<span class="number">42</span>)</span><br><span class="line">sgd_reg.fit(X, y.ravel())</span><br><span class="line">sgd_reg.intercept_, sgd_reg.coef_</span><br></pre></td></tr></table></figure>

<pre><code>(array([4.24365286]), array([2.8250878]))</code></pre><p>Much better. Alternatively we can reduce tolerance 10 times.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line">sgd_reg = SGDRegressor(max_iter=<span class="number">5000</span>, penalty=<span class="literal">None</span>, eta0=<span class="number">0.1</span>,random_state=<span class="number">42</span>)</span><br><span class="line">sgd_reg.fit(X, y.ravel())</span><br><span class="line">sgd_reg.intercept_, sgd_reg.coef_</span><br></pre></td></tr></table></figure>

<pre><code>(array([4.24365286]), array([2.8250878]))</code></pre><p>The results got worse we are dancing aroung correct value.</p>
<h1 id="Mini-batch-gradient-descent"><a href="#Mini-batch-gradient-descent" class="headerlink" title="Mini-batch gradient descent"></a>Mini-batch gradient descent</h1><p>Instead of computing the gradients based on the full training set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD computes the gradients on small random sets of instances called mini-batches. The main advantage of Mini-batch GD over Stochastic GD is that you can get a performance boost from hardware optimization of matrix operations, especially when using GPUs.</p>
<p>The algorithm’s progress in parameter space is less erratic than with SGD, especially with fairly large mini-batches. As a result, Mini-batch GD will end up walking around a bit closer to the minimum than SGD. But, on the other hand, it may be harder for it to escape from local minima (in the case of problems that suffer from local minima, unlike Linear Regression as we saw earlier). Next we will show the paths taken by the three Gradient Descent algorithms in parameter space during training. They all end up near the minimum, but Batch GD’s path actually stops at the minimum, while both Stochastic GD and Mini-batch GD continue to walk around. However, the Batch GD takes a lot of time to take each step, and Stochastic GD and Mini-batch GD would also reach the minimum if you used a good learning schedule.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">theta_path_mgd = []</span><br><span class="line"></span><br><span class="line">n_iterations = <span class="number">50</span></span><br><span class="line"><span class="comment"># We will just use 20 observations out of 100</span></span><br><span class="line">minibatch_size = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">theta = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)  <span class="comment"># random initialization</span></span><br><span class="line"></span><br><span class="line">t0, t1 = <span class="number">200</span>, <span class="number">1000</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learning_schedule</span><span class="params">(t)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> t0 / (t + t1)</span><br><span class="line"></span><br><span class="line">t = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">    <span class="comment"># Shuffle the data x and y. Because we will draw a series of 20 observation order now can matter a great deal. Before we</span></span><br><span class="line">    <span class="comment"># were just </span></span><br><span class="line">    shuffled_indices = np.random.permutation(m)</span><br><span class="line">    X_b_shuffled = X_b[shuffled_indices]</span><br><span class="line">    y_shuffled = y[shuffled_indices]</span><br><span class="line">    <span class="comment"># Loop each batch of observations from 0 to 20</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, m, minibatch_size):</span><br><span class="line">        <span class="comment"># set counter t</span></span><br><span class="line">        t += <span class="number">1</span></span><br><span class="line">        <span class="comment"># Get first of observation of the batch</span></span><br><span class="line">        xi = X_b_shuffled[i:i+minibatch_size]</span><br><span class="line">        yi = y_shuffled[i:i+minibatch_size]</span><br><span class="line">        <span class="comment"># calculate the gradient</span></span><br><span class="line">        gradients = <span class="number">2</span>/minibatch_size * xi.T.dot(xi.dot(theta) - yi)</span><br><span class="line">        <span class="comment"># calculate new step</span></span><br><span class="line">        eta = learning_schedule(t)</span><br><span class="line">        <span class="comment"># update theta</span></span><br><span class="line">        theta = theta - eta * gradients</span><br><span class="line">        <span class="comment"># record the path</span></span><br><span class="line">        theta_path_mgd.append(theta)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"final theta ="</span>,theta)</span><br></pre></td></tr></table></figure>

<pre><code>final theta = [[4.25214635]
 [2.7896408 ]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Lets create vetors for our paths</span></span><br><span class="line">theta_path_bgd = np.array(theta_path_bgd)</span><br><span class="line">theta_path_sgd = np.array(theta_path_sgd)</span><br><span class="line">theta_path_mgd = np.array(theta_path_mgd)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">7</span>,<span class="number">4</span>))</span><br><span class="line">plt.plot(theta_path_sgd[:, <span class="number">0</span>], theta_path_sgd[:, <span class="number">1</span>], <span class="string">"r-s"</span>, linewidth=<span class="number">1</span>, label=<span class="string">"Stochastic"</span>)</span><br><span class="line">plt.plot(theta_path_mgd[:, <span class="number">0</span>], theta_path_mgd[:, <span class="number">1</span>], <span class="string">"g-+"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Mini-batch"</span>)</span><br><span class="line"><span class="comment"># We called this function 3 times, so the vector has 3000 obs instead of 1000. We just use the first 1000</span></span><br><span class="line">plt.plot(theta_path_bgd[:<span class="number">1000</span>, <span class="number">0</span>], theta_path_bgd[:<span class="number">1000</span>, <span class="number">1</span>], <span class="string">"b-o"</span>, linewidth=<span class="number">3</span>, label=<span class="string">"Batch"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xlabel(<span class="string">r"$\theta_0$"</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.ylabel(<span class="string">r"$\theta_1$   "</span>, fontsize=<span class="number">20</span>, rotation=<span class="number">0</span>)</span><br><span class="line">plt.axis([<span class="number">2.5</span>, <span class="number">4.5</span>, <span class="number">2.3</span>, <span class="number">3.9</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_73_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot last 50 steps</span></span><br><span class="line">plt.figure(figsize=(<span class="number">7</span>,<span class="number">4</span>))</span><br><span class="line">plt.plot(theta_path_sgd[<span class="number">-50</span>:, <span class="number">0</span>], theta_path_sgd[<span class="number">-50</span>:, <span class="number">1</span>], <span class="string">"r-s"</span>, linewidth=<span class="number">1</span>, label=<span class="string">"Stochastic"</span>)</span><br><span class="line">plt.plot(theta_path_mgd[<span class="number">-50</span>:, <span class="number">0</span>], theta_path_mgd[<span class="number">-50</span>:, <span class="number">1</span>], <span class="string">"g-+"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Mini-batch"</span>)</span><br><span class="line"><span class="comment"># We called this function 3 times, so the vector has 3000 obs instead of 1000. We just use the first 1000</span></span><br><span class="line">plt.plot(theta_path_bgd[<span class="number">950</span>:<span class="number">1000</span>, <span class="number">0</span>], theta_path_bgd[<span class="number">950</span>:<span class="number">1000</span>, <span class="number">1</span>], <span class="string">"b-o"</span>, linewidth=<span class="number">3</span>, label=<span class="string">"Batch"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">plt.xlabel(<span class="string">r"$\theta_0$"</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.ylabel(<span class="string">r"$\theta_1$   "</span>, fontsize=<span class="number">20</span>, rotation=<span class="number">0</span>)</span><br><span class="line">plt.axis([<span class="number">2.5</span>, <span class="number">4.5</span>, <span class="number">2.3</span>, <span class="number">3.9</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_74_0.png" alt="png"></p>
<h1 id="Polynomial-regression"><a href="#Polynomial-regression" class="headerlink" title="Polynomial regression"></a>Polynomial regression</h1><p>What if your data is actually more complex than a simple straight line? A simple way to use Linear model for non-linear data is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called Polynomial Regression.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> numpy.random <span class="keyword">as</span> rnd</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<p>X is 100 observations draws from $X = 6*R - 3$, where $R \in (0,1)$. Then we estimate $Y = 0.5X^2 + X + 2 + R$.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = <span class="number">100</span></span><br><span class="line">X = <span class="number">6</span> * np.random.rand(m, <span class="number">1</span>) - <span class="number">3</span></span><br><span class="line">y = <span class="number">0.5</span> * X**<span class="number">2</span> + X + <span class="number">2</span> + np.random.randn(m, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>Plot X and Y</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(X, y, <span class="string">"b."</span>)</span><br><span class="line">plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">10</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_81_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import package to craeate polinomial features</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line">poly_features = PolynomialFeatures(degree=<span class="number">2</span>, include_bias=<span class="literal">False</span>)</span><br><span class="line">X_poly = poly_features.fit_transform(X)</span><br><span class="line">X[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<pre><code>array([-0.75275929])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(X_poly[<span class="number">0</span>])</span><br><span class="line">print((<span class="number">-0.75275929</span>)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>[-0.75275929  0.56664654]
0.566646548681304</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fit polinomial regression</span></span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X_poly, y)</span><br><span class="line">lin_reg.intercept_, lin_reg.coef_</span><br></pre></td></tr></table></figure>

<pre><code>(array([1.78134581]), array([[0.93366893, 0.56456263]]))</code></pre><p>The model estimates $\hat{Y} = 0.56X^2 + 0.93X + 1.78 + R$, whereas a real model is $Y = 0.5X^2 + X + 2 + R$. Polinomial regresion is capable of finding relationships<br>between features by addint all combinations of features up to the given degree. For<br>example, if there were two features $a$ and $b$, PolynomialFeatures with degree=3 would not only add<br>the features $a^2$, $a^3$, $b^2$, and $b^3$, but also the combinations $ab$, $a^{2}b$, and $ab^2$. Polynomial features models with a degree $d$ and $n$ features, would produce $\frac{(n+d)!}{d!n!}$ combinations of features.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Generate space of drawing a solid prediction line.</span></span><br><span class="line">X_new=np.linspace(<span class="number">-3</span>, <span class="number">3</span>, <span class="number">100</span>).reshape(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># square the X</span></span><br><span class="line">X_new_poly = poly_features.transform(X_new)</span><br><span class="line">y_new = lin_reg.predict(X_new_poly)</span><br><span class="line"><span class="comment"># Draw scattered data</span></span><br><span class="line">plt.plot(X, y, <span class="string">"b."</span>)</span><br><span class="line"><span class="comment"># Draw prediction line</span></span><br><span class="line">plt.plot(X_new, y_new, <span class="string">"r-"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Predictions"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">10</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_86_0.png" alt="png"></p>
<p>High-degree Polynomial Regression fits the training data much better than the plain Linear Regression. Next we estimate a 300-degree polynomial model to the preceding training data, and compare the result with a pure linear model and a quadratic model (2nd degree polynomial).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> style, width, degree <span class="keyword">in</span> ((<span class="string">"g-"</span>, <span class="number">1</span>, <span class="number">300</span>), (<span class="string">"b--"</span>, <span class="number">2</span>, <span class="number">2</span>), (<span class="string">"r-+"</span>, <span class="number">2</span>, <span class="number">1</span>)):</span><br><span class="line">    polybig_features = PolynomialFeatures(degree=degree, include_bias=<span class="literal">False</span>)</span><br><span class="line">    std_scaler = StandardScaler()</span><br><span class="line">    lin_reg = LinearRegression()</span><br><span class="line">    polynomial_regression = Pipeline([</span><br><span class="line">            (<span class="string">"poly_features"</span>, polybig_features),</span><br><span class="line">            (<span class="string">"std_scaler"</span>, std_scaler),</span><br><span class="line">            (<span class="string">"lin_reg"</span>, lin_reg),</span><br><span class="line">        ])</span><br><span class="line">    polynomial_regression.fit(X, y)</span><br><span class="line">    y_newbig = polynomial_regression.predict(X_new)</span><br><span class="line">    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)</span><br><span class="line"></span><br><span class="line">plt.plot(X, y, <span class="string">"b."</span>, linewidth=<span class="number">3</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.axis([<span class="number">-3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">10</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_88_0.png" alt="png"></p>
<ul>
<li>This high-degree Polynomial Regression model is severely overfitting the training data, while<br>the linear model is underfitting it. The model that will generalize best in this case is the quadratic model.</li>
<li>The data was generated using a quadratic model, but in general you won’t know what<br>function generated the data. In Chapter 2 you used cross-validation to get an estimate of a model’s generalization performance.</li>
</ul>
<p>Another way is to look at the learning curves: these are plots of the model’s performance on the training<br>set and the validation set as a function of the training set size. To generate the plots, simply train the model<br>several times on different sized subsets of the training set. The following code defines a function that plots<br>the learning curves of a model given some training data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import MS and train-test split</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curves</span><span class="params">(model, X, y)</span>:</span></span><br><span class="line">    <span class="comment"># Split data into test and train</span></span><br><span class="line">    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">10</span>)</span><br><span class="line">    <span class="comment"># create vectors to save errors</span></span><br><span class="line">    train_errors, val_errors = [], []</span><br><span class="line">    <span class="comment"># loop through all observations adding one at a time</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> range(<span class="number">1</span>, len(X_train)):</span><br><span class="line">        <span class="comment"># fit the model</span></span><br><span class="line">        model.fit(X_train[:m], y_train[:m])</span><br><span class="line">        <span class="comment"># predict y on training</span></span><br><span class="line">        y_train_predict = model.predict(X_train[:m])</span><br><span class="line">        <span class="comment"># predict on validation</span></span><br><span class="line">        y_val_predict = model.predict(X_val)</span><br><span class="line">        <span class="comment"># calculate training and validation errors</span></span><br><span class="line">        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))</span><br><span class="line">        val_errors.append(mean_squared_error(y_val, y_val_predict))</span><br><span class="line"><span class="comment"># plot errors and sample size</span></span><br><span class="line">    plt.plot(np.sqrt(train_errors), <span class="string">"r-+"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"train"</span>)</span><br><span class="line">    plt.plot(np.sqrt(val_errors), <span class="string">"b-"</span>, linewidth=<span class="number">3</span>, label=<span class="string">"val"</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">"upper right"</span>, fontsize=<span class="number">14</span>)   <span class="comment"># not shown in the book</span></span><br><span class="line">    plt.xlabel(<span class="string">"Training set size"</span>, fontsize=<span class="number">14</span>) <span class="comment"># not shown</span></span><br><span class="line">    plt.ylabel(<span class="string">"RMSE"</span>, fontsize=<span class="number">14</span>)              <span class="comment"># not shown</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set large data</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">m = <span class="number">100</span></span><br><span class="line">X = <span class="number">6</span> * np.random.rand(m, <span class="number">1</span>) - <span class="number">3</span></span><br><span class="line">y = <span class="number">0.5</span> * X**<span class="number">2</span> + X + <span class="number">2</span> + np.random.randn(m, <span class="number">1</span>)</span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">plot_learning_curves(lin_reg, X, y)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">80</span>, <span class="number">0</span>, <span class="number">3</span>])                         <span class="comment"># not shown in the book</span></span><br><span class="line">plt.show()                                      <span class="comment"># not shown</span></span><br></pre></td></tr></table></figure>

<p><img src="output_91_0.png" alt="png"></p>
<p>When there are just one or two instances in the training set, the model can fit them perfectly, which is why the curve starts<br>at zero. But as new instances are added to the training set, it becomes impossible for the model to fit the<br>training data perfectly, both because the data is noisy and because it is not linear at all. So the error on the<br>training data goes up until it reaches a plateau, at which point adding new instances to the training set<br>doesn’t make the average error much better or worse. <br></p>
<p>When the model is trained on very few training instances, it is incapable of generalizing properly, which is why the validation error is initially quite big. Then as the model is shown more training examples, it learns and thus the validation error slowly goes down. However, once again a straight line cannot do a good job modeling the data, so the error ends up at a plateau, very close to the other curve. These learning curves are typical of an underfitting model. Both curves have reached a plateau; they are close and fairly high. <br></p>
<p>Now let’s look at the learning curves of a 10th-degree polynomial model on the same data.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line">polynomial_regression = Pipeline([</span><br><span class="line">        (<span class="string">"poly_features"</span>, PolynomialFeatures(degree=<span class="number">10</span>, include_bias=<span class="literal">False</span>)),</span><br><span class="line">        (<span class="string">"lin_reg"</span>, LinearRegression()),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">plot_learning_curves(polynomial_regression, X, y)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">80</span>, <span class="number">0</span>, <span class="number">3</span>])           <span class="comment"># not shown</span></span><br><span class="line">plt.show()                        <span class="comment"># not shown</span></span><br></pre></td></tr></table></figure>

<p><img src="output_93_0.png" alt="png"></p>
<p>These learning curves look a bit like the previous ones, but there are two very important differences: <br></p>
<ol>
<li>The error on the training data is much lower than with the Linear Regression model.</li>
<li>There is a persistent gap between the curves. This means that the model performs significantly better on the<br>training data than on the validation data, which is the hallmark of an overfitting model. However, if you used a much larger training set, the two curves would continue to get closer.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">polynomial_regression = Pipeline([</span><br><span class="line">        (<span class="string">"poly_features"</span>, PolynomialFeatures(degree=<span class="number">2</span>, include_bias=<span class="literal">False</span>)),</span><br><span class="line">        (<span class="string">"lin_reg"</span>, LinearRegression()),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">plot_learning_curves(polynomial_regression, X, y)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">80</span>, <span class="number">0</span>, <span class="number">3</span>])           <span class="comment"># not shown</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_95_0.png" alt="png"></p>
<p>Maybe we have too few obervations?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set large data</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">m = <span class="number">1000</span></span><br><span class="line">X = <span class="number">6</span> * np.random.rand(m, <span class="number">1</span>) - <span class="number">3</span></span><br><span class="line">y = <span class="number">0.5</span> * X**<span class="number">2</span> + X + <span class="number">2</span> + np.random.randn(m, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">plot_learning_curves(lin_reg, X, y)</span><br><span class="line">plt.axis([<span class="number">700</span>, <span class="number">800</span>, <span class="number">0</span>, <span class="number">3</span>])                         <span class="comment"># not shown in the book</span></span><br><span class="line">plt.show()                                      <span class="comment"># not shown</span></span><br></pre></td></tr></table></figure>

<p><img src="output_97_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">polynomial_regression = Pipeline([</span><br><span class="line">        (<span class="string">"poly_features"</span>, PolynomialFeatures(degree=<span class="number">10</span>, include_bias=<span class="literal">False</span>)),</span><br><span class="line">        (<span class="string">"lin_reg"</span>, LinearRegression()),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">plot_learning_curves(polynomial_regression, X, y)</span><br><span class="line">plt.axis([<span class="number">700</span>, <span class="number">800</span>, <span class="number">0</span>, <span class="number">3</span>])           <span class="comment"># not shown</span></span><br><span class="line">plt.show()                        <span class="comment"># not shown</span></span><br></pre></td></tr></table></figure>

<p><img src="output_98_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">polynomial_regression = Pipeline([</span><br><span class="line">        (<span class="string">"poly_features"</span>, PolynomialFeatures(degree=<span class="number">2</span>, include_bias=<span class="literal">False</span>)),</span><br><span class="line">        (<span class="string">"lin_reg"</span>, LinearRegression()),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">plot_learning_curves(polynomial_regression, X, y)</span><br><span class="line">plt.axis([<span class="number">700</span>, <span class="number">800</span>, <span class="number">0</span>, <span class="number">3</span>])           <span class="comment"># not shown</span></span><br><span class="line">plt.show()                        <span class="comment"># not shown</span></span><br></pre></td></tr></table></figure>

<p><img src="output_99_0.png" alt="png"></p>
<p>We fixed overfitting issues by using more data.</p>
<h1 id="Regularized-models"><a href="#Regularized-models" class="headerlink" title="Regularized models"></a>Regularized models</h1><p>A good way to reduce overfitting is to regularize the model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. For example, a simple way to regularize a polynomial model is to reduce the number of polynomial degrees. <br><br>For a linear model, regularization is typically achieved by constraining the weights of the model. We will<br>now look at <strong>Ridge Regression</strong>, <strong>Lasso Regression</strong>, and <strong>Elastic Net</strong>, which implement three different ways<br>to constrain the weights. <br></p>
<h1 id="Ridge-Regression"><a href="#Ridge-Regression" class="headerlink" title="Ridge Regression"></a>Ridge Regression</h1><p>Ridge Regression (also called Tikhonov regularization) is a regularized version of Linear Regression: a regularization term equal to is added to the cost function. This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to evaluate the model’s performance using the unregularized performance measure. <br></p>
<p>The hyperparameter $\alpha$ controls how much you want to regularize the model. If $\alpha = 0$ then Ridge Regression is just Linear Regression. If $\alpha$ is very large, then all weights end up very close to zero and the result is a flat line going through the data’s mean. Ridge regression cost function, where we add a sum of squared parameters.<br>$$ J(\theta) = MSE(\theta) + \alpha \frac{1}{2} \sum^m_{i=1} \theta^{2}_i$$</p>
<p>Note that the bias term $\theta_0$ is not regularized (the sum starts at $i = 1$, not $0$). If we define $w$ as the vector of<br>feature weights ($\theta_1$ to $\theta_n$), then the regularization term is simply equal to $\frac{1}{2}(\lVert w \rVert _2)^2$, where $\lVert . \rVert _2$ is teh $l_2$ norm of the weight vector. It is important to scale the data (e.g., using a StandardScaler) before performing Ridge Regression, as it is sensitive to the scale<br>of the input features. This is true of most regularized models. For Gradient Descent, just add $\alpha w$ to the MSE gradient<br>vector. <br></p>
<p>Next we will show several Ridge models trained on some linear data using different $\alpha$ value. On the left, plain Ridge models are used, leading to linear predictions. On the right, the data is first expanded using PolynomialFeatures(degree=10), then it is scaled using a StandardScaler, and finally the Ridge models are applied to the resulting features: this is Polynomial Regression with Ridge regularization. <br></p>
<p>Increasing $\alpha$ leads to flatter (i.e., less extreme, more reasonable) predictions; this reduces the<br>model’s variance but increases its bias.</p>
<p>As with Linear Regression, we can perform Ridge Regression either by computing a closed-form<br>equation or by performing Gradient Descent. The closed-form solution (where $A$ is the $n \times n$ identity matrix except with a 0 in the top-left cell, corresponding to the bias term).<br>$$ \hat{\theta} = ( X^T \cdot X  + \alpha A)^{-1} \cdot X^T \cdot y$$<br>Remember the regression without ridge it:<br>$$ \hat{\theta} = ( X^T \cdot X )^{-1} \cdot X^T \cdot y$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line">train_errors = []</span><br><span class="line">val_errors = []</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"><span class="comment"># Take 30 obs</span></span><br><span class="line">m = <span class="number">50</span></span><br><span class="line">X = <span class="number">3</span> * np.random.rand(m, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">1</span> + <span class="number">0.5</span> * X + np.random.randn(m, <span class="number">1</span>) / <span class="number">1.5</span></span><br><span class="line"></span><br><span class="line">X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X_new = np.linspace(<span class="number">0</span>, <span class="number">3</span>, <span class="number">100</span>).reshape(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_model</span><span class="params">(model_class, polynomial, alphas, **model_kargs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> alpha, style <span class="keyword">in</span> zip(alphas, (<span class="string">"b-"</span>, <span class="string">"g--"</span>, <span class="string">"r:"</span>)):</span><br><span class="line">        model = model_class(alpha, **model_kargs) <span class="keyword">if</span> alpha &gt; <span class="number">0</span> <span class="keyword">else</span> LinearRegression()</span><br><span class="line">        <span class="keyword">if</span> polynomial:</span><br><span class="line">            model = Pipeline([</span><br><span class="line">                    (<span class="string">"poly_features"</span>, PolynomialFeatures(degree=<span class="number">10</span>, include_bias=<span class="literal">False</span>)),</span><br><span class="line">                    (<span class="string">"std_scaler"</span>, StandardScaler()),</span><br><span class="line">                    (<span class="string">"regul_reg"</span>, model),</span><br><span class="line">                ])</span><br><span class="line">        model.fit(X_train, y_train)</span><br><span class="line">        print(<span class="string">"Coefficients from"</span>,model_class,<span class="string">"with alpha ="</span>,alpha )</span><br><span class="line">        <span class="keyword">if</span> polynomial: </span><br><span class="line">            classifier = model.named_steps[<span class="string">'regul_reg'</span>]</span><br><span class="line">            print(classifier.coef_)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(model.coef_)</span><br><span class="line">        y_new_regul = model.predict(X_new)</span><br><span class="line">        <span class="comment"># look at cross validation error</span></span><br><span class="line">        y_train_predict = model.predict(X_train[:m])</span><br><span class="line">        <span class="comment"># predict on validation</span></span><br><span class="line">        y_val_predict = model.predict(X_val)</span><br><span class="line">        <span class="comment"># calculate training and validation errors</span></span><br><span class="line">        train_errors.append(mean_squared_error(y_train, y_train_predict))</span><br><span class="line">        val_errors.append(mean_squared_error(y_val, y_val_predict))</span><br><span class="line">        lw = <span class="number">2</span> <span class="keyword">if</span> alpha &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=<span class="string">r"$\alpha = &#123;&#125;$"</span>.format(alpha))</span><br><span class="line">    plt.plot(X, y, <span class="string">"b."</span>, linewidth=<span class="number">3</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">"upper left"</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">4</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line"><span class="comment"># Plot ridge regression</span></span><br><span class="line">plot_model(Ridge, polynomial=<span class="literal">False</span>, alphas=(<span class="number">0</span>, <span class="number">10</span>, <span class="number">100</span>), random_state=<span class="number">42</span>)</span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">plot_model(Ridge, polynomial=<span class="literal">True</span>, alphas=(<span class="number">0</span>, <span class="number">10</span>**<span class="number">-5</span>, <span class="number">1</span>), random_state=<span class="number">42</span>)</span><br><span class="line">plt.show()</span><br><span class="line">print(<span class="string">"Training Errors Linear regression with Ridge"</span>,train_errors[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"Validation Errors Linear regression with Ridge"</span>,val_errors[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"Training Errors Polynomial regression with Ridge"</span>,train_errors[<span class="number">3</span>:])</span><br><span class="line">print(<span class="string">"Validation Errors Polinomial regression with Ridge"</span>,val_errors[<span class="number">3</span>:])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Coefficients from &lt;class 'sklearn.linear_model.ridge.Ridge'&gt; with alpha = 0</span><br><span class="line">[[<span class="number">0.4176916</span>]]</span><br><span class="line">Coefficients from &lt;class 'sklearn.linear_model.ridge.Ridge'&gt; with alpha = 10</span><br><span class="line">[[<span class="number">0.31349263</span>]]</span><br><span class="line">Coefficients from &lt;class 'sklearn.linear_model.ridge.Ridge'&gt; with alpha = 100</span><br><span class="line">[[<span class="number">0.09660269</span>]]</span><br><span class="line">Coefficients from &lt;class 'sklearn.linear_model.ridge.Ridge'&gt; with alpha = 0</span><br><span class="line">[[ <span class="number">2.79676282e+01</span> <span class="number">-8.70301875e+02</span>  <span class="number">1.02393993e+04</span> <span class="number">-6.14861326e+04</span></span><br><span class="line">   <span class="number">2.14563424e+05</span> <span class="number">-4.60902057e+05</span>  <span class="number">6.18449227e+05</span> <span class="number">-5.05246948e+05</span></span><br><span class="line">   <span class="number">2.29908470e+05</span> <span class="number">-4.46826954e+04</span>]]</span><br><span class="line">Coefficients from &lt;class 'sklearn.linear_model.ridge.Ridge'&gt; with alpha = 1e-05</span><br><span class="line">[[  <span class="number">0.78077474</span>   <span class="number">2.17580376</span> <span class="number">-15.03623374</span>  <span class="number">26.89195816</span>  <span class="number">-5.6119261</span></span><br><span class="line">  <span class="number">-23.29096036</span>  <span class="number">-4.96673855</span>  <span class="number">21.51691618</span>  <span class="number">22.82230017</span> <span class="number">-24.96631119</span>]]</span><br><span class="line">Coefficients from &lt;class 'sklearn.linear_model.ridge.Ridge'&gt; with alpha = 1</span><br><span class="line">[[ <span class="number">0.32396213</span>  <span class="number">0.04946447</span>  <span class="number">0.02169417</span>  <span class="number">0.04508521</span>  <span class="number">0.05805455</span>  <span class="number">0.04884026</span></span><br><span class="line">   <span class="number">0.01942467</span> <span class="number">-0.02537665</span> <span class="number">-0.08092134</span> <span class="number">-0.14346268</span>]]</span><br></pre></td></tr></table></figure>

<p><img src="output_104_1.png" alt="png"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Training Errors Linear regression with Ridge [0.3850221633585592, 0.3931885655081954, 0.462567291650715]</span><br><span class="line">Validation Errors Linear regression with Ridge [0.2931679786457559, 0.3258256737772605, 0.44095925341635367]</span><br><span class="line">Training Errors Polynomial regression with Ridge [0.3252732506662376, 0.35973102994984707, 0.3788227343500468]</span><br><span class="line">Validation Errors Polinomial regression with Ridge [0.28745132128253525, 0.2921066809021208, 0.2793681709990138]</span><br></pre></td></tr></table></figure>

<p>Ridge regression improves fit of complex polynomial model and worsens the fit of simple linear model.</p>
<h1 id="Lasso-Regression"><a href="#Lasso-Regression" class="headerlink" title="Lasso Regression"></a>Lasso Regression</h1><p>Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the $l_1$ norm of the weight vector instead of half the square of the $l_2$ norm.</p>
<p>$$ J(\theta) = MSE(\theta) + \alpha \frac{1}{2} \sum^m_{i=1} |\theta_i|$$</p>
<p>An important characteristic of Lasso Regression is that it tends to completely eliminate the weights of the<br>least important features (i.e., set them to zero). For example, the dashed line in the right plot on a figure below (with $\alpha$ = $10^{-7}$) looks quadratic, almost linear: all the weights for the high-degree polynomial features are equal to zero. In other words, Lasso Regression automatically performs feature selection and outputs a sparse model (i.e., with few nonzero feature weights).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line">train_errors = []</span><br><span class="line">val_errors = []</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">4</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">plot_model(Lasso, polynomial=<span class="literal">False</span>, alphas=(<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">1</span>), random_state=<span class="number">42</span>)</span><br><span class="line">plt.ylabel(<span class="string">"$y$"</span>, rotation=<span class="number">0</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">plot_model(Lasso, polynomial=<span class="literal">True</span>, alphas=(<span class="number">0</span>, <span class="number">0.02</span>, <span class="number">1</span>), tol=<span class="number">1</span>, random_state=<span class="number">42</span>)</span><br><span class="line">plt.show()</span><br><span class="line">print(<span class="string">"Training Errors Linear regression with Lasso"</span>,train_errors[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"Validation Errors Linear regression with Lasso"</span>,val_errors[<span class="number">0</span>:<span class="number">3</span>])</span><br><span class="line">print(<span class="string">"Training Errors Polynomial regression with Lasso"</span>,train_errors[<span class="number">3</span>:])</span><br><span class="line">print(<span class="string">"Validation Errors Polinomial regression with Lasso"</span>,val_errors[<span class="number">3</span>:])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Coefficients from &lt;class &apos;sklearn.linear_model.coordinate_descent.Lasso&apos;&gt; with alpha = 0</span><br><span class="line">    [[0.4176916]]</span><br><span class="line">    Coefficients from &lt;class &apos;sklearn.linear_model.coordinate_descent.Lasso&apos;&gt; with alpha = 0.1</span><br><span class="line">    [0.28473922]</span><br><span class="line">    Coefficients from &lt;class &apos;sklearn.linear_model.coordinate_descent.Lasso&apos;&gt; with alpha = 1</span><br><span class="line">    [0.]</span><br><span class="line">    Coefficients from &lt;class &apos;sklearn.linear_model.coordinate_descent.Lasso&apos;&gt; with alpha = 0</span><br><span class="line">    [[ 2.79676282e+01 -8.70301875e+02  1.02393993e+04 -6.14861326e+04</span><br><span class="line">       2.14563424e+05 -4.60902057e+05  6.18449227e+05 -5.05246948e+05</span><br><span class="line">       2.29908470e+05 -4.46826954e+04]]</span><br><span class="line">    Coefficients from &lt;class &apos;sklearn.linear_model.coordinate_descent.Lasso&apos;&gt; with alpha = 0.02</span><br><span class="line">    [ 0.3569844   0.         -0.         -0.         -0.         -0.</span><br><span class="line">     -0.00764842 -0.00799212 -0.00746378 -0.00698341]</span><br><span class="line">Coefficients from &lt;class &apos;sklearn.linear_model.coordinate_descent.Lasso&apos;&gt; with alpha = 1</span><br><span class="line">    [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</span><br></pre></td></tr></table></figure>

<p><img src="output_107_1.png" alt="png"></p>
<p>Training Errors Linear regression with Lasso [0.3850221633585592, 0.3983174004851076, 0.5162468019225317]<br>Validation Errors Linear regression with Lasso [0.2931679786457559, 0.3374260613813014, 0.5127463561383779]<br>Training Errors Polynomial regression with Lasso [0.3252732506662376, 0.383267741215846, 0.5162468019225317]<br>Validation Errors Polinomial regression with Lasso [0.28745132128253525, 0.30262353461319985, 0.5127463561383779]</p>
<p>In our example Lasso regression worsens the fit.</p>
<p>For example, the dashed line in the right plot on (with $\alpha$ = 0.02) looks almost linear: it is not completely zero, there are positive weights on $X^8$, $X^9$, and $X^10$. In other words, Lasso Regression automatically performs feature selection and outputs asparse model (i.e., with few nonzero feature weights). <br></p>
<h1 id="Elastic-net"><a href="#Elastic-net" class="headerlink" title="Elastic net"></a>Elastic net</h1><p>Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term<br>is a simple mix of both Ridge and Lasso’s regularization terms, and you can control the mix ratio $r$. When<br>$r = 0$, Elastic Net is equivalent to Ridge Regression, and when $r = 1$, it is equivalent to Lasso Regression.</p>
<p>$$ J(\theta) = MSE(\theta) + r \alpha \frac{1}{2} \sum^m_{i=1} |\theta_i| + \frac{1-r}{2} \sum^m_{i=1} \theta_i^2 $$</p>
<p>So when should you use Linear Regression, Ridge, Lasso, or Elastic Net? It is almost always preferable<br>to have at least a little bit of regularization, so generally you should avoid plain Linear Regression.</p>
<ul>
<li>Ridge is a good default,</li>
<li>if you suspect that only a few features are actually useful, you should prefer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to zero.</li>
<li>In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">warn</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.warn = warn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">m = <span class="number">70</span></span><br><span class="line">X = <span class="number">3</span> * np.random.rand(m, <span class="number">1</span>)</span><br><span class="line">y = <span class="number">1</span> + <span class="number">0.5</span> * X + np.random.randn(m, <span class="number">1</span>) / <span class="number">1.5</span></span><br><span class="line">poly_features = PolynomialFeatures(degree=<span class="number">10</span>, include_bias=<span class="literal">False</span>)</span><br><span class="line">X_poly = poly_features.fit_transform(X)</span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_poly_std = scaler.fit_transform(X_poly)</span><br><span class="line">valvec = []</span><br><span class="line">elnet = ElasticNet(alpha = <span class="number">0.1</span>, l1_ratio = <span class="number">0.5</span>, max_iter=<span class="number">1000000</span> )</span><br><span class="line">cores = -cross_val_score(elnet, X_poly_std, y, cv=<span class="number">5</span>, scoring=<span class="string">'neg_mean_squared_error'</span>)</span><br><span class="line">alphavec = np.arange(<span class="number">0</span>, <span class="number">1</span>, <span class="number">0.01</span>)</span><br><span class="line">l1ratios = [<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.7</span>, <span class="number">1</span>]</span><br><span class="line"><span class="keyword">for</span> l1rat, style <span class="keyword">in</span> zip(l1ratios, (<span class="string">"b"</span>, <span class="string">"g"</span>, <span class="string">"r"</span>,<span class="string">"k"</span>, <span class="string">"m"</span>)):</span><br><span class="line">    mean_score = []</span><br><span class="line">    min_score = <span class="number">100</span></span><br><span class="line">    <span class="keyword">for</span> alpha <span class="keyword">in</span> alphavec:</span><br><span class="line">        elnet = ElasticNet(alpha = alpha, l1_ratio = l1rat,  normalize=<span class="literal">True</span>) <span class="keyword">if</span> alpha &gt; <span class="number">0</span> <span class="keyword">else</span> LinearRegression()</span><br><span class="line">        score = -cross_val_score(elnet, X_poly, y, cv=<span class="number">5</span>, scoring=<span class="string">'neg_mean_squared_error'</span>)</span><br><span class="line">        mean_score.append(np.mean(score))</span><br><span class="line">        <span class="keyword">if</span> np.mean(score) &lt; min_score:</span><br><span class="line">            min_score = np.mean(score)</span><br><span class="line">            best_alpha = alpha</span><br><span class="line">            best_l1 = l1rat</span><br><span class="line">    plt.plot(alphavec, mean_score, style, label=<span class="string">r"l1_ratio = &#123;&#125;"</span>.format(l1rat))</span><br><span class="line">plt.legend(loc=<span class="string">"bottom right"</span>, fontsize=<span class="number">10</span>)</span><br><span class="line">plt.xlabel(<span class="string">"$x_1$"</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">print(<span class="string">"best model has alpha ="</span>, best_alpha, <span class="string">"l1_ratio ="</span> ,l1rat)</span><br></pre></td></tr></table></figure>

<p>best model has alpha = 0.01 l1_ratio = 1</p>
<p><img src="output_111_1.png" alt="png"></p>
<h1 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h1><p>A very different way to regularize iterative learning algorithms such as Gradient Descent is to stop training as soon as the validation error reaches a minimum. This is called early stopping. Figure next shows a complex model (in this case a high-degree Polynomial Regression model) being trained using Batch Gradient Descent. As the epochs go by, the algorithm learns and its prediction error (RMSE) on the training set naturally goes down, and so does its prediction error on the validation set. However, after a while the validation error stops decreasing and actually starts to go back up. This indicates that the model has started to overfit the training data. With early stopping you just stop training as soon as the validation error reaches the minimum. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">m = <span class="number">100</span></span><br><span class="line">X = <span class="number">6</span> * np.random.rand(m, <span class="number">1</span>) - <span class="number">3</span></span><br><span class="line">y = <span class="number">2</span> + X + <span class="number">0.5</span> * X**<span class="number">2</span> + np.random.randn(m, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X_train, X_val, y_train, y_val = train_test_split(X[:<span class="number">50</span>], y[:<span class="number">50</span>].ravel(), test_size=<span class="number">0.5</span>, random_state=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">poly_scaler = Pipeline([</span><br><span class="line">        (<span class="string">"poly_features"</span>, PolynomialFeatures(degree=<span class="number">90</span>, include_bias=<span class="literal">False</span>)),</span><br><span class="line">        (<span class="string">"std_scaler"</span>, StandardScaler()),</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">X_train_poly_scaled = poly_scaler.fit_transform(X_train)</span><br><span class="line">X_val_poly_scaled = poly_scaler.transform(X_val)</span><br><span class="line"></span><br><span class="line">sgd_reg = SGDRegressor(max_iter=<span class="number">1</span>,</span><br><span class="line">                       penalty=<span class="literal">None</span>,</span><br><span class="line">                       eta0=<span class="number">0.0005</span>,</span><br><span class="line">                       warm_start=<span class="literal">True</span>,</span><br><span class="line">                       learning_rate=<span class="string">"constant"</span>,</span><br><span class="line">                       random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">n_epochs = <span class="number">500</span></span><br><span class="line">train_errors, val_errors = [], []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">    sgd_reg.fit(X_train_poly_scaled, y_train)</span><br><span class="line">    y_train_predict = sgd_reg.predict(X_train_poly_scaled)</span><br><span class="line">    y_val_predict = sgd_reg.predict(X_val_poly_scaled)</span><br><span class="line">    train_errors.append(mean_squared_error(y_train, y_train_predict))</span><br><span class="line">    val_errors.append(mean_squared_error(y_val, y_val_predict))</span><br><span class="line"></span><br><span class="line">best_epoch = np.argmin(val_errors)</span><br><span class="line">best_val_rmse = np.sqrt(val_errors[best_epoch])</span><br><span class="line"></span><br><span class="line">plt.annotate(<span class="string">'Best model'</span>,</span><br><span class="line">             xy=(best_epoch, best_val_rmse),</span><br><span class="line">             xytext=(best_epoch, best_val_rmse + <span class="number">1</span>),</span><br><span class="line">             ha=<span class="string">"center"</span>,</span><br><span class="line">             arrowprops=dict(facecolor=<span class="string">'black'</span>, shrink=<span class="number">0.05</span>),</span><br><span class="line">             fontsize=<span class="number">16</span>,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">best_val_rmse -= <span class="number">0.03</span>  <span class="comment"># just to make the graph look better</span></span><br><span class="line">plt.plot([<span class="number">0</span>, n_epochs], [best_val_rmse, best_val_rmse], <span class="string">"k:"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.plot(np.sqrt(val_errors), <span class="string">"b-"</span>, linewidth=<span class="number">3</span>, label=<span class="string">"Validation set"</span>)</span><br><span class="line">plt.plot(np.sqrt(train_errors), <span class="string">"r--"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Training set"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper right"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Epoch"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">"RMSE"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_114_0.png" alt="png"></p>
<h1 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h1><p>Some regression algorithms can be used for classification as well (and vice versa). Logistic Regression (also called Logit Regression) is commonly used to estimate the<br>probability that an instance belongs to a particular class (e.g., what is the probability that this email is<br>spam?). If the estimated probability is greater than 50%, then the model predicts that the instance belongs<br>to that class (called the positive class, labeled “1”), or else it predicts that it does not (i.e., it belongs to<br>the negative class, labeled “0”). This makes it a binary classifier</p>
<p>$$ \hat{p} = h_\theta (x) = \sigma(\theta^T \cdot x)$$<br>Logit function is:<br>$$ \sigma(t) = \frac{1}{1 + exp(-t)}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">t = np.linspace(<span class="number">-10</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">sig = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-t))</span><br><span class="line">plt.figure(figsize=(<span class="number">9</span>, <span class="number">3</span>))</span><br><span class="line">plt.plot([<span class="number">-10</span>, <span class="number">10</span>], [<span class="number">0</span>, <span class="number">0</span>], <span class="string">"k-"</span>)</span><br><span class="line">plt.plot([<span class="number">-10</span>, <span class="number">10</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>], <span class="string">"k:"</span>)</span><br><span class="line">plt.plot([<span class="number">-10</span>, <span class="number">10</span>], [<span class="number">1</span>, <span class="number">1</span>], <span class="string">"k:"</span>)</span><br><span class="line">plt.plot([<span class="number">0</span>, <span class="number">0</span>], [<span class="number">-1.1</span>, <span class="number">1.1</span>], <span class="string">"k-"</span>)</span><br><span class="line">plt.plot(t, sig, <span class="string">"b-"</span>, linewidth=<span class="number">2</span>, label=<span class="string">r"$\sigma(t) = \frac&#123;1&#125;&#123;1 + e^&#123;-t&#125;&#125;$"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"t"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.axis([<span class="number">-10</span>, <span class="number">10</span>, <span class="number">-0.1</span>, <span class="number">1.1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_117_0.png" alt="png"></p>
<p>if $\hat{p} &lt; 0.5$ then $\hat{y} = 0$ <br><br>if $\hat{p} &gt; 0.5$ then $\hat{y} = 1$</p>
<h1 id="Training-and-cost-function"><a href="#Training-and-cost-function" class="headerlink" title="Training and cost function"></a>Training and cost function</h1><p>The objective of training is to set the parameter vector $\theta$ so that the model estimates high probabilities for positive instances ($y = 1$) and low probabilities for negative instances ($y = 0$). This idea is captured by the cost function shown:?<br>$$<br>c(\theta) = \begin{cases}<br> -log(\hat{p})  &amp; \text{if } y = 1\<br> -log(1 - \hat{p})  &amp; \text{if } y = 0<br> \end{cases}<br>$$</p>
<p>$$<br>$-log(\hat{p})$ grows very large as $\hat{p}$ approaches $0$, and  $-log(1 - \hat{p})$ grows very large as  $\hat{p}$ approaches 1. The cost of the training set is the sum of the costs function for all instances:<br>$$</p>
<p>$$<br>J(\theta) = - \frac{1}{m} \sum^m_{i=1}[y^{(i)}log(\hat{p}^{(i)}) + (1 - y^{(i)})log(1-\hat{p}^{(i)})]<br>$$<br>There is no closed form solution. However, the gradient is convex, so we can find global minimum. The partial deriviatives of the cost function with regards to the jth model parameters $\theta_j$ is:<br>$$<br>\frac{\partial J(\theta)}{\partial \theta} =  \frac{1}{m} \sum^m_{i=1})(\sigma(\theta^T \cdot x^{(i)}) - y^{(i)})x^{(i)}_j<br>$$<br>After we have vector of partial deriviatives we can use it in the Batch Gradient Descent<br>algorithm, or mini-Batch or Stochastic.</p>
<h1 id="Decisions-Boundaries"><a href="#Decisions-Boundaries" class="headerlink" title="Decisions Boundaries"></a>Decisions Boundaries</h1><p>Let’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that contains the sepal<br>and petal length and width of 150 iris flowers of three different species: Iris-Setosa, Iris-Versicolor, and<br>Iris-Virginica. We will build a classifier to detect the Iris-Virginica type based only on the petal width feature. First<br>let’s load the data:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">list(iris.keys())</span><br></pre></td></tr></table></figure>

<p>[‘data’, ‘target’, ‘target_names’, ‘DESCR’, ‘feature_names’, ‘filename’]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(iris.DESCR)</span><br></pre></td></tr></table></figure>

<h1 id="Iris-plants-dataset"><a href="#Iris-plants-dataset" class="headerlink" title="Iris plants dataset"></a>Iris plants dataset</h1><hr>
<p><strong>Data Set Characteristics:</strong></p>
<p>:Number of Instances: 150 (50 in each of three classes)<br>:Number of Attributes: 4 numeric, predictive attributes and the class<br>:Attribute Information:</p>
<ul>
<li>sepal length in cm</li>
<li>sepal width in cm</li>
<li>petal length in cm</li>
<li>petal width in cm</li>
<li>class:<ul>
<li>Iris-Setosa</li>
<li>Iris-Versicolour</li>
<li>Iris-Virginica</li>
</ul>
</li>
</ul>
<p>:Summary Statistics:</p>
<p>============== ==== ==== ======= ===== ====================<br>        Min  Max   Mean    SD   Class Correlation<br>============== ==== ==== ======= ===== ====================<br>pal length:   4.3  7.9   5.84   0.83    0.7826<br>sepal width:    2.0  4.4   3.05   0.43   -0.4194<br>petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)<br>petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)<br>============== ==== ==== ======= ===== ====================</p>
<p>:Missing Attribute Values: None<br>:Class Distribution: 33.3% for each of 3 classes.<br>:Creator: R.A. Fisher<br>:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)<br>:Date: July, 1988</p>
<p>The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken<br>from Fisher’s paper. Note that it’s the same as in R, but not as in the UCI<br>Machine Learning Repository, which has two wrong data points.</p>
<p>This is perhaps the best known database to be found in the attern recognition literature.  Fisher’s paper is a classic in the field and is referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.  One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># Set one X - petal width</span></span><br><span class="line">X = iris[<span class="string">"data"</span>][:, <span class="number">3</span>:]  <span class="comment"># petal width</span></span><br><span class="line"><span class="comment"># Y is the classification of Iris-Virginica. </span></span><br><span class="line">y = (iris[<span class="string">"target"</span>] == <span class="number">2</span>).astype(np.int)  <span class="comment"># 1 if Iris-Virginica, else 0</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">log_reg = LogisticRegression(random_state=<span class="number">42</span>)</span><br><span class="line">log_reg.fit(X, y)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">LogisticRegression(C=<span class="number">1.0</span>, class_weight=<span class="literal">None</span>, dual=<span class="literal">False</span>, fit_intercept=<span class="literal">True</span>,</span><br><span class="line">                   intercept_scaling=<span class="number">1</span>, l1_ratio=<span class="literal">None</span>, max_iter=<span class="number">100</span>,</span><br><span class="line">                   multi_class=<span class="string">'warn'</span>, n_jobs=<span class="literal">None</span>, penalty=<span class="string">'l2'</span>,</span><br><span class="line">                   random_state=<span class="number">42</span>, solver=<span class="string">'warn'</span>, tol=<span class="number">0.0001</span>, verbose=<span class="number">0</span>,</span><br><span class="line">                   warm_start=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Green is the probability of Iris-Virginica, blue is the probability of Not Iris-Virginica. </span></span><br><span class="line">X_new = np.linspace(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1000</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">y_proba = log_reg.predict_proba(X_new)</span><br><span class="line"></span><br><span class="line">plt.plot(X_new, y_proba[:, <span class="number">1</span>], <span class="string">"g-"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Iris-Virginica"</span>)</span><br><span class="line">plt.plot(X_new, y_proba[:, <span class="number">0</span>], <span class="string">"b--"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Not Iris-Virginica"</span>)</span><br></pre></td></tr></table></figure>

<p><img src="output_126_1.png" alt="png"></p>
<p>The figure in the book actually is actually a bit fancier:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">X_new = np.linspace(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1000</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">y_proba = log_reg.predict_proba(X_new)</span><br><span class="line">decision_boundary = X_new[y_proba[:, <span class="number">1</span>] &gt;= <span class="number">0.5</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">3</span>))</span><br><span class="line">plt.plot(X[y==<span class="number">0</span>], y[y==<span class="number">0</span>], <span class="string">"bs"</span>)</span><br><span class="line">plt.plot(X[y==<span class="number">1</span>], y[y==<span class="number">1</span>], <span class="string">"g^"</span>)</span><br><span class="line">plt.plot([decision_boundary, decision_boundary], [<span class="number">-1</span>, <span class="number">2</span>], <span class="string">"k:"</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.plot(X_new, y_proba[:, <span class="number">1</span>], <span class="string">"g-"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Iris-Virginica"</span>)</span><br><span class="line">plt.plot(X_new, y_proba[:, <span class="number">0</span>], <span class="string">"b--"</span>, linewidth=<span class="number">2</span>, label=<span class="string">"Not Iris-Virginica"</span>)</span><br><span class="line">plt.text(decision_boundary+<span class="number">0.02</span>, <span class="number">0.15</span>, <span class="string">"Decision  boundary"</span>, fontsize=<span class="number">14</span>, color=<span class="string">"k"</span>, ha=<span class="string">"center"</span>)</span><br><span class="line">plt.arrow(decision_boundary, <span class="number">0.08</span>, <span class="number">-0.3</span>, <span class="number">0</span>, head_width=<span class="number">0.05</span>, head_length=<span class="number">0.1</span>, fc=<span class="string">'b'</span>, ec=<span class="string">'b'</span>)</span><br><span class="line">plt.arrow(decision_boundary, <span class="number">0.92</span>, <span class="number">0.3</span>, <span class="number">0</span>, head_width=<span class="number">0.05</span>, head_length=<span class="number">0.1</span>, fc=<span class="string">'g'</span>, ec=<span class="string">'g'</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Petal width (cm)"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Probability"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"center left"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">3</span>, <span class="number">-0.02</span>, <span class="number">1.02</span>])</span><br><span class="line">save_fig(<span class="string">"logistic_regression_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_128_1.png" alt="png"></p>
<p>The petal width of Iris-Virginica flowers (represented by triangles) ranges from 1.4 cm to 2.5 cm, while the other iris flowers (represented by squares) generally have a smaller petal width, ranging from 0.1 cm to 1.8 cm. Notice that there is a bit of overlap. Above about 2 cm the classifier is highly confident that the flower is an Iris-Virginica (it outputs a high probability to that class), while below 1 cm it is highly confident that it is not an Iris-Virginica (high probability for the “Not Iris-Virginica” class). In between these extremes, the classifier is unsure. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 50% probability is at</span></span><br><span class="line">decision_boundary</span><br></pre></td></tr></table></figure>

<pre><code>array([1.61561562])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Predict with sepal lenths of 1.7 and 1.5.</span></span><br><span class="line">log_reg.predict([[<span class="number">1.7</span>], [<span class="number">1.5</span>]])</span><br></pre></td></tr></table></figure>

<pre><code>array([1, 0])</code></pre><p>Next we will show the same dataset but this time displaying two features: petal width and length. Once trained, the Logistic Regression classifier can estimate the probability that a new flower is an Iris-Virginica based on these two features. The dashed line represents the points where the model estimates a 50% probability: this is the model’s decision boundary.  Each parallel line represents the points where the model outputs a specific probability, from 15% (bottom left) to 90% (top right). All the flowers beyond the top-right line have an over 90% chance of being Iris-Virginica according to the model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">X = iris[<span class="string">"data"</span>][:, (<span class="number">2</span>, <span class="number">3</span>)]  <span class="comment"># petal length, petal width</span></span><br><span class="line">y = (iris[<span class="string">"target"</span>] == <span class="number">2</span>).astype(np.int)</span><br><span class="line"><span class="comment"># C is the regularization parameter, we will cover it more in the next lecture</span></span><br><span class="line">log_reg = LogisticRegression(C=<span class="number">10</span>**<span class="number">10</span>, random_state=<span class="number">42</span>)</span><br><span class="line">log_reg.fit(X, y)</span><br><span class="line"></span><br><span class="line">x0, x1 = np.meshgrid(</span><br><span class="line">        np.linspace(<span class="number">2.9</span>, <span class="number">7</span>, <span class="number">500</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">        np.linspace(<span class="number">0.8</span>, <span class="number">2.7</span>, <span class="number">200</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">    )</span><br><span class="line">X_new = np.c_[x0.ravel(), x1.ravel()]</span><br><span class="line"></span><br><span class="line">y_proba = log_reg.predict_proba(X_new)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>], <span class="string">"bs"</span>)</span><br><span class="line">plt.plot(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>], <span class="string">"g^"</span>)</span><br><span class="line"></span><br><span class="line">zz = y_proba[:, <span class="number">1</span>].reshape(x0.shape)</span><br><span class="line">contour = plt.contour(x0, x1, zz, cmap=plt.cm.brg)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">left_right = np.array([<span class="number">2.9</span>, <span class="number">7</span>])</span><br><span class="line">boundary = -(log_reg.coef_[<span class="number">0</span>][<span class="number">0</span>] * left_right + log_reg.intercept_[<span class="number">0</span>]) / log_reg.coef_[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">plt.clabel(contour, inline=<span class="number">1</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.plot(left_right, boundary, <span class="string">"k--"</span>, linewidth=<span class="number">3</span>)</span><br><span class="line">plt.text(<span class="number">3.5</span>, <span class="number">1.5</span>, <span class="string">"Not Iris-Virginica"</span>, fontsize=<span class="number">14</span>, color=<span class="string">"b"</span>, ha=<span class="string">"center"</span>)</span><br><span class="line">plt.text(<span class="number">6.5</span>, <span class="number">2.3</span>, <span class="string">"Iris-Virginica"</span>, fontsize=<span class="number">14</span>, color=<span class="string">"g"</span>, ha=<span class="string">"center"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Petal length"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Petal width"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.axis([<span class="number">2.9</span>, <span class="number">7</span>, <span class="number">0.8</span>, <span class="number">2.7</span>])</span><br><span class="line">save_fig(<span class="string">"logistic_regression_contour_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_133_1.png" alt="png"></p>
<h1 id="Multinomial-Logit-Regression"><a href="#Multinomial-Logit-Regression" class="headerlink" title="Multinomial Logit Regression"></a>Multinomial Logit Regression</h1><p>The Logistic Regression model can be generalized to support multiple classes directly, without having to<br>train and combine multiple binary classifiers. This is called Softmax Regression, or Multinomial Logistic Regression.<br>For each instance x the model first computes a score $sk(x)$ for each class $k$, then estimates the probability of each class by applying the softmax function (also called the normalized exponential) to the scores:<br>$$<br>s_k(x) = \theta^T_k \cdot x<br>$$<br>Each class $k$ has it’s own vector of parameters $\theta_k$. For each instance $x$ we compute a probability $\hat{p}<em>k$ of belonging to class $k$. The probability is the ratio of the score $k$ and the sum of score of all other classes:<br>$$<br>\hat{p}_k = \sigma(s(x))_k = \frac{exp(s_k(x))}{\sum</em>{j=1}^K exp(s_j(x))}<br>$$<br>Multinomial logit predict class $k$ according to the highest probability. In the estimation of multinomial logit we minimize a cost function similar to the binomial logit. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># now we estimate multinomial logit using three classes:</span></span><br><span class="line">X = iris[<span class="string">"data"</span>][:, (<span class="number">2</span>, <span class="number">3</span>)]  <span class="comment"># petal length, petal width</span></span><br><span class="line">y = iris[<span class="string">"target"</span>]</span><br><span class="line"></span><br><span class="line">softmax_reg = LogisticRegression(multi_class=<span class="string">"multinomial"</span>,solver=<span class="string">"lbfgs"</span>, C=<span class="number">10</span>, random_state=<span class="number">42</span>)</span><br><span class="line">softmax_reg.fit(X, y)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">LogisticRegression(C=<span class="number">10</span>, class_weight=<span class="literal">None</span>, dual=<span class="literal">False</span>, fit_intercept=<span class="literal">True</span>,</span><br><span class="line">    intercept_scaling=<span class="number">1</span>, l1_ratio=<span class="literal">None</span>, max_iter=<span class="number">100</span>,</span><br><span class="line">    multi_class=<span class="string">'multinomial'</span>, n_jobs=<span class="literal">None</span>, penalty=<span class="string">'l2'</span>,</span><br><span class="line">    random_state=<span class="number">42</span>, solver=<span class="string">'lbfgs'</span>, tol=<span class="number">0.0001</span>, verbose=<span class="number">0</span>,</span><br><span class="line">    warm_start=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">x0, x1 = np.meshgrid(</span><br><span class="line">        np.linspace(<span class="number">0</span>, <span class="number">8</span>, <span class="number">500</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">        np.linspace(<span class="number">0</span>, <span class="number">3.5</span>, <span class="number">200</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">    )</span><br><span class="line">X_new = np.c_[x0.ravel(), x1.ravel()]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y_proba = softmax_reg.predict_proba(X_new)</span><br><span class="line">y_predict = softmax_reg.predict(X_new)</span><br><span class="line">	</span><br><span class="line">zz1 = y_proba[:, <span class="number">1</span>].reshape(x0.shape)</span><br><span class="line">zz = y_predict.reshape(x0.shape)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot(X[y==<span class="number">2</span>, <span class="number">0</span>], X[y==<span class="number">2</span>, <span class="number">1</span>], <span class="string">"g^"</span>, label=<span class="string">"Iris-Virginica"</span>)</span><br><span class="line">plt.plot(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>], <span class="string">"bs"</span>, label=<span class="string">"Iris-Versicolor"</span>)</span><br><span class="line">plt.plot(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>], <span class="string">"yo"</span>, label=<span class="string">"Iris-Setosa"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line">custom_cmap = ListedColormap([<span class="string">'#fafab0'</span>,<span class="string">'#9898ff'</span>,<span class="string">'#a0faa0'</span>])</span><br><span class="line"></span><br><span class="line">plt.contourf(x0, x1, zz, cmap=custom_cmap)</span><br><span class="line">contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)</span><br><span class="line">plt.clabel(contour, inline=<span class="number">1</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Petal length"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Petal width"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"center left"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">3.5</span>])</span><br><span class="line">save_fig(<span class="string">"softmax_regression_contour_plot"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_136_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">softmax_reg.predict([[<span class="number">5</span>, <span class="number">2</span>]])</span><br></pre></td></tr></table></figure>

<p>array([2])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">softmax_reg.predict_proba([[<span class="number">5</span>, <span class="number">2</span>]])</span><br></pre></td></tr></table></figure>

<p>array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])</p>
<h1 id="Exercise-solutions"><a href="#Exercise-solutions" class="headerlink" title="Exercise solutions"></a>Exercise solutions</h1><h2 id="1-to-11"><a href="#1-to-11" class="headerlink" title="1. to 11."></a>1. to 11.</h2><p>See appendix A.</p>
<h2 id="12-Batch-Gradient-Descent-with-early-stopping-for-Softmax-Regression"><a href="#12-Batch-Gradient-Descent-with-early-stopping-for-Softmax-Regression" class="headerlink" title="12. Batch Gradient Descent with early stopping for Softmax Regression"></a>12. Batch Gradient Descent with early stopping for Softmax Regression</h2><p>(without using Scikit-Learn)</p>
<p>Let’s start by loading the data. We will just reuse the Iris dataset we loaded earlier.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = iris[<span class="string">"data"</span>][:, (<span class="number">2</span>, <span class="number">3</span>)]  <span class="comment"># petal length, petal width</span></span><br><span class="line">y = iris[<span class="string">"target"</span>]</span><br></pre></td></tr></table></figure>

<p>We need to add the bias term for every instance ($x_0 = 1$):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_with_bias = np.c_[np.ones([len(X), <span class="number">1</span>]), X]</span><br></pre></td></tr></table></figure>

<p>And let’s set the random seed so the output of this exercise solution is reproducible:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">2042</span>)</span><br></pre></td></tr></table></figure>

<p>The easiest option to split the dataset into a training set, a validation set and a test set would be to use Scikit-Learn’s <code>train_test_split()</code> function, but the point of this exercise is to try understand the algorithms by implementing them manually. So here is one possible implementation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">test_ratio = <span class="number">0.2</span></span><br><span class="line">validation_ratio = <span class="number">0.2</span></span><br><span class="line">total_size = len(X_with_bias)</span><br><span class="line"></span><br><span class="line">test_size = int(total_size * test_ratio)</span><br><span class="line">validation_size = int(total_size * validation_ratio)</span><br><span class="line">train_size = total_size - test_size - validation_size</span><br><span class="line"></span><br><span class="line">rnd_indices = np.random.permutation(total_size)</span><br><span class="line"></span><br><span class="line">X_train = X_with_bias[rnd_indices[:train_size]]</span><br><span class="line">y_train = y[rnd_indices[:train_size]]</span><br><span class="line">X_valid = X_with_bias[rnd_indices[train_size:-test_size]]</span><br><span class="line">y_valid = y[rnd_indices[train_size:-test_size]]</span><br><span class="line">X_test = X_with_bias[rnd_indices[-test_size:]]</span><br><span class="line">y_test = y[rnd_indices[-test_size:]]</span><br></pre></td></tr></table></figure>

<p>The targets are currently class indices (0, 1 or 2), but we need target class probabilities to train the Softmax Regression model. Each instance will have target class probabilities equal to 0.0 for all classes except for the target class which will have a probability of 1.0 (in other words, the vector of class probabilities for ay given instance is a one-hot vector). Let’s write a small function to convert the vector of class indices into a matrix containing a one-hot vector for each instance:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_one_hot</span><span class="params">(y)</span>:</span></span><br><span class="line">    n_classes = y.max() + <span class="number">1</span></span><br><span class="line">    m = len(y)</span><br><span class="line">    Y_one_hot = np.zeros((m, n_classes))</span><br><span class="line">    Y_one_hot[np.arange(m), y] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> Y_one_hot</span><br></pre></td></tr></table></figure>

<p>Let’s test this function on the first 10 instances:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_train[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>

<p>array([0, 1, 2, 1, 1, 0, 1, 1, 1, 0])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">to_one_hot(y_train[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<p>array([[1., 0., 0.],<br>   [0., 1., 0.],<br>   [0., 0., 1.],<br>   [0., 1., 0.],<br>   [0., 1., 0.],<br>   [1., 0., 0.],<br>   [0., 1., 0.],<br>   [0., 1., 0.],<br>   [0., 1., 0.],<br>   [1., 0., 0.]])</p>
<p>Looks good, so let’s create the target class probabilities matrix for the training set and the test set:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Y_train_one_hot = to_one_hot(y_train)</span><br><span class="line">Y_valid_one_hot = to_one_hot(y_valid)</span><br><span class="line">Y_test_one_hot = to_one_hot(y_test)</span><br></pre></td></tr></table></figure>

<p>Now let’s implement the Softmax function. Recall that it is defined by the following equation:<br>$$<br>\sigma\left(\mathbf{s}(\mathbf{x})\right)<em>k = \dfrac{\exp\left(s_k(\mathbf{x})\right)}{\sum\limits</em>{j=1}^{K}{\exp\left(s_j(\mathbf{x})\right)}}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(logits)</span>:</span></span><br><span class="line">    exps = np.exp(logits)</span><br><span class="line">    exp_sums = np.sum(exps, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> exps / exp_sums</span><br></pre></td></tr></table></figure>

<p>We are almost ready to start training. Let’s define the number of inputs and outputs:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n_inputs = X_train.shape[<span class="number">1</span>] <span class="comment"># == 3 (2 features plus the bias term)</span></span><br><span class="line">n_outputs = len(np.unique(y_train))   <span class="comment"># == 3 (3 iris classes)</span></span><br></pre></td></tr></table></figure>

<p>Now here comes the hardest part: training! Theoretically, it’s simple: it’s just a matter of translating the math equations into Python code. But in practice, it can be quite tricky: in particular, it’s easy to mix up the order of the terms, or the indices. You can even end up with code that looks like it’s working but is actually not computing exactly the right thing. When unsure, you should write down the shape of each term in the equation and make sure the corresponding terms in your code match closely. It can also help to evaluate each term independently and print them out. The good news it that you won’t have to do this everyday, since all this is well implemented by Scikit-Learn, but it will help you understand what’s going on under the hood.</p>
<p>So the equations we will need are the cost function:</p>
<p>$J(\mathbf{\Theta}) =</p>
<ul>
<li>\dfrac{1}{m}\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K}{y_k^{(i)}\log\left(\hat{p}_k^{(i)}\right)}$</li>
</ul>
<p>And the equation for the gradients:</p>
<p>$\nabla_{\mathbf{\theta}^{(k)}} , J(\mathbf{\Theta}) = \dfrac{1}{m} \sum\limits_{i=1}^{m}{ \left ( \hat{p}^{(i)}_k - y_k^{(i)} \right ) \mathbf{x}^{(i)}}$</p>
<p>Note that $\log\left(\hat{p}_k^{(i)}\right)$ may not be computable if $\hat{p}_k^{(i)} = 0$. So we will add a tiny value $\epsilon$ to $\log\left(\hat{p}_k^{(i)}\right)$ to avoid getting <code>nan</code> values.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">eta = <span class="number">0.01</span></span><br><span class="line">n_iterations = <span class="number">5001</span></span><br><span class="line">m = len(X_train)</span><br><span class="line">epsilon = <span class="number">1e-7</span></span><br><span class="line"></span><br><span class="line">Theta = np.random.randn(n_inputs, n_outputs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">    logits = X_train.dot(Theta)</span><br><span class="line">    Y_proba = softmax(logits)</span><br><span class="line">    loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=<span class="number">1</span>))</span><br><span class="line">    error = Y_proba - Y_train_one_hot</span><br><span class="line">    <span class="keyword">if</span> iteration % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        print(iteration, loss)</span><br><span class="line">    gradients = <span class="number">1</span>/m * X_train.T.dot(error)</span><br><span class="line">    Theta = Theta - eta * gradients</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span> <span class="number">5.446205811872683</span></span><br><span class="line"><span class="number">500</span> <span class="number">0.8350062641405651</span></span><br><span class="line"><span class="number">1000</span> <span class="number">0.6878801447192402</span></span><br><span class="line"><span class="number">1500</span> <span class="number">0.6012379137693313</span></span><br><span class="line"><span class="number">2000</span> <span class="number">0.5444496861981873</span></span><br><span class="line"><span class="number">2500</span> <span class="number">0.5038530181431525</span></span><br><span class="line"><span class="number">3000</span> <span class="number">0.4729228972192248</span></span><br><span class="line"><span class="number">3500</span> <span class="number">0.4482424418895776</span></span><br><span class="line"><span class="number">4000</span> <span class="number">0.4278651093928793</span></span><br><span class="line"><span class="number">4500</span> <span class="number">0.41060071429187134</span></span><br><span class="line"><span class="number">5000</span> <span class="number">0.3956780375390373</span></span><br></pre></td></tr></table></figure>

<p>And that’s it! The Softmax model is trained. Let’s look at the model parameters:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Theta</span><br><span class="line"></span><br><span class="line">    array([[ <span class="number">3.32094157</span>, <span class="number">-0.6501102</span> , <span class="number">-2.99979416</span>],</span><br><span class="line">           [<span class="number">-1.1718465</span> ,  <span class="number">0.11706172</span>,  <span class="number">0.10507543</span>],</span><br><span class="line">           [<span class="number">-0.70224261</span>, <span class="number">-0.09527802</span>,  <span class="number">1.4786383</span> ]])</span><br></pre></td></tr></table></figure>

<p>Let’s make predictions for the validation set and check the accuracy score:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">logits = X_valid.dot(Theta)</span><br><span class="line">Y_proba = softmax(logits)</span><br><span class="line">y_predict = np.argmax(Y_proba, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">accuracy_score = np.mean(y_predict == y_valid)</span><br><span class="line">accuracy_score</span><br><span class="line"></span><br><span class="line">    <span class="number">0.9666666666666667</span></span><br></pre></td></tr></table></figure>

<p>Well, this model looks pretty good. For the sake of the exercise, let’s add a bit of $\ell_2$ regularization. The following training code is similar to the one above, but the loss now has an additional $\ell_2$ penalty, and the gradients have the proper additional term (note that we don’t regularize the first element of <code>Theta</code> since this corresponds to the bias term). Also, let’s try increasing the learning rate <code>eta</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">eta = <span class="number">0.1</span></span><br><span class="line">n_iterations = <span class="number">5001</span></span><br><span class="line">m = len(X_train)</span><br><span class="line">epsilon = <span class="number">1e-7</span></span><br><span class="line">alpha = <span class="number">0.1</span>  <span class="comment"># regularization hyperparameter</span></span><br><span class="line"></span><br><span class="line">Theta = np.random.randn(n_inputs, n_outputs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">    logits = X_train.dot(Theta)</span><br><span class="line">    Y_proba = softmax(logits)</span><br><span class="line">    xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=<span class="number">1</span>))</span><br><span class="line">    l2_loss = <span class="number">1</span>/<span class="number">2</span> * np.sum(np.square(Theta[<span class="number">1</span>:]))</span><br><span class="line">    loss = xentropy_loss + alpha * l2_loss</span><br><span class="line">    error = Y_proba - Y_train_one_hot</span><br><span class="line">    <span class="keyword">if</span> iteration % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        print(iteration, loss)</span><br><span class="line">    gradients = <span class="number">1</span>/m * X_train.T.dot(error) + np.r_[np.zeros([<span class="number">1</span>, n_outputs]), alpha * Theta[<span class="number">1</span>:]]</span><br><span class="line">    Theta = Theta - eta * gradients</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="number">0</span> <span class="number">6.629842469083912</span></span><br><span class="line">    <span class="number">500</span> <span class="number">0.5339667976629506</span></span><br><span class="line">    <span class="number">1000</span> <span class="number">0.503640075014894</span></span><br><span class="line">    <span class="number">1500</span> <span class="number">0.49468910594603216</span></span><br><span class="line">    <span class="number">2000</span> <span class="number">0.4912968418075477</span></span><br><span class="line">    <span class="number">2500</span> <span class="number">0.48989924700933296</span></span><br><span class="line">    <span class="number">3000</span> <span class="number">0.48929905984511984</span></span><br><span class="line">    <span class="number">3500</span> <span class="number">0.48903512443978603</span></span><br><span class="line">    <span class="number">4000</span> <span class="number">0.4889173621830818</span></span><br><span class="line">    <span class="number">4500</span> <span class="number">0.4888643337449303</span></span><br><span class="line">    <span class="number">5000</span> <span class="number">0.4888403120738818</span></span><br></pre></td></tr></table></figure>

<p>Because of the additional $\ell_2$ penalty, the loss seems greater than earlier, but perhaps this model will perform better? Let’s find out:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">logits = X_valid.dot(Theta)</span><br><span class="line">Y_proba = softmax(logits)</span><br><span class="line">y_predict = np.argmax(Y_proba, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">accuracy_score = np.mean(y_predict == y_valid)</span><br><span class="line">accuracy_score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="number">1.0</span></span><br></pre></td></tr></table></figure>

<p>Cool, perfect accuracy! We probably just got lucky with this validation set, but still, it’s pleasant.</p>
<p>Now let’s add early stopping. For this we just need to measure the loss on the validation set at every iteration and stop when the error starts growing.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">eta = <span class="number">0.1</span> </span><br><span class="line">n_iterations = <span class="number">5001</span></span><br><span class="line">m = len(X_train)</span><br><span class="line">epsilon = <span class="number">1e-7</span></span><br><span class="line">alpha = <span class="number">0.1</span>  <span class="comment"># regularization hyperparameter</span></span><br><span class="line">best_loss = np.infty</span><br><span class="line"></span><br><span class="line">Theta = np.random.randn(n_inputs, n_outputs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">    logits = X_train.dot(Theta)</span><br><span class="line">    Y_proba = softmax(logits)</span><br><span class="line">    xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=<span class="number">1</span>))</span><br><span class="line">    l2_loss = <span class="number">1</span>/<span class="number">2</span> * np.sum(np.square(Theta[<span class="number">1</span>:]))</span><br><span class="line">    loss = xentropy_loss + alpha * l2_loss</span><br><span class="line">    error = Y_proba - Y_train_one_hot</span><br><span class="line">    gradients = <span class="number">1</span>/m * X_train.T.dot(error) + np.r_[np.zeros([<span class="number">1</span>, n_outputs]), alpha * Theta[<span class="number">1</span>:]]</span><br><span class="line">    Theta = Theta - eta * gradients</span><br><span class="line"></span><br><span class="line">    logits = X_valid.dot(Theta)</span><br><span class="line">    Y_proba = softmax(logits)</span><br><span class="line">    xentropy_loss = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon), axis=<span class="number">1</span>))</span><br><span class="line">    l2_loss = <span class="number">1</span>/<span class="number">2</span> * np.sum(np.square(Theta[<span class="number">1</span>:]))</span><br><span class="line">    loss = xentropy_loss + alpha * l2_loss</span><br><span class="line">    <span class="keyword">if</span> iteration % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">        print(iteration, loss)</span><br><span class="line">    <span class="keyword">if</span> loss &lt; best_loss:</span><br><span class="line">        best_loss = loss</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(iteration - <span class="number">1</span>, best_loss)</span><br><span class="line">        print(iteration, loss, <span class="string">"early stopping!"</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="number">0</span> <span class="number">4.7096017363419875</span></span><br><span class="line">    <span class="number">500</span> <span class="number">0.5739711987633519</span></span><br><span class="line">    <span class="number">1000</span> <span class="number">0.5435638529109127</span></span><br><span class="line">    <span class="number">1500</span> <span class="number">0.5355752782580262</span></span><br><span class="line">    <span class="number">2000</span> <span class="number">0.5331959249285544</span></span><br><span class="line">    <span class="number">2500</span> <span class="number">0.5325946767399383</span></span><br><span class="line">    <span class="number">2765</span> <span class="number">0.5325460966791898</span></span><br><span class="line">    <span class="number">2766</span> <span class="number">0.5325460971327975</span> early stopping!</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">logits = X_valid.dot(Theta)</span><br><span class="line">Y_proba = softmax(logits)</span><br><span class="line">y_predict = np.argmax(Y_proba, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">accuracy_score = np.mean(y_predict == y_valid)</span><br><span class="line">accuracy_score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="number">1.0</span></span><br></pre></td></tr></table></figure>

<p>Still perfect, but faster.</p>
<p>Now let’s plot the model’s predictions on the whole dataset:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">x0, x1 = np.meshgrid(</span><br><span class="line">        np.linspace(<span class="number">0</span>, <span class="number">8</span>, <span class="number">500</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">        np.linspace(<span class="number">0</span>, <span class="number">3.5</span>, <span class="number">200</span>).reshape(<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">    )</span><br><span class="line">X_new = np.c_[x0.ravel(), x1.ravel()]</span><br><span class="line">X_new_with_bias = np.c_[np.ones([len(X_new), <span class="number">1</span>]), X_new]</span><br><span class="line"></span><br><span class="line">logits = X_new_with_bias.dot(Theta)</span><br><span class="line">Y_proba = softmax(logits)</span><br><span class="line">y_predict = np.argmax(Y_proba, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">zz1 = Y_proba[:, <span class="number">1</span>].reshape(x0.shape)</span><br><span class="line">zz = y_predict.reshape(x0.shape)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot(X[y==<span class="number">2</span>, <span class="number">0</span>], X[y==<span class="number">2</span>, <span class="number">1</span>], <span class="string">"g^"</span>, label=<span class="string">"Iris-Virginica"</span>)</span><br><span class="line">plt.plot(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>], <span class="string">"bs"</span>, label=<span class="string">"Iris-Versicolor"</span>)</span><br><span class="line">plt.plot(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>], <span class="string">"yo"</span>, label=<span class="string">"Iris-Setosa"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line">custom_cmap = ListedColormap([<span class="string">'#fafab0'</span>,<span class="string">'#9898ff'</span>,<span class="string">'#a0faa0'</span>])</span><br><span class="line"></span><br><span class="line">plt.contourf(x0, x1, zz, cmap=custom_cmap)</span><br><span class="line">contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)</span><br><span class="line">plt.clabel(contour, inline=<span class="number">1</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Petal length"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Petal width"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">3.5</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="output_178_0.png" alt="png"></p>
<p>And now let’s measure the final model’s accuracy on the test set:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">logits = X_test.dot(Theta)</span><br><span class="line">Y_proba = softmax(logits)</span><br><span class="line">y_predict = np.argmax(Y_proba, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">accuracy_score = np.mean(y_predict == y_test)</span><br><span class="line">accuracy_score</span><br><span class="line"></span><br><span class="line">    <span class="number">0.9333333333333333</span></span><br></pre></td></tr></table></figure>

<p>Our perfect model turns out to have slight imperfections. This variability is likely due to the very small size of the dataset: depending on how you sample the training set, validation set and the test set, you can get quite different results. Try changing the random seed and running the code again a few times, you will see that the results will vary.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          
            <a href="/tags/python/" rel="tag"># python</a>
          
            <a href="/tags/Sklearn/" rel="tag"># Sklearn</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/10/01/numpy/" rel="next" title="numpy常用函数与常见问题">
                <i class="fa fa-chevron-left"></i> numpy常用函数与常见问题
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/10/01/Lecture 4 Classification/" rel="prev" title="Hands on Machine Learning-Chapter 3">
                Hands on Machine Learning-Chapter 3 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Xiaoyu Lu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Terrylxy" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Training-Linear-Models"><span class="nav-number">1.</span> <span class="nav-text">Training Linear Models</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Setup"><span class="nav-number">2.</span> <span class="nav-text">Setup</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Linear-regression-using-the-Normal-Equation"><span class="nav-number">3.</span> <span class="nav-text">Linear regression using the Normal Equation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Linear-regression-using-batch-gradient-descent"><span class="nav-number">4.</span> <span class="nav-text">Linear regression using batch gradient descent</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Batch-Gradient-Descent"><span class="nav-number">5.</span> <span class="nav-text">Batch Gradient Descent</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Stochastic-Gradient-Descent-随机梯度下降"><span class="nav-number">6.</span> <span class="nav-text">Stochastic Gradient Descent 随机梯度下降</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Mini-batch-gradient-descent"><span class="nav-number">7.</span> <span class="nav-text">Mini-batch gradient descent</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Polynomial-regression"><span class="nav-number">8.</span> <span class="nav-text">Polynomial regression</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Regularized-models"><span class="nav-number">9.</span> <span class="nav-text">Regularized models</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Ridge-Regression"><span class="nav-number">10.</span> <span class="nav-text">Ridge Regression</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Lasso-Regression"><span class="nav-number">11.</span> <span class="nav-text">Lasso Regression</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Elastic-net"><span class="nav-number">12.</span> <span class="nav-text">Elastic net</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Early-Stopping"><span class="nav-number">13.</span> <span class="nav-text">Early Stopping</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Logistic-regression"><span class="nav-number">14.</span> <span class="nav-text">Logistic regression</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Training-and-cost-function"><span class="nav-number">15.</span> <span class="nav-text">Training and cost function</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Decisions-Boundaries"><span class="nav-number">16.</span> <span class="nav-text">Decisions Boundaries</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Iris-plants-dataset"><span class="nav-number">17.</span> <span class="nav-text">Iris plants dataset</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Multinomial-Logit-Regression"><span class="nav-number">18.</span> <span class="nav-text">Multinomial Logit Regression</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Exercise-solutions"><span class="nav-number">19.</span> <span class="nav-text">Exercise solutions</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-to-11"><span class="nav-number">19.1.</span> <span class="nav-text">1. to 11.</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-Batch-Gradient-Descent-with-early-stopping-for-Softmax-Regression"><span class="nav-number">19.2.</span> <span class="nav-text">12. Batch Gradient Descent with early stopping for Softmax Regression</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xiaoyu Lu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>








        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
